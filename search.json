[{"title":"blog tutorio","url":"/2022/02/08/blog_tutorio/","content":"hexo build up\n\n\n通过Hexo在Github上搭建博客教程 - 简书 (jianshu.com)\nhexo史上最全搭建教程_Fangzh的技术博客-CSDN博客_hexo\n\n","categories":["blog"]},{"title":"CNN","url":"/2021/08/15/cv/11.%20CNN/","content":"CNN\n\n1. Why CNN for Image?\n参数过多，我们可以减少全连接神经元、共享参数、卷积\n\n\n\nSome patterns are much smaller than the whole image\n神经元不需要看整张图片，而只需要看一小部分\n\n\n\n\n\nThe same patterns appear in different regions.\n对于重复出现的模式可以共享参数\n\n\n\n\n\nSubsampling the pixels will not change the object\n降采样不影响图像语义\n\n\n\n\n\nWe can subsample the pixels to make image smaller\nLess parameters for the network to process the image\n\n\n\n1.2 The whole CNN\n\n\n\n卷积是局部区域的加权和，做卷积时大小相同就共享参数了\n而maxpooling相当于降采样\n\n1.3  CNN Convolution\n\n由于过于一个输出通道的卷积核都可以学习一种模式，这相当于就是共享参数\n\n\n\nDo the same process for every filter\n\n\n\nCNN Zero Padding\n\n\n\nCNN Colorful image\n\n\n\nConvolution v.s. Fully Connected\n减少了很多参数\n输出多个feature map，说明其增加更多的非线性变换，增强网络的表征能力\n\n\n\n\n\nCNN Max Pooling\n\n\n\n增加非线性，以及减少参数\n\n\n\n\nFlatten\nConvolutional Neural Network\nWhat does CNN learn?\nWhat is the essential difficulty?\n深度学习解决语义鸿沟：提取高阶语义模式、不受光照、旋转等影响\n\n\nWhat can CNN do for computer vision?Before deep learning was born\nFeature extraction example #1\nFeature name: Local Binary Pattern (LBP)\n\nUse center pixel value to threshold the 3x3 neighborhood\n\nResult in binary number\n\nHistogram of the labels is used as a texture descriptor\n\n\n\nFeature extraction example #2\nFeature name: Scale invariant feature transform (SIFT)\n\nDivide the 16x16 window into a 4x4 grid of cells\n\nCompute an orientation histogram for each cell\n\n16 cells x 8 orientations = 128 dimensional descriptor\n\n\n\nWhat’s wrong with traditional features?\n\nImage classification with deep learning\nFour typical image classification nets\nImage classification with deep learning\nAlexNet\n\n\n\n\nCharacters of AlexNet\nTrained by two GPUs\n\nData augmentation\n\nClipping / flipping / …\n\nUsing ReLU rather than sigmoid function\n\nOverlapped pooling\n\nDropout in full connection layers\n\n\n\nVGG\n\n\n\nQ: Why use smaller filters? (3x3 conv)\n\n\n\n大的卷积核可以分解成小的卷积核，网络加深，获得更大感受野，减少参数，速度加快\n两个conv 33 相当于5\\5，三个相当于7*7，（2*3+1）\n\nGoogLENet\n\n\n\n\n尺度信息更丰富，防止丢失信息\n增加每一层学到的模式\n\n\n\n\nWhy not going much deeper?\n\n\n\n\nResNet\n学习到的函数变为残差：F(x)-x\n\n\n\n\n\n\n瓶颈残差块：使得计算量变少\n\n\n\n好处：保证前向信息传播的流畅性、其次保证梯度回传的稳定性\n\n\nFrom classification to segmentation\n\nConverting the segmentation problem to classification\n把一个窗口扣成小块去卷积\n\n\n\n\n\n列举所有滑动窗口去卷积分类\n这样会导致参数爆炸\n\n\n\n\nDownsampling and Upsampling\n\n先下采样，使得参数变少，再上采样还原分类结果\n\nReview: Unpooling\n\n记住pooling的所有位置，然后反pooling，除了最大值的位置，其他标为零\n\n\n\n\n\nReview: Deconvolution\n末尾补零即可\n\n\n\n\nObject detection\n\nPredict bounding boxes, class labels, and confidence scores\nFor each detection, determine whether it is true or false\n\nBasic idea to detection: Sliding windows\n\n\n\n通过设计一些判断的准则，找一些置信度最高的框保留下来\n\n\nR-CNN: Region proposals + CNN features\nRegional-based Convolutional Neural Network (R-CNN)\n\n\n\n\nFast R-CNN\n\n\nRoI pooling goal\n“Crop and resample” a fixed size feature representing a region of interest out of the feature map\n\nUse nearest neighbor interpolation of coordinates, max pooling\n\n把原始图片的候选框映射到feature map上去\n\n\n\n\nFor each RoI , predicts probabilities for c+1 classes (with background) and four bounding box offsets for c classes\n\n\nFast R-CNN training\nBounding box regression\nFaster R CNN\n\nSlide a small window (3x3) over the conv5 layer\nPredict object/no object\nRegress bounding box coordinates with reference to anchors (3 scales x 3 aspect ratios)\n\n\n一开始是调整每个候选框，而最后是只对一些框进行计算loss\n\n\n\nYOLOStreamlined detection architectures\nThe Faster R CNN pipeline separates proposal generation and regionclassification:\n\n\n\nIs it possible do detection in one shot?\n\n\n\nIdea: No bounding box proposals. Predict a class and a box for every location in a grid.\n\n\n\n\nDivide the image into 7x7 cells.\nEach cell trains a detector.\nThe detector needs to predict the object’s class distributions.\nThe detector also predicts bounding boxes and confidencescores.\n\nObjective function\n\n为了让小的框的长宽对loss的影响更大一点\n\n\n\n\n\nLocalization accuracy suffers compared to Fast(er) R CNN due to coarser features, errors on small boxes\n7x speedup over Faster R CNN (45 155 FPS vs. 7 18 FPS)\n\n\n","categories":["CV"]},{"title":"review","url":"/2021/08/15/cv/14.%20review/","content":"review\n\n1.  Course Outline\n2.  What is computer vision\nComputer vision is a field of artificial intelligence (AI)that enables computers and systems to derive meaningful information from digital images, videos and other visual inputs —and take actions or make recommendations based on that information.   —IBM\n利用计算机系统从数字图像、视频和其他可视化输入提取有意义的信息，并基于这些信息做出决策\n\n\n\nKey point 1: human visual system\n能够模拟人\n双目-&gt;三维重构\n多尺度特性-&gt;sift\n注意力机制-&gt;op算法、空间注意力机制FCNN\n并行处理-&gt;多尺度多特征融合\n\n\nPhilosophies learn from the human visual system for computer vision systems.\nHierarchical :Multi-scale fusion\nMeaning\n\n\nApplications:\nHandcrafted features, e.g., SIFT, HOG..….\nDeep learning architectures, e.g., segmentation….\n\n\nAttention mechanism\nMeaning\nApplications: various CNNs\n\n\n\n3. Key point 2: computer vision system (CVS)\n系统分析\n\nRelated domain knowledge when you construct a CVS.Examples: a self-driving system, a video surveillance system……\n\n天气预报\n\n云图：根据图像以及过往天气预测未来天气、用到图像处理、信息、天气学、深度学习\n\n\n\n\n4. Key point 3: CVS in our daily lives\n人脸识别：VGmodel RCNN \n耗时、准确率低\n\n\n指纹识别、身份证识别\n\nVarious applications\n\n分析计算机视觉应用，分析算法优劣\n\n\n\n5. Key point 4: challenges\nAnalyze the challenges with real-life CV systems\n\n\n\n尺度：尺度金字塔\n光线：边缘、角点\n划归为cell，增加光照鲁棒性\n\n\n视角\n遮挡：特征点检测、局部特征应对遮挡\n形变：分开考虑\n\n\n6. Image filtering\n平滑滤波移除高频特征\n高斯滤波：高斯核可分，降低计算量\n\n两个的$\\sigma$关系\n\n\n补零关系\n\nComputing\n\nProperties:\n\nRemove “high-frequency” components from the image\nConvolution with self is another Gaussian\nSo can smooth with small-σ kernel, repeat, and get same result as larger-σ kernel would have\nConvolving two times with Gaussian kernel with std. dev. σ is same as convolving once with kernel with std. dev. σ√2\n\n\nSeparable kernel\nFactors into product of two 1D Gaussians\n\n\nPadding on the edge: methods and problems\nWhat is the complexity of filtering an n×nimage with an m×mkernel?\nWhat if the kernel is separable?\nO(n2m2)\nO(n2m)\n\n\n\n\n\n\n\n6.2 Key point 2: Separability\n6.3 Key point 3: Image filtering -noise\nSalt and pepper noise:Contains random occurrencesof black and white pixels\nImpulse noise:Contains random occurrencesof white pixels\nGaussian noise:Variations in intensity drawn from a Gaussian normal distribution\n\n\n\n中值滤波利于处理椒盐噪声，容易滤去过低或过高的异常值\n利于保护边缘\n不好是非线性、不能写成卷积\n\n\n高斯滤波利于去除高斯噪声\n可能会把边缘模糊掉\n\n\n\n\n6.4 Key point 4: Sharpening\nUnderstand the process and parameter influence\nWhat does blurring take away?\n\n\n\n\n\nLet’s add it back:\n\n\n7. Edge detection7.1 Key point 1: Image gradient\n7.2 Key point 2: Edge filters\nDesign philosophy and their functions\n\nCompute\n\n\n\n7.3 Key point 3: Canny edge detector\nSteps and their motivations\n\nParameter choice and reasons\n\n\n\nFilter image with derivative of Gaussian\nFind magnitude and orientation of gradient\nNon-maximum suppression:-Thin wide “ridges” down to single pixel width\nLinking and thresholding (hysteresis):-Define two thresholds: low and high-Use the high threshold to start edge curves and the low threshold to continue them\n\n8. Local features -corner8.1 Key point 1: Corner Detection -Basic Idea\nWe should easily recognize the point by looking through a small window\nShifting a window in any direction should give a large change in intensity\n\n8.2 Key point 1: Harris detector\n\n\nStep3: Compute corner response function R and judge it is a corner or edge or ….\n\n8.3 Key point 2: Harris detector –Properties\n9.1 Key point 1: Scale space/SIFT\n9.2 Key point 4: HOG -steps and motivations\n怎么算\n\nBlocks and cells:\n\nEach block contains 2×2 cells\nEach cell is with 8×8 pixels\nEach block：16×16 pixels\nNeighboring blocks are with 50% overlap.\nFor a 64×128 image, it cantains7×15 = 105 blocks in total\n\n\n\n\n9.3 Key point 5: HOG for Detection\n卷积、hog特征、三维\n10. Key point 1: RANSAC for line fitting\n\n\n\n参数选择\n\n\n11. K-means: pros and cons\n\nNormalized cut：为什么不一样\n\n\n\nKey point 1: Viola-Jones face detector\n\nVisual vocabularies: Issues\nHow to choose vocabulary size?•Too small: visual words not representative of all patches•Too large: quantization artifacts, overfitting•Why BOW?•Efficiency•BOW have been useful in matching an image to a large database\n\n\n\nKey point 2: Pedestrian detection with HOG\n\nTrain a pedestrian template using a linear support vector machine\n\nAt test time, convolve feature map with template\nFind local maxima of response\nFor multi-scale detection, repeat over multiple levels of a HOG pyramid\n\n\nStrengthsWorks very well for non deformable objects with canonical orientations: faces, cars, pedestriansFast detectionWeaknessesNot so well for highly deformable objects or “stuff”Not robust to occlusionRequires lots of training data\nMotion and Tracking\nKey point 1: Optical flow\n2D transformations\n\n\n\n6个自由度\nCamera model World\n\n\n\n\n\n\n\n\n目标检测：传统方法、现在方法\nRCNN演变过程，区别，目的，为什么更快了\n\n\n\ntransfer learning\n\n","categories":["CV"]},{"title":"Transfer learning for CV","url":"/2021/08/15/cv/13.%20Transfer%20learning%20for%20CV/","content":"Transfer learning for CV、self-supervised learning\n\n1. Transfer learning for CV\n\n1.1 Why?\n1.2 Traditional vs. Transfer Learning\n\n由源数据学到一些共用的知识，在进行微调\n\nTraditional machine learning:\n\nlearn a system for a task, respectively\n\n\nTransfer learning:\n\ntransfer the knowledge form the source model for the target task\n\n\nTask description\n\nSource data: $(x^s, y^s)$  A large amount\nTarget data: $(x^t, y^t)$​  Very little\nOne-shot learning: only a few examples in target domain\n\n\nExample: (supervised) speaker adaption\nSource data: audio data and transcriptions from many speakers\nTarget data: audio data and its transcriptions of specific user\n\n\nIdea: training a model by source data, then fine-tune the model by target data\nChallenge: only limited target data, so be careful about overfitting\n\n\n\n1.3 Conservative Training\n\n学习率调的很低\n\n1.4 Layer Transfer\n\nWhich layer can be transferred (copied)?\nSpeech: usually copy the last few layers\nImage: usually copy the first few layers\n\n\n\n\n1.5 Neural Network Layers: General to Specific\nBottom/first/earlier layers: general learners\nLow-level notions of edges, visual shapes\n\n\nTop/last/later layers: specific learners\nHigh-level features such as eyes, feathers\n\n\n\n1.6 Multitask Learning\nThe multi-layer structure makes NN suitable for multitask learning\n任务相关则可以共享部分参数\n\n\n\n\n1.7 Progressive Neural Networks\n\n不考虑任务相关性\n只进行特征共享，但是不共享参数\n\n2. Domain-adversarial training2.1 Task description: domain adaptation\n\nHow to remove the domain shift?\nHow to bridge the domain gap?\nThe domain can be a general concept:\nDatasets: transfer from an “easy” dataset to a “hard” one\nModalities: transfer from RGB to depth, infrared images, point cloud……\n\n\n\n\n\nRemove the domain shift\n\n\n2.2 Discrepancy-based approaches\n我们希望两者数据越接近越好，这样在源数据训练可以迁移到目标数据\n\nIdea: minimize the domain distance in a feature space\n\nWorks focus on designing a reasonable distance\n\n\n\nExample: Metric learning based\n\n\n\\begin{aligned}\n&D_{t s}^{(m)}\\left(\\mathcal{X}_{t}, \\mathcal{X}_{s}\\right)= \n\\quad\\left\\|\\frac{1}{N_{l}} \\sum_{i=1}^{N_{t}} f^{(m)}\\left(\\mathbf{x}_{t i}\\right)-\\frac{1}{N_{s}} \\sum_{i=1}^{N_{z}} f^{(m)}\\left(\\mathbf{x}_{s i}\\right)\\right\\|^{2}\n\\end{aligned}2.3 Adversarial-based approaches\n\n我们希望找到一个特征空间可以使他们的特征领域可以混在一起\n\n2.4 Adversarial-based approaches\nMethod 1: Domain-adversarial training\n\n\n\n\n不同于GAN，GAN的分类器希望能分开fake数据，而对抗学习希望分类器越分不开越好\n\n\n\n所以我们对于domain classifier不能使用梯度下降，而应该使用梯度反向\n\n\n\nMethod 2: GAN-based methods\n\n\n2.5 Reconstruction-based approaches\n\nThe data reconstruction of source or target samples is an auxiliary task that simultaneously focuses on creating a shared representation between the two domains and keeping the individual characteristics of each domain.\n\n2.6 Knowledge distillation\nDistill the knowledge from a larger deep neural network into a small network\n\n\n\nResponse-based knowledge\nUse the neural response of the last output layer of the teacher model to transfer.\nDirectly mimic the final prediction of the teacher model.\nSimple yet effective\n\n\n\n\n\n大型网络与轻型网络分类越相近，越好\n\nFeature-based knowledge\n\nExtend the transfer point from the last layer to intermediate layers\nA good extension of response-based knowledge, especially for the training of thinner and deeper networks.\nGeneralize feature maps to attention maps\n\n\n\n\n\nRelation-based knowledge\nBoth response-based and feature-based knowledge use the outputs of specific layers in the teacher model.\nRelation-based knowledge further explores the relationships between different layers or data samples.\n\n\n考虑不同的特征分布\n\n\n\nExtension: Cross-modal distillation\nThe data or labels for some modalities might not beavailable during training or testing\n\n\n\n\n3. Self-supervised learning3.1  Motivation\nRecall the idea of transfer learning: start with general-purpose feature representation pre-trained on a large, diverse dataset and adapt it to specialized tasks\n\nChallenge: overcoming reliance on supervised pre-training\n\n\n\n\n3.2 Self-supervised pretext tasks\nSelf-supervised learning methods solve “pretext” tasks that producegood features for downstream tasks.\nLearn with supervised learning objectives, e.g., classification, regression.\nLabels of these pretext tasks are generated automatically\n\n\nExample: learn to predict image transformations / complete corrupted images\n\n\n3.2.1 Self-supervised learning workflow (I)\n\nLearn good feature extractors from self-supervised pretext tasks, e.g., predicting image rotations\n\n3.2.2 Self-supervised learning workflow (II)\n\nAttach a shallow network on the feature extractor; train the shallownetwork on the target task with small amount of labeled data\nEvaluate the learned feature encoders on downstream target tasks\n\n3.2.3 Self-supervisedvs. unsupervisedlearning\nThe terms are sometimes used interchangeably in the literature, but self-supervised learning is a particular kind ofunsupervised learning\nUnsupervised learning: any kind of learning without labels\n\nClustering and quantization\nDimensionality reduction, manifold learning\nDensity estimation…\n\n\nSelf-supervised learning: the learner “makes up” labels from the data and then solves a supervised task\n\n\n3.3 Self-supervisedvs. Generative learning\nBoth aim to learn from data without manual label annotation.\n\nGenerative learning aims to model data distribution, e.g., generating realistic images.\n\n希望能生成和真实越相近越好的图片，更注重细节\n\n\nSelf-supervised learning aims to learn high-level semantic features with pretext tasks\n\n只学习高阶语义信息\n\n\n\n3.4 Types of self-supervised learning\n预测遮挡，预测上色，预测未来\n\n\n\n预测拼图\n\n\n\n对比学习\n\n\n3.5 Self-Supervision as data prediction3.5.1Colorization\n\n要考虑固有颜色的歧义性，只要上色会在自然界出现，就不判错\n\nColorization: Training data generation\n\n数据灰度化\n\n\n\n\n\n用ab作为监督信息\n对ab空间进行量化，从而预测一个分布，最终考虑到了颜色的歧义性\n\n\n3.6 Self-supervision by transformation prediction\nPretext task:randomly sample a patch and one of 8 neighbors，Guess the spatial relationship between the patches\n\n\n\n3.6.1 Context prediction: Details\n切割时留有gap，防止学到这些边缘\n\n\n3.6.2 Jigsaw puzzle solving\n\n不同于预测位置，而是考虑九个块整体的一个顺序\n\nDetails\n防止过拟合，只考虑64种组合，其hamming loss较大\n\n\n3.6.3 Rotation prediction\nPretext task: recognize image rotation (0, 90, 180, 270 degrees)\n\n\n\nDuring training, feed in all four rotated versions of an image in the same mini-batch\n\n\n3.7 Contrastive methods\nEncourage representations of transformed versions of the same image to be the same and different images to be different\n希望同种信息越相近越好，不同种数据越不相近越好\n\n\n\n\n\nEncourage representations of transformed versions of the same image to be the same and different images to be different\n\n\n\nGiven: query point $x$, positive samples $x^{+}$, negative samples $x^{-}$\nPositives are typically transformed versions of $x$, negatives are random examples from the same mini-batch or memory bank\n\n\nKey idea: learn representation to make $x$ similar to $x^{+}$, dissimilar from $x^{-}$(similarity is measured by dot product of normalized features)\nGiven 1 positive sample and $N$ - 1 negative samples, Contrastive loss:\n\n\nl\\left(x, x^{+}\\right)=-\\log \\frac{\\exp \\left(f(x)^{T} f\\left(x^{+}\\right) / \\tau\\right)}{\\frac{\\exp \\left(f(x)^{T} f\\left(x^{+}\\right) / \\tau\\right)}{\\text { Score for the positive }}+\\frac{\\sum_{j=1}^{N} \\exp \\left(f(x)^{T} f\\left(x_{j}^{-}\\right) / \\tau\\right)}{\\text { pair }}}\nThis seems familiar as cross entropy loss for a N-way Softmaxclassifier!Try to find the positive samples from the Nsamples.\n$\\tau$​​ is the temperature hyperparameter(determines how concentrated the softmaxis)\n我们希望温度参数越小越好，这样预测越集中\n\n\n3.8 SimCLR: A Simple Framework for Contrastive Learning\nGenerate positive samples through data augmentation.\n\nUse a projection network 𝒉𝒉(·)to project features to a space where contrastive learning is applied\n\n\n\n\nSimCLR：Evaluation\n\n\n\n\nTrain feature encoder on ImageNet (entire training set)using SimCLR.\nFreeze feature encoder, train a linear classifier on top withlabeled data.\n\n3.8.1 SimCLRdesign choices: projection head\n\nLinear / non-linear projection heads improve representation learning.A possible explanation:\nrepresentation space 𝒛𝒛is trained to be invariant to data transformation\ncontrastive learning objective may discard useful information for downstream tasks\nby leveraging the projection head 𝒈(ᐧ), more information can be preserved in the 𝒉 representation space\n\n\n\n","categories":["CV"]},{"title":"Edge Detection","url":"/2021/08/15/cv/3.1%20Edge%20Detection/","content":"Edge Detection\n\nConvolution1. calculate\n\nMeaning\n保存不变\n\n\n\n向右移动\n\n\n\n平滑\n\n\n\n图像锐化\n\n\n\n\nSharpening filter: Accentuates differences with local average \n锐化过滤器：突出与局部平均值的差异\n\n\n\n\n计算结果大小\npadding\nzero “padding”\n\nedge value replication\n\nmirror extension\n\nmore (beyond the scope of this\n\n\nSmoothing with box filter revisited\n使用平滑滤波器会导致边缘消失\n为了消除边缘效应，对邻域的权重贡献，根据像素与中心的接近程度确定像素。\n\n\nGaussian Kernel\n\n记得归一化，一般来说$\\sigma$的大小决定了高斯核的大小，所以标准差𝜎: 确定平滑的范围\n作用：从图像中删除“高频”分量（低通滤波器）\n\n与自身的卷积是另一种高斯函数\n\n所以可以用小的平滑𝜎内核，重复与自身卷积，可以得到和大卷积核卷积相同的结果\n卷积两次的高斯核相当于标准差变为$\\frac{\\sigma}{\\sqrt{2}}$​\n\n\n\nSeparable kernel\n2D高斯核具有可分性，可以分为两个一维卷积核\n\n\nG(x, y)=\\frac{1}{2 \\pi \\sigma^{2}} \\exp ^{-\\frac{x^{2}+y^{2}}{2 \\sigma^{2}}}=\\left(\\frac{1}{2 \\pi \\sigma} \\exp ^{-\\frac{x^{2}}{2 \\sigma^{2}}}\\right)\\left(\\frac{1}{2 \\pi \\sigma} \\exp ^{-\\frac{y^{2}}{2 \\sigma^{2}}}\\right)\n\nWhat is the complexity of filtering an $𝑛×𝑛$ image with an $𝑚×𝑚$​​ kernel?\n$O(n^2m^2)$\nWhat if the kernel is separable?\n$O(n^2m)$\n\n(Cross) correlation\nProperties\nCommutative property:\nf * * h=h * * f\nAssociative property:\n\\left(f * * h_{1}\\right) * * h_{2}=f * *\\left(h_{1} * * h_{2}\\right)\nDistributive property:\nf * *\\left(h_{1}+h_{2}\\right)=\\left(f * * h_{1}\\right)+\\left(f * * h_{2}\\right)\nThe order doesn’t matter! $\\quad h_{1}   h_{2}=h_{2}   h_{1}$\n\nShift property:\n\nf[n, m] * * \\delta_{2}\\left[n-n_{0}, m-m_{0}\\right]=f\\left[n-n_{0}, m-m_{0}\\right]\nShift-invariance:\n\n\n\\begin{gathered}\ng[n, m]=f[n, m] * h[n, m] \\\\\n\\Longrightarrow f\\left[n-l_{1}, m-l_{1}\\right] * h\\left[n-l_{2}, m-l_{2}\\right] \\\\\n=g\\left[n-l_{1}-l_{2}, m-l_{1}-l_{2}\\right]\n\\end{gathered}Convolution vs. (Cross) Correlation\nA convolution is an integral that expresses the amount of overlap of one function as it is shifted over another function.\nconvolution is a filtering operation\n\n\nCorrelation compares the similarity of two sets of data. Correlation computes a measure of similarity of two input signals as they are shifted by one another. The correlation result reaches a maximum at the time when the two signals match best.\ncorrelation is a measure of relatedness of two signals\n\n\n\nEdge Detection1. Edges1.1 Def\nsignificant local changes of intensity (discontinuities) in an image. 图像中强度的显着局部变化（不连续性）\n\n1.2 Origins of edges\ndiscontinuity in depth 深度不连续\nsurface normal/color/texture discontinuity 表面法线\\颜色\\纹理不连续\nspecularity /shadows 由于光照的阴影\n\n2. Image gradient2.1 The gradient of an image\n\\nabla f=\\left[\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\\right]\n\\theta=\\tan ^{-1}\\left(\\frac{\\partial f}{\\partial y} / \\frac{\\partial f}{\\partial x}\\right)\n\\|\\nabla f\\|=\\sqrt{\\left(\\frac{\\partial f}{\\partial x}\\right)^{2}+\\left(\\frac{\\partial f}{\\partial y}\\right)^{2}}\n\n梯度垂直于图片边缘\n\n\n3. Effects of noise\n\n如果信号中有噪声，边缘的特征可能会淹没在噪声中，从而无法通过求梯度的方法，对边缘进行定位。\n所以，实际上我们经常先对信号做平滑处理，然后再求导。\n\n\n4. Sobel Operator4.1 算法介绍\nUses two 3 3×3 kernels which are convolved with the original image to calculate approximations of the derivatives\nOne for horizontal changes, and one for vertical\n\n\n\\mathbf{G}_{x}=\\left[\\begin{array}{ccc}\n+1 & 0 & -1 \\\\\n+2 & 0 & -2 \\\\\n+1 & 0 & -1\n\\end{array}\\right] \\quad \\mathbf{G}_{y}=\\left[\\begin{array}{ccc}\n+1 & +2 & +1 \\\\\n0 & 0 & 0 \\\\\n-1 & -2 & -1\n\\end{array}\\right]\nSmoothing + differentiation：平滑处理＋微分\n\n\n\\begin{aligned}\n&\\mathbf{G}_{x}=\\left[\\begin{array}{ccc}\n+1 & 0 & -1 \\\\\n+2 & 0 & -2 \\\\\n+1 & 0 & -1\n\\end{array}\\right]=\\left[\\begin{array}{l}\n1 \\\\\n2 \\\\\n1\n\\end{array}\\right]\\left[\\begin{array}{lll}\n+1 & 0 & -1\n\\end{array}\\right]\\\\\n&\\text { Gaussian smoothing differentiation }\n\\end{aligned}\n之所以$[1 2 1]^t$可以看作为高斯核，这是因为其数值呈现类似高斯分布的效果，并且可以通过数学手段验证。\n\nMagnitude: 模值\n\n\n\n\\mathbf{G}=\\sqrt{\\mathbf{G}_{x}^{2}+\\mathbf{G}_{y}^{2}}\nAngle or direction of the gradient: 方向\n\n\n\\Theta=\\operatorname{atan}\\left(\\frac{\\mathbf{G}_{y}}{\\mathbf{G}_{x}}\\right)4.2 Sobel Filter Problems\n\nPoor Localization (Trigger response in multiple adjacent pixels)：定位不够准确，边缘可能很粗\nThresholding value favors certain directions over others\nCan miss oblique edges more than horizontal or vertical edges 可能丢失除了水平以及垂直的边缘\nFalse negatives 最终造成把边缘识别为不是边缘\n\n\n\n4.3 Other approximations of derivative filters4.3.1 Prewitt:\nG_{x}=\\left[\\begin{array}{lll}\n-1 & 0 & 1 \\\\\n-1 & 0 & 1 \\\\\n-1 & 0 & 1\n\\end{array}\\right] \\quad G_{y}=\\left[\\begin{array}{ccc}\n1 & 1 & 1 \\\\\n0 & 0 & 0 \\\\\n-1 & -1 & -1\n\\end{array}\\right]\\\n除了考虑中心像素左右邻近的像素值，还考虑了其对角的领域像素。\n\n4.3.2 Roberts:\nG_{x}=\\left[\\begin{array}{cc}\n0 & 1 \\\\\n-1 & 0\n\\end{array}\\right] \\quad G_{y}=\\left[\\begin{array}{cc}\n1 & 0 \\\\\n0 & -1\n\\end{array}\\right]\n$G_x$用于检测135°的边缘，$G_y$用于检测45°的边缘。\n\n5. Canny edge detector\nThis is probably the most widely used edge detector in computer vision\n\n5.1 Derivative of Gaussian filter\n\\frac{d}{d x}(f * g)=f * \\frac{d}{d x} g\n对于高斯平滑核而言所有元素都是正的，对于高斯偏导核而言有可能存在非正的元素。\n\n对于高斯平滑核而言，其元素之和为1；对于高斯偏导核而言，其所有元素之和为0（奇函数）。\n\n\n5.2 Problems of Gaussian filter5.2.3 细节过多\n\n我们可以发现高斯偏导核可能检测出许多我们不需要的细节。所以我们尝试将高斯偏导核的结果再输入一个阈值核（Tresholdingb Kernel），滤去不用的边缘。\n\n5.2.4 边缘过粗\n\n\n这是因为，我们原本的边缘取的是梯度的最大值，但是阈值的方法取的是阈值以上的整个部分。\n\n\n5.2.5 Non-maximum suppression\n为了解决边缘过粗的问题，我们可以采用Non-maximum的方法：\n\n\n\n沿着边缘的梯度方向，每次记录邻近像素最大的一个值，一般地，如果当前像素梯度方向不存在邻近像素，则考虑用像素插值地方法，充当其邻近像素。最终使得整个边缘的宽度为1\n\n\n5.2.6 边缘消失（FN problem）\n5.2.7 Hysteresis thresholding\nAvoid streaking near threshold value 避免在阈值附近的边缘丢失\n\nDefine two thresholds: Low and High\n\nIf less than Low, not an edge\n\nIf greater than High, strong edge\n\nIf between Low and High, weak edge\n\n\n\n\n\n首先能够检测出强边缘的像素，以及弱边缘的像素：\n\n接着，我们让强边缘的像素不断与周围像素进行比较，总是取最接近的邻居进行延申；同理，弱边缘也进行延申，若强边缘最终可以和强边缘连接起来，则保留该弱边缘，反之则忽略该弱边缘。\n\n\n5.3 Summary\nFilter image with 𝑥,𝑦derivatives of Gaussian 使用x,y方向的高斯偏导核进行滤波\nFind magnitude and orientation of gradient 找到边缘的梯度大小以及梯度方向\nNon maximum suppression:\nThin multi pixel wide ridges down to single pixel width 将多像素的边缘降至单像素\n\n\nThresholding and linking (hysteresis): 设置高低阈值，滤去不必要的细节同时，保留阈值附近及以上的边缘\nDefine two thresholds: low and high\nUse the high threshold to start edge curves and the low threshold to continue them\n\n\n\n\n5.4 Effect of $\\sigma$ Gaussian kernel spread/size)\n\n\n$\\sigma$越小，窗口越小（由于$3\\sigma$原则），能检测出更多的细节，适合人脸检测；\n$\\sigma$越大，窗口越大（由于$3\\sigma$​原则），能检测出更少的细节，更注重整体轮廓，适合行人检测。\n\n5.4 Concluding remarks\nAdvantages:\nConceptually simple. 概念简单\nEasy implementation 易于实施\nHandles missing and occluded data very gracefully. 易于解决缺失值\nCan be adapted to many types of forms, not just lines 可以适应多种形式\n\n\nDisadvantages:\nComputationally complex for objects with many parameters. 参数过多，计算复杂\nLooks for only one single type of object \nCan be “fooled” by “apparent lines”.  可能被“明显的线条”迷惑，比如共线\nThe length and the position of a line segment cannot be determined.\nCo linear line segments cannot be separated.\n\n\n\n","categories":["CV"]},{"title":"Seam Carving","url":"/2021/08/15/cv/3.3%20Seam%20Carving/","content":"Seam Carving\n\n1. DefProblem statement:\n\nInput Image $𝐼_{𝑛×𝑚}$ and new size 𝑛′×𝑚′\n\nOutput Image 𝐼′of size 𝑛′×𝑚′which will be “ good representative ” of the original image 𝐼\n即：对图像进行缩放的同时希望输出图像对内容有好的展示\n\n\n2. General Retargeting Framework\n\nAssume $m \\times n \\rightarrow m \\times n’, n’&lt;n$​​​ (summarization)\nBasic Idea: remove unimportant pixels from the image 移除不重要的像素出去\nUnimportant = pixels with less “energy” 不重要指的是能量较低\n\n\n\n\nE_{1}(\\mathbf{I})=\\left|\\frac{\\partial}{\\partial x} \\mathbf{I}\\right|+\\left|\\frac{\\partial}{\\partial y} \\mathbf{I}\\right|\nIntuition for gradient based energy:\nPreserve strong contours\nHuman vision more sensitive to edges so try remove content from smoother areas\nSimple enough for producing some nice results\n\n\n\n3. A Seam3.1 Def\nA connected path of pixels from top to bottom (or left to right).可以理解为一个像素的八领域像素点4\n\n\n\\begin{aligned}\n&\\mathrm{s}^{\\mathrm{x}}=\\left\\{s_{i}^{x}\\right\\}_{i=1}^{n}=\\{(x(i), i)\\}_{i=1}^{n}, \\text { s.t. } \\forall i,|x(i)-x(i-1)| \\leq 1 \n\\end{aligned}\n\\begin{aligned}\n&\\mathrm{~s}^{\\mathrm{y}}=\\left\\{s_{j}^{y}\\right\\}_{j=1}^{m}=\\{(j, y(j))\\}_{j=1}^{m}, \\text { s.t. } \\forall j|y(j)-y(j-1)| \\leq 1\n\\end{aligned}\n3.2 Finding the Seam?\n\n3.3 The Optimal Seam\nThe recursion relation\n\n\n\\mathbf{M}(i, j)=E(i, j)+\\min (\\mathbf{M}(i-1, j-1), \\mathbf{M}(i-1, j), \\mathbf{M}(i-1, j+1))\nCan be solved efficiently using dynamic programming in $O(s\\times n\\times m)$\n\n3.4 Dynamic Programming3.4.1 Recursion\n𝑀(𝑖,𝑗)= minimal cost of a seam going through (𝑖,𝑗) 先计算整张图的能量，计算的方法为该像素点过去的领域像素中能量的最小值加上自身的能量。\n\n\n\n\n最终：\n\n\n3.4.2 Backtrack\nBacktrack (can store choices along the path, but do not have to)\n在最后一层找最小的一个能量的像素点\n然后回溯，找该像素上三领域中能量最小的像素\n直到遍历到第一行结束。\n\n\n\n\n3.4.3 伪代码\n3.5 效果\n\nQ: Will the result be the same if the image is flipped upside down?\nA: Yes (up to numerical stability)\n\n\nQ: What happens to the overall energy in the image during seam carving?\n变大\n\n\n所以当我们resize图像后，对于整个图像的平均能量应该变大\n\n\n\n比较：\n\n\n\n我们可以发现seam的效果最好\n\n3.6 Both Dimensions?\n$m \\times n \\rightarrow m’ \\times n’, n’&lt;n, m’&lt;m$\n\n\n\nThe recursion relation:\n\n\n\\begin{aligned}\n&\\min _{\\mathbf{s}^{\\mathbf{x}}, \\mathbf{s}^{y}, \\alpha} \\sum_{i=1}^{k} E\\left(\\alpha_{i} \\mathbf{s}_{\\mathbf{i}}^{\\mathbf{x}}+\\left(1-\\alpha_{i}\\right) \\mathbf{s}_{\\mathbf{i}}^{\\mathbf{y}}\\right)\n\\end{aligned}\n\\begin{aligned}\n&r=n-n^{\\prime} \\quad c=m-m^{\\prime} \\quad \\mathrm{k}=r+c\n\\end{aligned}\nTransport map :\n\n\n\\begin{aligned}\n\\mathbf{T}(r, c)=\\min \\left(\\mathbf{T}(r-1, c)+E(\\mathbf{s}^{\\mathbf{x}}(\\mathbf{I}_{\\mathbf{n}-\\mathbf{r}-1 \\times \\mathbf{m}-\\mathbf{c}}\\right)),\n\\\\\n\\left.\\mathbf{T}(r, c-1)+E\\left(\\mathbf{s}^{\\mathbf{y}}\\left(\\mathbf{I}_{\\mathbf{n}-\\mathbf{r} \\times \\mathbf{m}-\\mathbf{c}-\\mathbf{1}}\\right)\\right)\\right)\n\\end{aligned}\n其物理含义就是，\n\n\n","categories":["CV"]},{"title":"Edge Preserving","url":"/2021/08/15/cv/3.2%20Edge%20Preserving/","content":"Edge Preserving\n1. Bilateral filter双边滤波算法原理及代码介绍_leonardohaig的博客-CSDN博客_联合双边滤波上采样算法原理\n1.1 算法解析\n\n值域核r,表示邻域内某点(k,l)的灰度值f(k,l)与中心点(i,j)灰度值f(i,j)的差的绝对值:\n可以看到值域核用来表征邻域内像素的相似程度(接近程度)\n\n\n\n\nG_r(P_i-P_j)=r(i, j, k, l)=\\exp \\left(-\\frac{\\|f(i, j)-f(k, l)\\|^{2}}{2 \\sigma_{r}^{2}}\\right)\n空间域核d,表示邻域内某点(k,l)与中心点(i,j)的欧式距离: 这就是高斯滤波核\n\n\nG_s(x_i-x_j)=d(i, j, k, l)=\\exp \\left(-\\frac{(i-k)^{2}+(j-l)^{2}}{2 \\sigma_{d}^{2}}\\right)\n权重系数为空间域核和值域核的乘积:\n\n\n\\begin{gathered}\nw(i, j, k, l)=d(i, j, k, l) * r(i, j, k, l)= \\\\\n\\exp \\left(-\\frac{(i-k)^{2}+(j-l)^{2}}{2 \\sigma_{d}^{2}}-\\frac{\\|f(i, j)-f(k, l)\\|^{2}}{2 \\sigma_{r}^{2}}\\right)\n\\end{gathered}\n双边滤波中(i,j)位置的像素值g(i,j)依赖于邻域内像素值f与其权重w的加权组合(k,l表示邻域像素位置)：\n\n\ng(i, j)=\\frac{\\sum_{k, l} f(k, l) w(i, j, k, l)}{\\sum_{k, l} w(i, j, k, l)}\n对于高斯滤波，离中心像素距离越近，权重越大（物理距离）\n对于值域滤波，灰度值相差大的点权重越小，所以最终边缘交界处的权重接近于零，而非边缘处，由于灰度值相差小，所以权重较大，不可能为边缘。\n当两者相乘，最终可以做到既消除噪音，又巧妙保留了边缘。\n\n1.2 算法案例\n\n其可以看作是10×10的一张图像，图中的数字表示每个点的像素值。在图中存在一个5×5大小的滑动窗口，我们需要求出中心点灰度值146的新像素值。\n\n边缘复制/边界填充(不多说)\n首先遍历整个窗口，第一个遍历到的点是165，那么中心点与该点的空间域计算结果为：\n\n\n\n再计算中心点与该点的像素域结果：\n\n\n\n\n​            当σs=5与σr=20时，Gσs = 0.8521，Gσr = 0.6368。\n​          3. 接着遍历整个窗口，将窗口内每个像素点都与中心点建立联系，求出它们的Gσs与Gσr的值，将Gσs与Gσr             相乘即得到每个点对应的权重Wp，即Wp = Gσs × Gσr。\n​            在遍历结束后，用每个点的Wp乘上该点的像素值I(i,j)，并求和，这是作为分子。将每个点的Wp相加，作为            分母，两者相除，即得到需要的新输出图像的中心点（i，j）的像素值。\n1.3 opencv 代码实现CV_EXPORTS_W void bilateralFilter( InputArray src, OutputArray dst, int d,                                   double sigmaColor, double sigmaSpace,                                   int borderType = BORDER_DEFAULT );//第一个参数，InputArray类型的src，输入图像，即源图像，需要为8位或者浮点型单通道、三通道的图像。//第二个参数，OutputArray类型的dst，即目标图像，需要和源图片有一样的尺寸和类型。//第三个参数，int类型的d，表示在过滤过程中每个像素邻域的直径。如果这个值我们设其为非正数，那么OpenCV会从第五个参数sigmaSpace来计算出它来。//第四个参数，double类型的sigmaColor，颜色空间滤波器的sigma值。这个参数的值越大，就表明该像素邻域内有更宽广的颜色会被混合到一起，产生较大的半相等颜色区域。//第五个参数，double类型的sigmaSpace坐标空间中滤波器的sigma值，坐标空间的标注方差。他的数值越大，意味着越远的像素会相互影响，从而使更大的区域足够相似的颜色获取相同的颜色。当d&gt;0，d指定了邻域大小且与sigmaSpace无关。否则，d正比于sigmaSpace。//第六个参数，int类型的borderType，用于推断图像外部像素的某种边界模式。注意它有默认值BORDER_DEFAULT。\n2. Joint Bilateral filter联合双边滤波器（joint bilateral filter)【OpenCV】_ShaderJoy 的兴趣技术杂货铺-CSDN博客\n2.1 简介\n\n前面介绍了双边滤波器（bilateral filter,LBF），然而BF的权值是不稳定的，因此在边缘附近会出现一些翻转。此外BF计算复杂度是O(r^2)；为了改善BF权值的稳定性，引入了联合双边滤波器（joint bilateral filter ,LBF)。两者之间的差别就是JBF用了一个导向图作为值域权重的计算依据。下面我们通过数学公式展示二者的不同：\n\n先看BF的，如（1）所示，\n\n\n\nJ_{p}=\\frac{1}{k_{p}} \\sum_{g \\in \\Omega} I_{q} f(\\|p-q\\|) g\\left(\\left\\|I_{p}-I_{q}\\right\\|\\right),\n再次解释一下公式中的符号意义，其中I表示输入图像，p、q表示像素在图像中的坐标，Ip表示对应位置的像素值，J表示输出， f、g是权重分布函数，一般为高斯函数。这种滤波的结果就是周边像素的权值不仅和距离有关还和那个位置的像素值有关。\n再看JBF，如（2）所示，\n\n\n\nJ_{p}=\\frac{1}{k_{p}} \\sum_{a \\in 0} I_{q} f(\\|p-q\\|) g\\left(\\left\\|\\tilde{I}_{p}-\\tilde{I}_{q}\\right\\|\\right)\n如果在值域的权重计算过程引入另外一幅图像，如下式，则称之为联合双边滤波。 $\\tilde{I}$​​ 就是引入的另外一幅图像。该图像 必须与待处理的图像相似。\n联合双边滤波上采样技术也很简单，一种便于理解的也便于写代码的方式就是把下采样并进行处理过后的小图按照最近邻插值的方式放大到原图大小，然后再用原图的数据和这个放大的结果进行联合双边滤波处理 。\n\n2.2 代码实现：function B = jbfltGray(D,C,w,sigma_d,sigma_r)%    D should be a double precision matrix of size NxMx1 (i.e., grayscale)  with normalized values in the %    closed interval [0,1]. %    C should be similar to D, from which the weights are calculated, with normalized values in the%   closed interval [0,1]. % Pre-compute Gaussian distance weights.[X,Y] = meshgrid(-w:w,-w:w);G = exp(-(X.^2+Y.^2)/(2*sigma_d^2));% Apply bilateral filter.dim = size(D);B = zeros(dim);for i = 1:dim(1)   for j = 1:dim(2)               % Extract local region.         iMin = max(i-w,1);         iMax = min(i+w,dim(1));         jMin = max(j-w,1);         jMax = min(j+w,dim(2));         I = D(iMin:iMax,jMin:jMax);          % To compute weights from the color image         J = C(iMin:iMax,jMin:jMax);         % Compute Gaussian intensity weights according to the color image         H = exp(-(J-C(i,j)).^2/(2*sigma_r^2));         % Calculate bilateral filter response.         F = H.*G((iMin:iMax)-i+w+1,(jMin:jMax)-j+w+1);         B(i,j) = sum(F(:).*I(:))/sum(F(:));      endend\nOpenCV 3.x.x的扩展模块（ximgproc. Extended Image Processing） 也添加了JBF的 API ：\n\n#include &quot;stdafx.h&quot;#include &lt;opencv2/opencv.hpp&gt;#include &lt;ximgproc.hpp&gt; int main()&#123;\tcv::Mat src = cv::imread(&quot;data/dp.png&quot;, 1); // 原始带噪声的深度图\tcv::Mat joint = cv::imread(&quot;data/teddy.png&quot;, 0); \tcv::Mat dst;\tint64 begin = cvGetTickCount();\tcv::ximgproc::jointBilateralFilter(joint, src, dst, -1, 3, 9);\tint64 end = cvGetTickCount(); \tfloat time = (end - begin) / (cvGetTickFrequency() * 1000.);\tprintf(&quot;time = %fms\\n&quot;, time); \timshow(&quot;src&quot;, src);\timshow(&quot;joint&quot;, joint);\timshow(&quot;jointBilateralFilter&quot;, dst);\tcv::waitKey(0);      return 0;&#125;\n2.3 效果\n2.4 评价Advantages：\nPreserve edges in the smoothing process 可以在平滑处理后仍保留边缘\nSimple and intuitive 简单直观\n\nProblems：\nComplexity\n\nBrute force: $O(r^2)$\nDistributive histogram: $O(log r)$\nIntegral histogram: $O(1)$\n\n\nGradient distortion: 梯度翻转\n\nPreserves edges, but not gradients\n由于高斯滤波对边缘敏感，容易对边缘部分的像素值造成很大的变化，这就导致边缘部分可能有梯度翻转的现象\n\n\n\n\n3. Guided filter3.1 算法解析\n\n这里要说的引导滤波，某像素点的输出结果为：\n\n\nq_{i}=a_{k} I_{i}+b_{k}, \\forall i \\in \\omega_{k}\n其中，q 为输出图像，I 为引导图像，a 和 b 是当窗口中心位于 k 时该线性函数的不变系数。对于给定k个窗口，我们可以假设对于k个窗口覆盖到引导图像的每个像素都和输出图像对应的像素点有一个线性映射的关系，其中对于每一个窗口，其线性的参数是是一样的。\n$n_i$为噪声，p是q受到噪声n污染的退化图像\n\n\nn_i = p_i-q_i\n价值函数如下：\n\n\nE\\left(a_{k}, b_{k}\\right)=\\sum_{i \\in \\omega_{k}}\\left(\\left(a_{k} I_{i}+b_{k}-p_{i}\\right)^{2}+\\varepsilon a_{k}^{2}\\right)\n即：\n\n\n\\min _{(a, b)} \\sum_{i}\\left(a I_{i}+b-p_{i}\\right)^{2}+\\varepsilon a^{2}\n第二项为正则项，用于限制a的大小，$\\epsilon$为超参。类似于最下二乘法求解，式（9）的解为：\n\n\n\\begin{aligned}\na_{k} &=\\frac{\\frac{1}{|\\omega|} \\sum_{i \\in \\omega_{k}} I_{i} p_{i}-\\mu_{k} \\bar{p}_{k}}{\\sigma_{k}^{2}+\\varepsilon } \n\\end{aligned}\n\\begin{aligned}\nb_{k} &=\\bar{p}_{k}-a_{k} \\mu_{k}\n\\end{aligned}\n这里的$p_i$指的是对于原图像对应的第$i$个像素，$\\mu_k$表示对应第k个窗口引导图像的所有像素值的均值，$\\bar{p_k}$指的是对于该窗口覆盖原图像所有像素的均值，$\\sigma^2_k$指的是对于第k个窗口引导图像上像素值的方差，$\\varepsilon$​是超参。\n最后取均值可以得到式（7）的结果为：\n\n\n\\begin{aligned}\nq_{i} &=\\frac{1}{|\\omega|} \\sum_{k: i \\in \\omega_{k} \\ldots t_{p i a 0 x u e}}\\left(a_{k} I_{i}+b_{k}\\right) \\\\\n&=\\bar{a}_{i} I_{i}+\\bar{b}_{i}\n\\end{aligned}\n其中，$\\bar{a}_{i}=\\frac{1}{|\\omega|} \\sum_{k \\in \\omega_{i} } a_{k},  \\bar{b}_{i}=\\frac{1}{|\\omega|} \\sum_{k \\in \\omega_{i}} b_{k}$​​​，其物理含义是对于经过第$i$个像素的所有窗口对$a_k, b_k$​​​进行平均。\n即：\n\n\n\\begin{aligned}\na_{k} &=\\frac{\\operatorname{cov}_{k}(I, p)}{\\operatorname{var}_{k}(I)+\\varepsilon} \\\\\nb_{k} &=\\bar{p}_{k}-a \\bar{I}_{k} \\\\\nq_{i} &=\\frac{1}{|\\omega|} \\sum_{k \\mid \\in \\omega_{k}}\\left(a_{k} I_{i}+b_{k}\\right) \\\\\n&=\\bar{a}_{i} I_{i}+\\bar{b}_{i}\n\\end{aligned}3.2 总结\n总结：导引图像I与 q 之间存在线性关系，这样设定可以使导引图像提供的信息主要用于指示哪些是边缘。如果导引图告诉我们这里是边缘，最终的结果就设法保留这些边缘信息。所以，引导滤波的前提条件是：当I和q满足线性关系才有意义。\n\n\n\n其最终结果可以理解为，对于每个输出的像素为：对引导图像对应的像素的一个变换的均值。\n对于第k个经过该像素的窗口，我们可以得到对应的一个线性变换，从而得到对应的输出像素值；\n遍历这么多个窗口的变换后，我们在求一个均值，最终就会得到我们想要的答案\n这里的$\\bar{a_i}$和$\\bar{b_i}$可以理解为经过第$i$个像素窗口的系数的均值\n\n3.3 为何可行3.3.1 Def\n\\begin{aligned}\na_{k} &=\\frac{\\operatorname{cov}_{k}(I, p)}{\\operatorname{var}_{k}(I)+\\varepsilon} \\\\\nb_{k} &=\\bar{p}_{k}-a \\bar{I}_{k} \\\\\nq_{i} &=\\frac{1}{|\\omega|} \\sum_{k \\mid \\in \\omega_{k}}\\left(a_{k} I_{i}+b_{k}\\right) \\\\\n&=\\bar{a}_{i} I_{i}+\\bar{b}_{i}\n\\end{aligned}即：\n\n\\begin{aligned}\na &=\\frac{\\operatorname{cov}(I, p)}{\\operatorname{var}(I)+\\varepsilon} \\\\\nb &=\\bar{p}-a \\bar{I}\n\\end{aligned}\n$\\varepsilon$可以视为边缘保留程度的评判​\n\n3.3.2 Smoothing\n\n其意义是，如果要对图像进行平滑处理，只需增大$\\varepsilon$，这样就会使得都$\\begin{gathered}a \\approx 0 , b \\approx \\bar{p} \\end{gathered}$ 则输出图像的像素近似为高斯过滤后的输出图像，或者说此时由于是非边缘处，所以变化很小，方差和卷积自然很小，会远小于一个阈值，这个阈值就是$\\varepsilon$​。\n对于$\\bar{p}_k$指的是对于第k个窗口在原图像覆盖的所有像素值的均值，$\\overline{\\bar{p}}$则为对于这样k个窗口得出的$\\bar{p}_k$的均值化，其实结果就相当于做了滤波。​\n\n3.3.3 edge-preserving\n\n由于$\\bar{a}$和$\\bar{b}$是常数，所以求梯度是零，这里表示了对于输出图像每一个像素的梯度值可以理解为引导图像梯度值的增益或者衰落。\n在这里，也说明了超参$\\varepsilon$​可以控制a的大小，从而控制输出图像的像素值梯度，我们知道边缘处的像素值梯度会明显大于周围，这就意味着超参实质是在增强输出图像的边缘\n\n\n\n我们可以发现，$\\varepsilon$​越小，边缘越明显，当然越大，则越模糊。​\n\n3.4 与BF对比\n\n由于引导图像和输出图像的梯度值具有线性关系，所以不会在边缘处出现梯度翻转的现象，相反，其会对边缘处进行增益。\n\n","categories":["CV"]},{"title":"Histogram of Oriented Gradients","url":"/2021/08/15/cv/4.2%20HoG/","content":"Histogram of Oriented Gradients\n\n1. Some Challenges\nFind robust feature set that allows object form to be discriminated.\n\nChallenges:\n\nWide range of pose and large variations in appearances 我们想得到比较有判别力的特征，他的形态会发生比较大的变化\nCluttered backgrounds under different illumination 背景车的变化，风吹树的变化，光照的变化\n“Speed” for mobile vision 希望实时检测，对速度有很高的要求\n\n\n\n\n\nLocal object appearance and shape can often be characterized rather well by the distribution of local intensity gradients or edge directions.局部梯度增强或者边缘检测\n\n2. Histogram of Oriented Gradients\nDividing the image window into small spatial regions (cells)\nCells can be either rectangle or radial（径向的）.\nEach cell accumulating a weighted local 1-D histogram of gradient directions over the pixels of the cell. 计算每个像素的梯度，将梯度的方向量化为K个，然后将每个单元内相同的梯度方向的梯度幅值相加得到该方向的梯度强度，也就是直方图每个bin的值。\n\n\n\n3. Normalization\nFor better invariance to illumination and shadowing. it is useful to contrast-normalize the local responses before using them.\n\nAccumulate local histogram “energy” over a larger regions (“blocks”) to normalize all of the cells in the block.\n\n\n\n4. Visualizing HoG\n5. Difference between HoG and SIFT\nHoG is usually used to describe entire images. 即一下子计算了整张图的梯度SIFT is used for key point matching\n\nSIFT histograms are oriented 朝向towards the dominant gradient主方向.（提高了对旋转的鲁棒性）HoG is not.\n\nSIFT descriptors use varying scales to compute multiple descriptors. 提高对尺度鲁棒性\n\nHoGgradients are normalized using neighborhood bins（block内的直方图之和）.\n\n\n","categories":["CV"]},{"title":"LBP","url":"/2021/08/15/cv/4.3%20LBP/","content":"LBP\n\n1. Texture\nDef:\n\nIncludes: more regular patterns\n\n\n\nIncludes: more random patterns\n\n\n2. Texture-related tasks\nShape from texture\nEstimate surface orientation方向 or shape from image texture\n\n\nSegmentation/classification from texture cues\nAnalyze, represent texture\nGroup image regions with consistent texture\n\n\nSynthesis合成\nGenerate new texture patches/images given some examples\n\n\n\n\n3.Why analyze texture?Importance to perception:\n\nOften indicative of a material’s properties 表示一种材料的特性\n\nCan be important appearance cue, especially if shape is similar across objects\n\nAim to distinguish between shape, boundaries, and texture\n\n\nTechnically:\n\nRepresentation-wise, we want a feature one step above “building blocks” of filters, edges.\n\n4. Texture representation\nTextures are made up of repeated local patterns, so:\nFind the patterns\nUse filters that look like patterns (spots, bars, …)\nConsider magnitude of response\n\n\nDescribe their statistics within each local window, e.g., 用数值特征表示\nMean, standard deviation\nHistogram\nHistogram of “prototypical” feature occurrences\n\n\n\n\n\n4.1 Texture representation: example\n\n图中，分别使用x梯度算子与y梯度算子，对每个图像中的pattern进行卷积，然后取这个pattern的平均值作为输出\n\n\n\n将其投射到坐标系，可以得到，靠近原点即为平滑区域，远离则为角点\n\n\n\n坐标系中也可以看出纹理的区别\n\n\nD(a, b)=\\sqrt{\\left(a_{1}-b_{1}\\right)^{2}+\\left(a_{2}-b_{2}\\right)^{2}}\nDistance reveals how dissimilar texture from window a is from texture in window b.\n\n\n4.2 window scale\nWe’re assuming we know the relevant window size for which we collect these statistics. \n\n\n\n用不同窗口的size，提取不同尺度的纹理，类似SIFT，遍历所有size\n\nPossible to perform scale selectionby looking for window scale where texture description not changing.\n\n\n5. Filter banks\nOur previous example used two filters, and resulted in a 2-dimensional feature vector to describe texture in a window.\nx and y derivatives revealed something about local structure.\n\n\nWe can generalize to apply a collection of multiple (d) filters: a “filter bank”\nThen our feature vectors will be d-dimensional.\nstill can think of nearness, farness in feature space\n\n\n\n\n5.1 What filters to put in the bank?\nTypically we want a combination of scales and orientations, different types of patterns. 由检测不同尺度与方向的特征组合\n\n5.1.1 Multivariate Gaussian\nFilter bank：\n\nCan you match the texture to the response?\n\n5.2 Representing texture by mean abs response\n\nWe can form a feature vector from the list of responses at each pixel.\n\n5.3 d-dimensional features\nD(a, b)=\\sqrt{\\sum_{i=1}^{d}\\left(a_{i}-b_{i}\\right)^{2}}\nEuclidean distance ($L_2$​)\n\n\n\n跟这六类比较，使用SVM分类\n\n6. Local Binary Pattern (LBP)\n\n用中心像素与领域像素的大小关系来表示其特征，最终化为二值向量\nUse center pixel value to threshold the 3x3 neighborhood 用中心像素阈值，因为比它小的直接就变成零了\nResult in binary number 转化为二进制向量\nMultiplied by powers of two (Decimal) 转化为十进制，以缩短长度\nSummed to obtain a label for the center pixel -&gt; 256 different labels（每个window都用1*8的二进制数表示，其范围为0-255）\nHistogram of the labels is used as a texture descriptor 用LBP直方图统计label个数，然后用1*256的向量表示一张图片\n从而使得一张20002000的图片变成1\\256\n\n\n\n    \n\n6. 1 What are the problems? How can you be invariant to changes in scale ?\n没有考虑不同的尺度空间。\n\n解决方法\n6.2 Circle LBP\nLBP is extended to use different sizes of neighborhoods. \nLocal neighborhoods is defined as a set of sampling points.\npoints evenly （平均） spaced on a circle centered at the labeled pixel.\n(P,R) , P = number of sampling points , R = radius\n\n\n6.3 Uniform LBP\nStandard LBP has $2^n$​​ patterns for n sampling points 因为最终会表示成二进制数的直方图\nHistogram dimension becomes high when n is increased\nSensitive to noise\n\n\nUniform patterns has at most 2 bitwise transitions in binary pattern. 当0变成1就被噪音影响了\n\nHistogram assigns separate bin for every uniform pattern.\n\nHistogram assigns a single bin for all non-uniform pattern.\nIn FERET dataset, (8,1) neighborhoods : 90.6 percent of patterns are uniform.\n\n特征空间还是过长了\n\nLBP是比较中间像素与领域像素的比较，但如果有噪声1就变成0了，即对噪声过于敏感了\n如果有大于两次跳变，就是不平滑，所以我们把所有超过两次跳变的归为一类，在直方图统一计算bin，而小于等于两次跳变的，即为uniform LBP，这些可以分开计数，从而压缩噪声\n\n\n\n步骤：\nStep 1: facial image is divided into local regions (blocks). {R0, R1, …, Rm-1} (pixel-level locality)\n\nStep 2: Extract LBP histogram for each region. (regional-level locality)\n\nStep 3: Concatenated（连接） all histograms into a spatially enhanced histogram with length of m x n (n is length of a single LBP histogram). (global-level locality)\n\nm blocks\n\n\nStep 4: Chi-square distance\n\n\n7. Summary\nTexture is a useful property that is often indicative of materials, appearance cues\nTexture representations attempt to summarize repeating patterns of local structure\nFilter banks useful to measure redundant（冗余的） variety of structures in local neighborhood\nFeature spaces can be multi-dimensional\n\n\nLocal binary patterns describe small-scale appearance (textures) of the image\n\n","categories":["CV"]},{"title":"知识工程简介","url":"/2021/08/15/knowledge%20engineering/1.%20%E7%9F%A5%E8%AF%86%E5%B7%A5%E7%A8%8B%E7%AE%80%E4%BB%8B/","content":"知识工程简介\n朱启鹏 581193041. What is a knowledge graph? what is its role in machine intelligence?What is a knowledge graph?\n\n知识图谱也被称为语义网络（semantic network），它表示了现实世界中的实体事物的一种网络关系，并具体展示了这些事物之间的关系，而其中的实体事物可以理解为现实中的一些对象、事件、情况或概念。而这些信息通常被存储在图数据库（Graph database）中，并可视化为图结构。\n\n具体而言，一个知识图谱由三个主要部分组成，分别为：节点、边、标签。任何物体、地点、或者节点等都可以是一个节点。而边定义了节点的关系。\n\n对于网络中的每一条知识，构成一个三元组（subject, predicate, object），所以知识图谱也可以简单理解为RDFS这样的模型，对于三元组\n\n\n\n  ​        A 表示主语，B表示谓语，C表示宾语。\n\nRDF：Triple-based Assertion Model，具体如下图展示\nRDF Graph: Directed Labeled Graph\n\n\n\n\nRDFS：Simple Vocabulary and Schema，可以由下图展示：\n\n\n  这个模型的下半部分为一些具体的实例，而模型的上半部分为这些实力的抽象类型，其中我们可以知道人工智能这个实例是属于计算机科学这个类别，而计算机科学为信息科学的子类，而这个图谱的表示了图灵的研究领域为人工智能这样一条知识。\nwhat is its role in machine intelligence?\n下图为强人工智能金字塔示意图，我们可以看到知识工程位于金字塔的第二层\n\n\n\n下图为智能AI与有知识的AI对比的示意图\n\n\n\n由如上两张图我们可以得出，要想实现真正的人工智能，发展知识图谱是十分重要的一部分，其地位与智能型AI平分秋色，或者说是智能型AI的一个重要基础。\n\n数十年来，人工智能知识表示和感知推理一直是人工智能 (AI) 的基石。而知识图谱 (KG) 是一种强大的数据结构，其主要特点为以图的格式储存并表示表示信息。DBpedia （一个开源的知识图谱）将知识图谱定义为“一种特殊的数据库，它以机器可读的形式存储知识，并提供一种收集、组织、共享、搜索和利用信息的手段。”最重要的是，知识图谱有助于其任何数据点之间的关系推理。 通过这样的工具，使得人工智能能够对海量的知识进行存储以及表示，并且实现像人一样的推理以及得出结论。\n\n\n2.List at least 3 techniques of Knowledge Graph, using examples in real life (do not use examples in this ppt)\n（1）Reasoning based on Rules\n\n\n\n(2)  Knowledge-based Question Answering旅游内容推荐旅游产品非常多样化，所以很依赖推荐。我们往往会抓住旅游产品和用户的浅层特征去推荐，而忽略了一些深层次的特征。当用户和某个知识图谱实体产生关联，我们可以用知识图谱去补充用户特征。\n\n\n\n  如上图所示，当我们通过数据挖掘发现用户A的兴趣点是海岛和五星级酒店后，我们可以通过知识图谱关联出相关的团队游产品。知识图谱提供的信息可以作为推荐系统的一个重要维度，参与下一步的计算，为精准推荐增加一块砝码。\n(3)Knowledge Extraction半结构化数据抽取\n无结构化数据抽取\n","categories":["KnowledgeEngineering"]},{"title":"Graph Database","url":"/2021/08/15/knowledge%20engineering/14.%20Graph%20Database/","content":"Graph Database\n\n1.  Definition\nA database for storing and querying data in a data structure like graphs Composition\n\nNode \n\nSpecific entities, such as the movie “Harry Potter 2“,\n\n\nRelation \nThe connection between entities.\n\n\nLabel \nAbstract concepts that group similar nodes together, such as movie characters.\n\n\nProperty \nSpecific information about the node or relationship, such as name, age, etc.\n\n\n\n2. Neo4j2.1 Introduction\n知识图谱存储工具：RDF4J、Jena、Neo4j\n\nNeo4j is a robust, scalable, high-performance open source graph database\n\n\n\n2.2 Neo4j-Important element2.2.1 Structural Node\nID: unique identifier\nLabel: a form of pattern syntax to group nodes together\nMap(properties): \nkey: property name \nvalue: property value\n\n\n\n\n\n2.2.2  Properties\nKey: property name\nValue: property value\n\nValue Type\nBoolean\nByte\nShort\nInt\nLong\nFloat\nDouble\nChar\nString\nArray\n\n\n2.2.3 Structural Relation\nID\nType\nMap(properties)\nID of the start node\nID of the end node\n\n\n\n2.2.4 Path\nAn sequence of nodes and relationships\nComposition \nat least one node \nconnected relationships\n\n\nOften as a result of a query or traversal\n\n\n\nA path from nodeA to nodeB\n\n\n3. Cypher3.1 Introduction\nCypher is a declarative graph query language that allows for expressive and efficient querying, updating and administering of the graph.\n\n\n\nNote： Keywords are not case sensitive\n\n3.2 Cypher-Create3.2.1 Create a node\n\n\n\nCREATE(\tnode:Movie\t&#123;\t\ttitle:&quot;The American President&quot;\t&#125;)\n3.2.2 Create relationshipsCREATE(&lt;node1-name&gt;:&lt;node1-label-name&gt;&#123;&lt;define-properties-list&gt;&#125;)-[&lt;relationship-name&gt;:&lt;relationship-label-name&gt;&#123;&lt;define-properties-list&gt;&#125;]-&gt;(&lt;node2-name&gt;:&lt;node2-label-name&gt;&#123;&lt;define-properties-list&gt;&#125;)\n\n\nExercise\nCreate the relationship using cypher.\n\n\nCREATE\t(n1:Person &#123;name:&#x27;Oliver Stone&#x27;&#125;)\t-[r:DIRECTED]\t-&gt;(N2:Movie&#123;title:&quot;Wall Street&quot;&#125;)\n3.3 Cypher-MATCH\nMatch\n\nMATCH    (&lt;node-name&gt;:&lt;label-name&gt;    &#123;    &lt;Property1-name&gt;:&lt;Property1-Value&gt;    ........    &lt;Propertyn-name&gt;:&lt;Propertyn-Value&gt;    &#125;)    -[&lt;relationship-name&gt;:&lt;relationship-label-name&gt;&#123;&lt;define-properties-list&gt;&#125;]    -&gt;(&lt;node-name&gt;:&lt;label-name&gt;    &#123;    &lt;Property1-name&gt;:&lt;Property1-Value&gt;    ........    &lt;Propertyn-name&gt;:&lt;Propertyn-Value&gt;    &#125;)RETURN ........\n\nReturn\nVariable, like node-name: node\nSpecific value, like node-name.property: node.name\n\n\n\n3.3.1 Match node\n查询所有节点\n\nMATCH (n) RETURN n\n\n\n查询所有电影title\n\nMATCH (movie:Movie)RETURN movie.title\n\n3.3.2 Match related nodes\n名叫Oliver Stone相关联的事物标题\n\nMATCH (director &#123;name: &#x27;Oliver Stone&#x27;&#125;)--(movie) RETURN movie.title\n\n\n\n— : connected relationship of unknown relationship type and direction\n\n与人物Oliver Stone相关联的电影标题\n\n\nMATCH (:Person &#123;name: &#x27;Oliver Stone&#x27;&#125;)--(movie:Movie) RETURN movie.title\n\n\n与人物Oliver Stone相关联的关系类型\n\nMATCH (:Person &#123;name: &#x27;Oliver Stone&#x27;&#125;)-[r]-&gt;(movie) RETURN type(r)\n\n\n-&gt; / &lt;- : directed relation, &gt; towards the tail entity\n\n出演名为‘Wall Street’电影的演员姓名\n\n\nMATCH (wallstreet:Movie &#123;title: &#x27;Wall Street&#x27;&#125;)&lt;-[:ACTED_IN]-(actor) RETURN actor.name\n\n3.3.4 Match on multiple relationship types\n参演或导演名为Wall Street的电影的所有人物\n\nMATCH (wallstreet:Movie &#123;title: &#x27;Wall Street&#x27;&#125;)&lt;-[:ACTED_IN|:DIRECTED]-(person:Person) RETURN person.name\n\n3.3.5 Match on relationship type and use a variable\n名为Wall Street的电影的中所有的角色\n\nMATCH (wallstreet:Movie &#123;title: &#x27;Wall Street&#x27;&#125;)&lt;-[r:ACTED_IN]-(actor) RETURN r.role\n3.4 Match and Create\nwhen nodes already exist, we can use MATCH first, then CREATE\n\n\nMATCH \t(charlie:Person &#123;name: &#x27;Charlie Sheen&#x27;&#125;),     (rob:Person &#123;name: &#x27;Rob Reiner&#x27;&#125;) CREATE     (rob)-[:TYPE INCLUDING A SPACE]-&gt;(charlie)\nExercise\n查询参演名为‘The American President’的所有演员姓名\n\n\nMATCH\t(person:Person)-[:ACTED_IN]\t-&gt;(movie:Movie&#123;title:&quot;The American President&quot;&#125;)RETURN person.name\n\n查询包含角色’Card Fox’的电影名\n\n\nMATCH\t(actor)-[:ACTED&#123;role:&quot;Card Fox&quot;&#125;]\t-&gt;(movie:Movie)RETURN movie.title\n3.5 Cypher-DELETE\nDelete single node\nDelete all nodes and relationships\nDelete a node with all its relationships\nDelete relationships only\n\n\n3.5.1 Delete single node\n删除名为UNKNOWN的节点\n\nMATCH (n:Person &#123;name: &#x27;UNKNOWN’&#125;) DELETE n\n3.5.2 Delete all nodes and relationships\n删除数据库中所有节点及与其相连的关系\n\nMATCH (n) DETACH DELETE n\n3.5.3 Delete a node with all its relationships\n删除名为Andy的节点和与其相连的所有关系\n\nMATCH (n &#123;name: &#x27;Andy’&#125;) DETACH DELETE n\n3.5.4 Delete relationships only\n删除Andy的所有KNOWS关系\n\nMATCH (n &#123;name: &#x27;Andy&#x27;&#125;)-[r:KNOWS]-&gt;() DELETE r\nExercise\n删除人物年龄为34岁的所有关系\n\n\nMATCH (person:Person &#123;age:34&#125;)-[r]-&gt;()DELETE r\n3.6 Cypher-UPDATE :3.6.1 SET\nSET can be used with a map — provided as a literal, a parameter, or a node or relationship — to set properties.\n\nUpdate a property\n\n\nMATCH (n &#123;name: &#x27;Andy&#x27;&#125;) SET n.age = toString(n.age) RETURN n.name, n.age\n\n\n3.6.2 multiple properties using one SET clauseMATCH (n &#123;name: &#x27;Andy&#x27;&#125;) SET n.position = &#x27;Developer&#x27;, n.surname = &#x27;Taylor&#x27;\n3.6.3 Replace all properties using a mapMATCH (p &#123;name: &#x27;Peter&#x27;&#125;) SET p = &#123;name: &#x27;Peter Smith’, \tposition: &#x27;Entrepreneur&#x27;&#125; RETURN p.name, p.age, p.position\n\nExercise\n更新George的年龄为28\n\n\nMATCH (p &#123;name:&#x27;George&#x27;&#125;)SET p = &#123;name: &#x27;George&#x27;\tage: 28&#125;\nExercise\n\nWrite cypher to get the second graph\n\nMATCH \t(actor1:Actor &#123;name: &#x27;Anthony Hopkins&#x27;&#125;),\t(actor2:Actor &#123;name: &#x27;Hitchcock&#x27;&#125;),\t(movie:Movie &#123;title: &#x27;Hitechcock&#x27;&#125;)CREATE\t(actor1)-[r:ACTS_IN]\t-&gt;(movie)DELETE actor2\n\nQuery all relationship types connected by actor Anthony Hopkins\n\nMATCH\t(actor:Actor &#123;name: &#x27;Anthony Hopkins&#x27;&#125;)-[r]-()RETURN type(r)\n","categories":["KnowledgeEngineering"]},{"title":"1. XML","url":"/2021/08/15/knowledge%20engineering/2.%20XML%20Ref%20and%20Refs/","content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n1.1 Def:\nA markup language for documents containing structured information.\n用于数据交换的一种标记语言\n\n\n1.2 Comparison：1.2.1 XML：\nExtensible set of tags  标签可以自定义\n\nContent orientated  数据与格式分离\n\nStandard Data infrastructure  不允许出错\nAllows multiple output forms  有多种输出格式\n1.2.2 HTML：\nFixed set of tags  标签无法自定义\n\nPresentation oriented  数据与格式镶嵌\nNo data validation capabilities 允许有error显示\nSingle presentation  单一输出格式\n\n1.3 XML Syntax\nempty elements can be abbreviated: e.g.  can be written as \nthe outermost element is called root element (there is only one)\n\nExample：&lt;?xml version=\"1.0\" encoding=\"GB2312\" ?&gt; &lt;!--版本号，编码--&gt;&lt;author&gt;&lt;!--开始tag--&gt;&lt;firstName&gt;Guilin&lt;/firstName&gt;\t&lt;lastName&gt;Qi&lt;/lastName&gt;\t&lt;email&gt;gqi@seu.edu.cn&lt;/email&gt; &lt;!--子元素--&gt;\tThis is some text inside an XML element. &lt;!--text--&gt;&lt;/author&gt; &lt;!--end tag--&gt;\n1.4 XML Attributes:1.4.1 EP1:&lt;City ZIP=“210000”&gt; Nanjing&lt;/City&gt;\n​    \n1.4.2 EP2:&lt;author&gt;\t&lt;firstName&gt;Guilin&lt;/firstName&gt;\t&lt;lastName&gt;Qi&lt;/lastName&gt;\t&lt;email&gt;gqi@seu.edu.cn&lt;/email&gt;\tThis is some text inside an XML element.&lt;/author&gt;\n等价于\n&lt;author email=“gqi@seu.edu.cn”&gt;\t&lt;firstName&gt;Guilin&lt;/firstName&gt;\t&lt;lastName&gt;Qi&lt;/lastName&gt;\tThis is some text inside an XML element.&lt;/author&gt;\n1.5 规范：Authoring guidelines:\n\nAll elements must have an end tag. 标签有头有尾\nAll elements must be cleanly nested (overlapping elements are not allowed). 所有元素必须不能重复\nAll attribute values must be enclosed in quotation marks. \nEach document must have a unique first element, the root node.\n大小写敏感\n\nExercise：&lt;book&gt;\t&lt;title&gt;Knowledge Graph&lt;/Title&gt; &lt;!--尾标签有误--&gt;\t&lt;author&gt;\t\t&lt;firstName&gt;Guilin&lt;/firstName&gt;\t\t&lt;lastName&gt;Qi&lt;/lastName&gt;\t\t&lt;email&gt;gqi@seu.edu.cn&lt;/email&gt;\t\tThis is some text inside an XML element.\t&lt;/author&gt;\t&lt;author&gt;\t\t&lt;firstName&gt;Tianxing&lt;lastName&gt;\t\t&lt;/firstName&gt;Wu&lt;/lastName&gt; &lt;!--嵌套出错--&gt;\t&lt;email&gt;tianxingwu@seu.edu.cn&lt;/email&gt;&lt;/author&gt;&lt;!--缺少&lt;/book&gt;--&gt;\n1.6 XML插入HTML&lt;文章&gt; \t&lt;段落&gt;&lt;![CDATA[ \t\t&lt;html&gt; &lt;head&gt;&lt;title&gt;&lt;/title&gt;&lt;/head&gt; \t\t\t&lt;body&gt;            \t&lt;h1&gt;东南大学&lt;/h1&gt;             &lt;/body&gt;         &lt;/html&gt;]]&gt;      &lt;/段落&gt;  &lt;/文章&gt;\n1.7 XML Namespaces：\n为了解决属性相同产生歧义而提出\n\n&lt;h:table xmlns:h=\"http://www.w3.org/TR/html4/\"&gt;\t&lt;h:tr&gt;\t\t&lt;h:td&gt;Apples&lt; / h:td&gt;\t\t&lt;h:td&gt;eananas&lt; / h:td&gt;\t&lt; / h:tr&gt;&lt;/ h:table&gt;\n\nDefining the default namespaces:\n\n&lt;table xm1ns=\"http:// www.w3.org/TR/htm14/ \"&gt;\t&lt;tr&gt;\t\t&lt;td&gt;Apples&lt;/td&gt;\t\t&lt;td&gt;Bananas&lt;/td&gt;\t&lt;/tr&gt;&lt;/table&gt;\n\n1.8 URI format:\n1.9 XML Schema:\n\n由于XML过于灵活，所以需要定义一种规范，以便于数据交换\n\n下面为示例代码：\n\n\n&lt;?xml version=“1.1” encoding=“utf-16”?&gt;&lt;xsd:schema xmlns:xsd=“http://www.w3.org/2001/XMLSchema”&gt; &lt;!--可以理解为定义了一个格式--&gt;\t&lt;xsd:element name=“author” type=“xsd:string” \t\t\t\tminOccurs=“1” maxOccurs=“unbounded”&gt;  &lt;!--定义了元素--&gt;\t\t&lt;xsd:attribute name=“email” type=“xsd:string”use=“required”&gt; &lt;!--具体一些属性--&gt;\t\t&lt;xsd:attribute name=“homepage” type=“xsd:anyURI” use=“optional”&gt;\t&lt;/xsd:element&gt;&lt;/xsd:schema&gt;\n2 RDF2.1 Def:\n对网站源数据进行标注，用于机器可读的数据交换。\n\nThe data model of Semantic Technologies and of the Semantic Web\n\n\n\n2.2 URI\n为了解决命名模糊问题，RDF也采用URI定义source的形式\n\n\n2.3 QName2.3.1 Def:used in RDF as shorthand for long URIs (IRIs)\nExample:\n\n\n\n既可以用QNames形式，也可以用URI形式\n\n\n2.4 RDF Triple (Statement):\n\n可以发现S P O都可能是Resource\n\n2.4.1 ResourcesDef: IRIs  类似命名空间2.4.2 LiteralsDef:  类似一个值，放在尖叫括号外\ndata values; \nencoded as strings; \ninterpreted by datatypes; \ntreated the same as strings without datatypes, called plain literal; \nA plain literal may have a language tag; \nDatatypes are not defined by RDF, but usually from XML Schema.\n\n\n大致意思是：literals分为有类型的和无类型的，其有类型的类型一般来自于命名空间，对于无类型的，被称为plain literals；其中plain literals又可以被语言标签标注，用于解释literals的语言类型，当然也可以不进行不标注，但注意这两种literals是不同的。\n\n&lt;!--Typed Literals:--&gt;“Beantown”^^xsd:string“The Bay State” ^^xsd:string&lt;!--Plain literal and literals with language tags:--&gt;“France” “France”@en “France”@fr“法国”@zh “Frankreich”@de&lt;!--Equalities for Literals:--&gt; “001”^^xsd:integer = “1”^^xsd:integer “123.0”^^xsd:decimal = “00123”^^xsd:integer (based on datatype hierarchy)&lt;!--上面这两种形式等价，因为integer是decimal的父节点--&gt;\n\n\nDoes the datatype “德国” equals to “德国” @ zh ?\nAnswer：不相同，因为他们位于的层次结构不同\n\n\n\nBlank nodeDef: unnamed resource or complex node (later)无名的资源或者复杂的节点，简单来说就是图上空的节点，语义较模糊的位置\nRepresentation of blank nodes is syntax-dependent:underline+colon+ID (Turtle syntax): _:xyz, _:bn; 下划线加冒号加ID\n\n\n2.5 RDF Syntax2.5.1 Turtle\nlist as S P O triples (easy to read)将主谓宾依次列出\nIRIs are in  IRIs在&lt;&gt;中，也就是sources\ntriples end with a full-stop .以 . 结束\nwhitespaces are ignored空白可以省略\n\n\n\nIRIS直接表示\n\n&lt;http://dbpedia.org/resource/Massachusets&gt; &lt;http://example.org/terms/captial&gt; &lt;http://dbpedia.org/resource/Boston&gt; . &lt;http://dbpedia.org/resource/Massachusets&gt; &lt;http://example.org/terms/nickname&gt; “The Bay State”. &lt;http://dbpedia.org/resource/Boston&gt; &lt;http://example.org/terms/inState&gt; &lt;http://dbpedia.org/resource/Massachusets&gt;. &lt;http://dbpedia.org/resource/Boston&gt; &lt;http://example.org/terms/nickname&gt; “Beantown”. &lt;http://dbpedia.org/resource/Boston&gt; &lt;http://example.org/terms/population&gt; “642109”^^xsd:integer.\n\nQName表示：\n\n@prefix db: &lt;http://dbpedia.org/resource/&gt; @prefix dbo: http://example.org/terms/ #预定义QNamesdb:Massachusets dbo:capital db:Boston . db:Massachusets dbo:nickname “The Bay State” . db:Boston dbo:inState db:Massachusets . db:Boston dbo:nickname “Beantown” . db:Boston dbo:population “642109”^^xsd:integer .\n\nQName简化书写条例：\n\nGrouping of triples with the same subject using semi-colon ‘;’; 主语相同可用;间隔\n\nGrouping of triples with the same subject and predicate using comma ‘,’.主语谓语相同可用,间隔\n\n\n\n\n@prefix db: &lt;http://dbpedia.org/resource/&gt; @prefix dbo: http://example.org/terms/ db:Massachusets dbo:captial db:Boston ; \t\t\t\tdbo:nickname “The Bay State” . db:Boston dbo:inState db:Massachusets ; \t\t  dbo:nickname “Beantown” ; \t\t  dbo:population “642109”^^xsd:integer .\n2.5.2 RDF/XML:Def: RDF is originally designed on basis of XML (data exchange format on the Web)\na lot of tools and libraries support XML\n\nNamespaces are used for disambiguating tags; \n\nTags belonging to the RDF language come with a fixed namespace, usually abbreviated “rdf”. rdf有固定的命名空间\n\n\n\n\n可以这么理解，首先要说明这个部分为RDF语句，以及声明这部分所需要使用的命名空间；\n\n然后，定义主体描述内容：主语 谓语 宾语\n\n对于rdf:Description的element包含对resource的描述，并被rdf:about识别\nex:publishedBy也蕴含了resource常常用作谓语\n\n&lt;rdf:Description rdf:about=\"http: //semantic-web-book.org/uri\"&gt; &lt;!--主语--&gt;&lt;ex:title&gt;Foundations of Semantic web Technologies&lt;/ex:title&gt; &lt;!--谓语 宾语（literals）--&gt;&lt;ex :publishedBy&gt; &lt;!--并列谓语--&gt;\t&lt;rdf : Description rdf:about=\"http://crcpress.com/uri\"&gt;      &lt;!--上一级的宾语 也是下一级的主语 此处为嵌套结构--&gt;\t\t&lt;ex : name&gt;CRC Press&lt;/ex :name&gt;         &lt;!--宾语（literals）--&gt;\t&lt;/rdf : Description&gt;&lt;/ex :publishedBy&gt;&lt;l rdf :Description&gt;\n\n\n2.6 RDF表示N元关系\n\n用一个节点中介\n\n\n\n\n利用一个空节点\n\n\n&lt;rdf :Description rdf :about=\"http: //example.org/Chutney\"&gt;\t&lt;ex :hasIngredient rdf:nodeID=\"id1\"/&gt; &lt;!--不具体指明宾语，而是用属性nodeID定义一个字符串--&gt;&lt;/rdf : Description&gt;&lt;rdf : Description rdf :nodeID=\"id1\"&gt;    &lt;!--紧接上文的nodeID，称为主语resource--&gt;\t&lt;ex : ingredient rdf :resource=\"http : / /example.org/greenMango\" /&gt;    &lt;ex : amount&gt;1lb&lt;/ex : amount&gt;&lt;/rdf : Description&gt;\n2.7 RDF vs XML\nIRIs solve the problem of term meaning.   IRIs解决命名重复问题\nTriple-based data model describe relations or properties among terms. RDF解决数据间的关系\n\nTriple is good and easy to use, but cannot cover all kinds of knowledge! Semantic Web Knowledge Graph\n2.8 Exercise\n@prefix sw: &lt;http://www.semanticweb.org/ontology-9/&gt;sw:John sw:is_a sw:professors;\t\tsw:has_id sw:987654321;\t\tsw:has_name sw:John Doe.\n3. RDFs3.1 Def\n为RDF data提供词汇集，帮助定义RDF schema\nallows for specifying schema knowledge; ∙\nMothers are female \nOnly persons write books \nis a part of the W3C Recommendation.\n\n\n\n\nRDFs为RDF定义一些抽象类别词汇，以便于规范RDF的使用\n为何不用XML Schema?\n因为XML Schema没有语义semantics\n因为其引用的things不能超过document\n\n\n\n3.2 RDFS: Class and Instance\nGiven a triple: \n\nex:Semantic     Web rdf:type     ex:Textbook .\n\nInstance and class names cannot be distinguished syntactically with IRIs   \n但是rdf不能显示表示这是一种抽象的关系\n\n\n\nRDFS helps explicitly state that a resource denotes a class:\n\nrdfs:Class is the“class of all classes”.\n\n\n\n\n3.3 RDFS: Class Hierarchy (Taxonomy)3.3.1 rdfs: subClassOf is also reflexive: 自反性\nex:Textbook rdfs: subClassOf ex:TextBook . \nex:Book rdfs:subClassOf ex:Book .\n\n3.3.2 rdfs: subClassOf can derive class equivalence:等价性\n\n3.4 RDFs可以缩写\n\n大致意思是可以用rdfs:Class同时代替rdf:Description与rdf:type，大致是因为rdfs直接包含了class，直接表面了该定义下为一个类\n\n\n3.5 RDFS: Property and Property Hierarchy\n可以进行简答推理\n\n\n3.6 RDFS: Property Restrictions\n谓语有值域与定义域：即主语的取值范围以及宾语的取值范围\n\n\n\n谓词的值域可以进行交集与并集\n\n\n3.7 RDFS: Reification\n用空节点表示一种复杂的关系\n\n\n\nRepresent the following sentence graphically by means of the blank node:Wikipedia said that Tolkien wrote Lord of the Rings.\n\n\n3.8 Example: Reasoning with RDFS\nGiven：\n\nex:happilyMarriedWith rdfs:subPropertyOf ex:isMarriedTo . ex:isMarriedTo rdfs:domain ex:Person . ex:isMarriedTo rdfs:range ex:Person . ex:pascal ex:happilyMarriedWith ex:lisa .\n\n可推出：\n\nex:pascal ex:isMarriedTo ex:lisa . ex:pascal rdf:type ex:Person . ex:lisa rdf:type ex:Person .\nExercise：What can be inferred from the following triples using RDFS semantics?\nex:Postgraduate_student rdfs:subClassOf ex:Student ex:Professor rdfs:subClassOf ex:Academic_staff ex:Supervise rdfs:domain ex:Professor ex:Supervise rdfs:range ex:Postgraduate_student ex:John ex:Supervise ex:Mary\nex:John rdf:type ex:Professor ex:Mary rdf:type ex:Postgraduate_student ex:John rdf:type ex:Academic_staffex:Mary rdf:type ex:Studentt\n","categories":["KnowledgeEngineering"]},{"title":"OWL","url":"/2021/08/15/knowledge%20engineering/3.%20OWL/","content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n1. Introduction to Knowledge Graph Representation (II)1.1 Write RDF in Turtle for the part circled by the red line. All resources are defined in the namespace, and the prefix of its QName is “sw”.\nsw:John sw:is_a sw:professors . sw:John sw:has_name “John Doe” . sw:John sw:has_id “987654321” .\nsw:John sw:is_a sw:professors ; \t\tsw:has_name “John Doe” ; \t\tsw:has_id “987654321” .\n1.2 Represent the following sentence graphically by means of the blank node: Wikipedia said that Tolkien wrote Lord of the Rings.\n1.3 What can be inferred from the following triples using RDFS semantics?1) ex:Postgraduate_student rdfs:subClassOf ex:Student 2) ex:Professor rdfs:subClassOf ex:Academic_staff 3) ex:Supervise rdfs:domain ex:Professor 4) ex:Supervise rdfs:range ex:Postgraduate_student 5) ex:John ex:Supervise ex:Mary\n3)5) → ex:John rdf:type ex:Professor (6) 4)5) → ex:Mary rdf:type ex:Postgraduate_student (7) 2)6) → ex:John rdf:type ex:Academic_staff 1)7) → ex:Mary rdf:type ex:Student\n1.4 New Exercises:1.4.1 Use a RDF graph to represent the sentence “John is working in a company located in Sydney” (bland node can be considered).\n这里的空白节点是一个实例（区别于上一题是一个三元组）\n\n\n\n不能用located in修饰company，因为这里的company只是一个概念，不是所有company都在Sydney\n\n1.4.2 2. Does the datatype “3.14”^^xsd:string equal to “+03.14”^^xsd:string? If not, why?Answer:不是；这是字符串形式，字符串要考虑每个character，明显两个字符串不一样。\n2. owl2.1 Def\nA more expressive vocabulary definition language.词汇定义语言。\n\n2.2 Why do we need OWL?\nRDFS does not have constraints on property domain and range: 对谓词没有约束\nFor “Human”, the range of “hasChild” should be “Human”; \nFor “Elephant”, the range of “hasChild” should be “Elephant”.\n\n\nRDFS does not have cardinality constraints: 没有一个基数约束\nThe number of “Father” for one person should be one; One person has at most two parent. (property: “hasParent”)\n\n\nRDFS does not have descriptions on property characteristics:\ntransitivity 传递性, symmetry对称性, functionality函数性, relations to other properties (inverse, equivalence)…\n\n\nRDFS lacks definitions on equivalence: 缺少对等价的定义\nequivalence on individuals, classes, and properties\n\n\n\n2.3 What is Ontology?\n\n可以理解为就是Schema。\n\nDef:\n\nformal : machine-readable 机器可理解\nexplicit : unambiguous vocabulary definitions 没有歧义\nshared : widely accepted and reused 公认的\nconceptualization ：特定领域建模\n\n整体理解就是一种机器可理解、清晰无歧义、公认的、特定领域建模的语言\n2.4 Details2.4.1 Equivalence between classes, individuals, and properties: 一些等价关系exp:Athlete owl:equivalentClass exp:SportsPlayer . exp:obtain owl:equivalentProperty exp:acquire . exp:SportsPlayerA owl:sameAs exp:Thomas .\n2.4.2 Disjointness between classes: 类别不相交关系exp:Man owl:disjointWith exp:Woman .\n2.4.3 Intersection of classes: 交集exp:Mother rdf:type owl:class ; \t\t   owl:intersectionOf (exp:Woman exp:Parent) .\n\n这里括号里面可以有更多类别\n\n2.4.4 Negation of a class: 类别取反exp:ABC rdf:type owl:class ; \t\towl:complementOf exp:Meat .\n2.4.5 Property definitions: object property and data property.对属性的定义（谓词）ex:friendOf rdf:type owl:ObjectProperty . ex:age rdf:type owl:DataProperty .\n2.4.6 Transitive property: 传递性exp:ancestor rdf:type owl:TransitiveProperty .\n\nGiven triples\n\nexp:Thmoas exp:ancestor exp:Jimmy . exp:Jimmy exp:ancestor exp:Michael .\n\nThen what can we infer from them?\n\nexp:Thmoas exp:ancestor exp:Michael .\n2.4.7 Functional property:exp:hasMother rdf:type owl:FunctionalProperty .\n\nPlease explain using natural language:Everyone has only one mother.\n\n谓词的函数特性表示他的range取值只能有一个\n\n\n2.4.8 inverse functional property 具有反函数特性exp:postgraduateSupervisor rdf:type owl:InverseFunctionalProperty .\n\nPlease explain using natural language:\n\nEach post-graduate student has only one supervisor.\n\n这表示谓语的定义域只能有一个\n\n2.4.9 Symmetric property 对称特性exp:friendOf rdf:type owl:SymmetricProperty .\n\nGiven a triple\n\nexp:Thmoas exp:friendOf exp:Lisa .\n\nThen what can we infer from it?\n\nexp:Lisa exp:friendOf exp:Thmoas .\n2.4.10 Inverse relations between propertiesexp:ancestor owl:inverseOf exp:descendant .\n\nGiven a triple\n\nexp:Thmoas exp:ancestor exp:Jimmy .\n\nThen what can we infer from it?\n\nexp:Jimmy exp:descendant exp:Thmoas .\n2.4.11 Constraints on properties: universal quantifier ∀任意（谓词限定词）exp:Person rdf:type owl:Restriction ; \t\t   owl:onProperty exp:hasMother ; #表示Person可以充当hasMother的主语           owl:allValuesFrom exp:Women . #表示前一句谓语的range的取值范围\n\nWhat do we know from the above triples?\nIf the subject of exp:hasMother comes from the class exp:Person, then the object can be only from the class exp:Women\n\n2.4.12 Constraints on properties: existential quantifier ∃ 存在exp:SemanticWebPapers rdf:type owl:Restriction ; \t\t\t\t\t  owl:onProperty exp:publishedIn ; \t\t\t\t\t  owl:someValuesFrom exp:AAAIPapers . #存在一部分的发表在AAAIPapers\n\nWhat do we know from the above triples?\nIf the subject of exp:publishedIn comes from the class exp:SemanticWebPapers, then the object may be (at least one) from the class exp:AAAIPapers.\n\nA part of the Semantic Web papers were published in AAAI.\n\n\n2.4.13 Constraints on properties: cardinalities 基数约束exp:Person rdf:type owl:Restriction ; \t\t   owl:onProperty exp:hasParent ; \t\t   owl:cardinality “2”^^xsd:integer . #限定hasParent的range只能有两个\n\nPlease explain using natural language:\nEach person should have two parents.\n\ncardinality也可以换成owl:maxCardinality/ owl:minCardinality，表示最多或者最少\n\n\n2.5 Exercise2.5.1\nPlease explain the following triples using natural language:\n\n@prefix sw: &lt;http://semanticweb.org/&gt; @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; . sw:Giraffe rdfs:subClassOf _:x ._:x rdfs:subClassOf sw:Animal ; \trdf:type owl:Restriction ; \towl:onProperty sw:eat ; \towl:allValuesFrom sw:Leaf .\n\nGiraffe is a kind of animal which eats leaves.\n\n2.5.2\nPlease write the corresponding RDF triples (in Turtle) of the following sentence: “ChildlessPersons are the persons who are not parents”. Note that classes “ChildlessPerson”, “Person”, “Parent” are defined in the namespace http://semanticweb.org/ with the prefix “SW”.\n\n_:x0 rdf:type owl:class;\t owl:complement owl:Parent._:x1 rdf:type owl:class;\t owl:intersectionOf (sw:Person _:x0);\t owl:equivalentClass sw:ChildlessPerson.\n2.6 OWL Sub-Languages\nOWL Lite \nOWL DL \nOWL Full\n\n\nOWL2\n","categories":["KnowledgeEngineering"]},{"title":"Description Logic","url":"/2021/08/15/knowledge%20engineering/4.%20Description%20Logic/","content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n1. Why adding such definitions in OWL? How to know them?\nWe expect the vocabulary definition language should have following features: \nEasy to use and understand \nFormal representation \nSufficient expressive ability \nSupport automated reasoning (decidable with suitable complexity)\n\n\n\n2. Propositional Logic2.1 Proposition2.1.1 Def\nA Proposition is a statement which has truth value: it is either true (T) or false (F).\n\n可以判断对错的陈述句\n\n\n2.1.2 Example\nWhich of the following are propositions?\n17 + 25 = 42 (√) \nJuly 4 occurs in the winter in the Northern Hemisphere. (√) \nThe population of the United States is less than 250 million. (√) \nIs the moon round  (√)\n\n\n\n2.2 Propositional Logic2.2.1 Def:\ndeals with propositions (which can be true or false) and relations between propositions, including the construction of arguments based on them.\n即处理命题以及命题之间的关系。\n\n2.2.2 multiple statements with logical connectives\nNegation (not): ¬\nConjunction (and): ∧ ,\nDisjunction (or): ∨,\nMaterial implication (if…then): → ,\nBiconditional (if and only if): ↔ .\n\n2.2.3 Example\nIf it is sunny outside then I walk to work; otherwise I drive, and if it is raining then I carry my umbrella.\n\np = “If it is sunny outside” \nq = “I work to work” \nr =“I drive” \ns =  “It is raining” \nt = “I carry my umbrella”\n\n\nIf p then q; otherwise r and if s then t. \n\nIf p then q and (if not p then (r and (if s then t))). \n(p → q) ∧ (¬ ¬ p →(r ∧ (s → t)))\n\n2.2.4 Rule Reasoning: Modus Ponens 推理三段式\n","categories":["KnowledgeEngineering"]},{"title":"Knowledge Graph Reasoning","url":"/2021/08/15/knowledge%20engineering/5.Knowledge%20Graph%20Reasoning/","content":"Knowledge Graph Reasoning\n1. 简介1.1 Def\nKG Reasoning is to infer new knowledge from the given KG.\n\nClassification:\n\nLogical reasoning 主要介绍 \nStatistical reasoning\n\n\n\n1.2 Logical Reasoning include:\nDeductive Reasoning determines whether the truth of a conclusion can be determined for that rule, based solely on the truth of the premises. 推导结论\n\nInductive Reasoning attempts to support a determination of the rule. 学习规则\n\nAbductive reasoning selects a cogent set of preconditions.寻找前提\n\nExample:\n\npre-condition: it rains \nrule: if it rains, then the grass gets wet \nconclusion: the grass gets wet\n\nDeductive Reasoning: If it rains, then the grass gets wet. Since today is raining, the grass is wet. 若下雨，则草地会变湿。因为今天下雨了，所以今天草地是湿的。 \n\nInductive Reasoning: The grass is wet each time when it rains. Thus, if it rains, then the grass gets wet. 每次下雨，草地都是湿的。所以下雨会使草地变湿。\n\nAbductive Reasoning: If it rains, then the grass gets wet. The grass is wet because it is raining. 若下雨，草地会变湿。之所以草地是湿的，因为正在下雨。\n\n\n1.3 Classification:\nForward reasoning \nBackward reasoning\n\n2. Forward reasoning2.1 Def\nstarts with the available data and uses inference rules to extract more data until a goal is reached.利用已有的规则不断获取新的知识，直到知识产生，注：目标并不是明确的\n\nAn inference engine using forward chaining searches the inference rules until it finds one where the antecedent  先前的 (If clause) is known to be true.\nWhen such a rule is found, the engine can conclude, or infer, the consequent (Then clause), resulting in the addition of new information to its data. 归纳新知识\n\nThe forward chaining approach is often employed by expert systems.\n\n\n\n\n2.2 Materialization 实体化\nMaterialization is to compute all implicit（隐式） statements by applying rules on assertions.\n\nExample1:\n\nExample2:\n\n\n\nSo what type of Endocarditis is?\n\n\n2.3 ClassificationExample1:\n\n\nClassification is to compute all implicit subclass relations by applying rules on a TBox.\n利用TBox的规则计算所有子类的依赖关系\n\n\nExample2:\n\nHighvalue_Company ⊑ ∃beInvestedBy. Investment_Company\n高价值公司由投资公司投资。\nInvestment_Company ⊑ Financial_Institute\n投资公司属于金融机构\n∃beInvestedBy. Financial_Institute ⊑ Solvent_Company\n借助金融机构投资的公司都是具备偿还能力的 企业。\n\nAnswer:\n\nHighvalue_Company ⊑ ∃beInvestedBy. Financial_Institute\nHighvalue_Company ⊑ Solvent_Company\n\n3. Backward Reasoning\nBackward reasoning (Backward chaining) is an inference method described colloquially as working backward from the goal.\n\nIt is often used in entailment（蕴含） checking or query answering in KG.\nIt uses the rules to rewrite the query in several ways and the initial query is entailed if a rewritten query maps to the initial facts.\n\n\n有目标，QA——真或假\n\n\nExample1:\n\nRule: If you are human, then you are mortal.\nHuman(x)→Mortal(x)\nQuestion: Is Socrates a mortal?\nSolution: Check whether Mortal(Scorates) or Human(Scorates) is true.\n\nExample2:\n\n\nQuery: Find all patients with heart diseases, i.e., Heart_Disease(x)\n\nRewriting: Endocarditis(x) ⋁ Miocardial_Infarction(x) ⋁ Coronary_disease(x)\n\n可以理解成重写问题\n\n\nExercize：\nFind all sports players with the following KG, i.e., rewrite the query Sports_Player(x).\n\n\nQuery: Find all sports players \nRewriting: SportsPlayer(x) ⋁ BasketballPlayer(x) ⋁ FootballPlayer(x) ⋁ TennisPlayer(x) \n\nOther Tasks in Logical Reasoning: Inconsistency CheckingIncoherent不一致 ontology: ontology with at least one unsatisfiable concept 至少有一个不可满足概念的本体（空集）\nExample: \n\nPhDStudent ⊑ Student, \n\nPhDStudent ⊑ Employee, \n\nStudent ⊑ ¬ Employee\n\n以上最终可以退出Student和Employee都是空集，所以找不到一个model\nInconsistent ontology: ontology without a model. \n\n\n\nExample: \n\nPhDStudent ⊑ Student, \n\nPhDStudent ⊑ Employee,\n\nStudent ⊑ ¬ Employee, \n\nPhDStudent(John)\n\n这里则产生了矛盾\n\n\n\nExample: DICE ontology:\n\nBrain $\\subseteq$​ CentralNervousSystem п $\\exists$​systempart.NervousSystem ח BodyPart п $\\exists$​ region.HeadAndNeck п $\\forall$​​​ region.HeadAndNeck\nCentralNervousSystem $\\subseteq$​​ NervousSystem\nBodyPart $\\subseteq \\neg$​ NervousSystem    or   DisjointWith(BodyPart,NervousSystem)\nReasoning: \nBrain $\\subseteq$​ Bodypart $\\subseteq$​ $\\neg$ NervousSystem\nBrain $\\subseteq$ CentralNervousSystem $\\subseteq$ NervousSystem\nBrain = $\\emptyset$\n\n\nBrain是空集\n\n\n\nExample from Foaf:\n\nPerson(timbl)\nHomepage(timbl, http://w3.org/)\nHomepage(w3c, http://w3c.org/)\nOrganization(w3c)\nInverseFunctionalProperty(Homepage) (Homepage的domain只有一个值，所以矛盾)\nDisjointWith(Organization, Person)\n\nExample from OpenCyc:\n\nArtifactualFeatureType(PopulatedPlace)\nExistingStuffType(PopulatedPlace)\nDisjointWith(ExistingobjectType,ExistingStuffType)\nArtifactualFeatureType $\\subseteq$ ExistingObjectType\n\nExistingObjectType与ExistingStuddType不相交，但是却拥有相同的实例，所以矛盾\n\n\nExercise：The following ontology is inconsistent, and remove one axiom or assertion in it to make it consistent.\n\n\\begin{aligned}\n&\\operatorname{Ph} D_{\\text {student }} \\sqcup \\text { Undergraduatge }_{\\text {student }} \\sqsubseteq \\text { Student, } \\\\\n&\\text { Student } \\sqsubseteq \\neg \\text { Employee } \\\\\n&\\text { PhD }_{\\text {student }} \\text { (John), } \\\\\n&\\text { Employee(John), } \\\\\n&\\text { Marriedto(John, Lisa), } \\\\\n&\\text { Undergraduate(Jack) }\n\\end{aligned}\n$\\text { Student } \\sqsubseteq \\neg \\text { Employee }$ or $\\text { Employee(John)}$ or $\\text { PhD }_{\\text {student }} \\text { (John) }$\n\n4. Summary\nIn logical reasoning, we mainly focus on deductive reasoning, i.e., leveraging rules and preconditions to infer new knowledge.\nDeductive reasoning including forward reasoning and backward reasoning.\nForward reasoning actually gets as much knowledge as possible before application. 相当于补全知识\nBackward reasoning starts from the target/ application and can quickly get the result. 不考虑补全，专注于找到结果，实用性\n\n5. Logical Reasoning in Practice\nDatalog is a declarative声明式 logic programming language, which is designed for knowledge base and database.\nDatalog has similar expressive ability with OWL.\nDatalog supports recursion, and rule editing (more freely) for reasoning. 可以自定义规则，来做推理\n\n\n\nOWL与Datalog相交的语言\n\nDatalog syntax: \n\nAtom: $𝑝(𝑡_1,𝑡_2,…,𝑡_𝑛)$ , predicate(谓语): p, variable or constant: t \nExample: $\\text{has_child}(𝑋,𝑌) $​​\n\nRule: $𝐻:−𝐵_1,𝐵_2,…,𝐵_𝑚,$ \n\nhead (an atom): H, \nbody (one atom or the AND of more atoms): 𝐵1,𝐵2,…,𝐵𝑚  (body 可以理解为解释，即可以由body推出head)\n\nExample: $\\text{has_child}(𝑋,𝑌):−\\text{has_son}(𝑋,𝑌)$ \n\nFact: $𝐹(𝑐_1,𝑐_2,…,𝑐_𝑛):−$​ , constant: $c$ , without body or variable \nExample: $\\text{has_child}(Alice,Bob):−$\n\n\n6. Datalog program:%%Factsfriend(joe,sue) .friend(ann,sue) .friend(sue,max) .friend(max,ann) .%% Rulesfof(X,Y) :- friend(X,Y)fof(X,Z) :- friend(X,Y), fof(Y,Z)%%Quiery 1query(X) :- fof(X,ann)%%Answer:fof(sue,ann) .fof(max,ann) .fof(joe,ann) .fof(ann,ann) .\n\nDatalog Rule Operators: \n\nIntersection: $Q(x_1, x_2, ⋯ , x_n) :- R(x_1, x_2, ⋯ , x_n) , S(x_1, x_2, ⋯ ,x_n) $\nExample:\n\n$\\text{smart_phone(x)} :- \\text{cell_phone(x)}, \\text{intelligent_device(x)}$​ \n\n\nUnion: $Q(x_1, x_2, ⋯, x_n) :- R(x_1, x_2, ⋯ ,x_n) ,  Q(x_1, x_2, ⋯, x_n) :- S(x_1, x_2 , ⋯ , x_n) $\nExample: \n\n$human(x) :- woman(x) $\n$human(x) :- man(x) $\n\n\nDifference: $Q(x_1, x_2, ⋯ ,x_n) :- R(x_1, x_2, ⋯, x_n) , \\text{not} S(x_1, x_2, ⋯ , x_n)$​​​ \nExample:\n\n$younger_brother(x, y) :- brother(x, y), not elder_brother(x, y)$​\n\n\n\n\nExercise\n\nRule：fatherinlawOf(X,Z) :- wifeOf(X,Y), fatherOf(Y,Z)\nFact： wifeOf (YaoMing，YeLi)\nFact： fatherOf (YeLi，YeFa)\nWho is the father-in-law of Yao Ming？\n\nfatherinlawOf(YaoMing, YeFa)\n\n","categories":["KnowledgeEngineering"]},{"title":"Statistical Resoning","url":"/2021/08/15/knowledge%20engineering/6%20Statistical%20Resoning/","content":"Statistical Resoning\nStatistical Resoning1.Def\nStatistical Reasoning tries to find suitable statistical models to fit the samples and predicts the expected probabilities of the inferred knowledge. 预测未来知识出现的概率\n\nknowledge graph embedding based reasoning \n\ninductive rule learning based reasoning \nmulti-hop reasoning\n\n\nTasks:\nPredicting the missing link.  \nGiven e1 and e2, predict the relation r. \nPredicting the missing entity.  \nGiven e1 （e2）and relation r, predict the missing entity e2 （e1）. \nFact Prediction.  \nGiven a triple, predict whether it is true or false.\n\n2. Embedding: Meaning of a Word\nWhat is the meaning of a word?\nBy ontologies? By Knowledge Graph?\nBut ontologies and KGs are hard to construct and often incomplete 无法穷举\nHow to encode the meaning of a word?\n\n3. One-hot Representation\nVocabulary: (cat, mat, on, sat, the) \ncat: 10000 mat: 01000 on: 00100 sat: 00010 the: 00001\n\n\n“The cat sat on the mat”\n\n\n\nDisadvantage: too sparsity\n\n\n\nOne-hot representation: \nFoundation of Bag-of-words Model\n\n\n无法衡量语义相关度\n\n\n4. Distributional Representation\nWhen a word w appears in text, its context is the set of words that appear nearby (within a fixed-size window)： 用中心词周围的词表示该词\n\nUse many contexts of w to build up a representation of w\n\n\n\n\n建立一个稠密向量\n\n5. Word Vectors\nWe will build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts.\n\n\n\nNote: word vectors are sometimes called word embeddings. They are a distributed representation.\n\nSimilarity:\n\\begin{gathered}\n\\operatorname{euclidean}(u, v)=\\sqrt{\\sum_{i=1}^{n}\\left|u_{i}-v_{i}\\right|^{2}} \\\\\n\\operatorname{cosine}(u, v)=1-\\frac{\\sum_{i=1}^{n} u_{i} \\times v_{i}}{\\|u\\|_{2} \\times\\|v\\|_{2}}\n\\end{gathered}6. Advantage of Distributed Representation\nDeal with data sparsity problem in NLP\nRealize knowledge transfer across domains and across objects\nProvide a unified representation for multi-task learning\n\n\n6.1 Representation Learning\nWhat is the representation learning?\nObjects are represented as dense, real-value and low-dimensional vector\n\n\n\n\n6.2 Different ways of KG Representation\n\nTensor: 自由度更高，隐式知识，但不容易扩展，不容易解释\n\n6.3 Knowledge Graph Embedding: Application\nEntity Prediction\n卧虎藏龙 Has-director ?\n卧虎藏龙 Has-director：Ang Lee\n\n\n\n\n\nRelation Prediction\n\n\n\nRecommendation System\n\n\n7. TransE: Take Relation as Translation\nFor a fact (head, relation, tail), take the relation as a translation operator from the head to the tail .\n\n\n\n实体经过关系的翻译到另一个实体\n\nTransE\nFor each triple , h is translated to t by r.\n\n\n\n\n\n\n\n\nt=h+r\nTrain TransE Energy Function:\n\n\nf(h, r, t)=|h+r-t|_{L_{1} / L_{2}}\nIf the triple is true, the translated distance between (h + r) and t is shorter.\n\nL1 (Manhattan) distance:\n\n\n\n\\mathbf{d}_{1}(a, b)=\\|a-b\\|_{1}=\\sum_{i=1}\\left|a_{i}-b_{i}\\right|\nL2 (Euclidean) distance:\n\n\n\\begin{aligned}\n \\mathbf{d}_{2}(a, b)=\\|a-b\\|=\\|a-b\\|_{2}=\\sqrt{\\sum_{i=1}^{d}\\left(a_{i}-b_{i}\\right)^{2}}\n\\end{aligned}TransE \n\nTriple1:  \nTriple2:   \nTriple3:  \n… \nfalse triple examples：\n\n\n\n…\n\nHow to distinguish？(true and false)\n\nMinimize the distance between (h+l) and t.\nMaximize the distance between (h’+l) to a randomly sampled tail t’ (negative example).\n最小化正类表示的差距，最大化负类表示的差距\n\n\n\n\n\nTbatch就是一个正例和负例元组的集合\n\n\ninput Training set $S=\\{(h, \\ell, t)\\}$, entities and relations. sets $E$ and $L$, margin $\\lambda$, embeddings dim. $k$.\nInitialize entity and relationship embedding;\nEntity and relationship embedding normalization;For each entity e（Suppose there are M elements in the entity set E）\n\n\ne=\\frac{e_{i}}{\\sqrt{e_{1}^{2}+e_{2}^{2}+\\cdots+e_{M}^{2}}}4、Negative Sampling \n\n\nEvaluation protocol:\n\nMetrics: 遍历所有实例，进行距离计算，并排序\n\nLink Prediction( WALL-E , _has_genre , ? )\n\nMean Ranks: the mean of those predicted ranks.\n\nHits@10: the proportion of correct entities ranked in the top 10.e.g. Entity 1: rank -&gt; 50; Entity 2: rank -&gt; 100; MR = (50+100)/2 = 75\n\n8.QuestionWe have two types of relations in KG, for example:\n\nSymmetric Relation: \n\ne.g., (stu1, classmate, stu2), (stu2, classmate, stu1)\n\n\nComposition（组合） Relation: \n\ne.g., (B, husband_of, A)，(A, mother_of, C)，(B, father_of, C)\n\n\n\nWhich Relation can be modeled by TransE? Why?\nTransE cannot model symmetric relations\n\n\n\nTransE can model composition relations，when $r_3=r_1+r_2$\n\n\n\nCan TransE model 1-to-N relations? \ne.g., (qiguilin, teacher_of, stu1), (qiguilin, teacher_of, stu2)，(qiguilin, teacher_of, stu3), (qiguilin, teacher_of, stu4)…\n不能，否则stui均相等\n\n\n\nIssue of TransE\nTransE is too simple to handle complex relations\n1-to-N, N-to-1, N-to-N relations 不可能发生\n\n\n\n\n\\begin{array}\\\\\na+r_1=b_1\\\\\na+r_2=b_2\n\\end{array}\n9. Variants（变种） of TransE: TransHFor each relation, define a hyperplane $W_r$​ and a relation vector dr. Then project the head entity vector $h$ and the tail entity vector $t$ onto the hyperplane $W_r$. 将向量映射到超平面做翻译\n\n\nFor example:\n in TransE, h and h’’ will overlap. While in TransH, entity h and entity h’’ will overlap only with the projection h⊥.\n\n10. Variants of TransE: TransR\nBoth TransE and TransH models assume that entities and relationships are vectors in the same semantic space.\n\n\n\n假设每一个关系，有自己的向量空间\n因为毛主席和奥巴马虽然在总统空间接近，但是诗人空间却是不接近\n\nTransR proposes:\n\nBuild entity and relation embeddings in the separate entity space and relation spaces;\n\nThen projecting entities from entity space to the corresponding relation space and building translations between projected entities.\n\n\nTransR: \n\nMapping entity embeddings into different semantic spaces\n\n\n\\mathbf{h}_{r}=\\mathrm{h} \\mathbf{M}_{r}, \\quad \\mathbf{t}_{r}=\\mathrm{tM}_{r}\n\nThe score(energy) function is correspondingly defined as (same as TransE):\n\n\nf_{r}(h, t)=\\left\\|\\mathbf{h}_{r}+\\mathbf{r}-\\mathbf{t}_{r}\\right\\|_{2}^{2}11. Summary\nStatistical reasoning uses statistical models to fit the samples and predicts the expected probabilities of the inferred knowledge .\n\nKnowledge graph embedding based reasoning actually performs entity prediction and relation prediction with vector calculations.\n\nTranslation-based models are now widely used KG embedding models for KG completion and other applications due to its good performance and succinctness.\n\n\n","categories":["KnowledgeEngineering"]},{"title":"Parsing","url":"/2021/08/15/nlp%20learning/Chapter10_Parsing/","content":"Parsing\n\nParsing1.上下无关文法与上下有关文法\nStarting unit:\n\n\n\\underset{\\text{非终结符}}{S}V\\ O\ntransition rule:\n\n\n\\begin{array}{l}\nS\\rightarrow \\text{人|天}\\\\\nV\\rightarrow \\text{吃|下}\\\\\nO\\rightarrow \\text{雨|饭|肉}\\\\\n\\end{array}\n上下文文无关语法\n不考虑前后依赖关系\n\n\n\n\nsent\\rightarrow SVO\\rightarrow\\text{天}VO\\rightarrow\\text{天吃}O\\rightarrow\\text{天吃肉}\n上下文有关语法\n考虑前后依赖关系\n\n\n\n\n\\begin{array}{l}\nS\\rightarrow \\text{人|天}\\\\\n\\text{人V}\\rightarrow \\text{人吃}\\\\\n\\text{天V}\\rightarrow \\text{天下}\\\\\n\\text{下O}\\rightarrow \\text{下雨}\\\\\n\\text{吃O}\\rightarrow \\text{吃饭|吃肉}\\\\\n\\end{array}\n上下文无关文法以及上下文有关文法都具有歧义，即不同语料表达的意思不一定相同\n\n2. Parsing2.1 Def\nSuntactic Structure: Constitunency = phrase structure grammar = context - free grammars\n传统认为，句子构成应该是：从最简单的单词出发，然后不同的词组成短语，再将短语组成大的短语。\n(the, cuddly, cat, by, the, door)(the cuddly cat|by the door)(the cuddly cat by the door)\n\n\n\n\n\\begin{array}{l}\n\\text{Lexicon:}\\\\\nN\\rightarrow \\text{dog}\\\\\nN\\rightarrow \\text{cat}\\\\\nDet\\rightarrow \\text{a}\\\\\nDet\\rightarrow \\text{the}\\\\\nP\\rightarrow \\text{in|on|by}\\\\\nV\\rightarrow \\text{talk|walk|...}\\\\\n\\end{array}\nGramma:\n\n\n\\begin{array}{l}\n\\text{NP}\\rightarrow\\text{Det N}\\\\\n\\text{NP}\\rightarrow\\text{Det $(Adj)^*$ N (PP)}\\\\\nPP\\rightarrow\\text{P NP}\\\\\nVP\\rightarrow\\text{V PP}\\\\\nsent\\rightarrow\\text{NP VP}\\\\\n\\end{array}2.2 PP Attachment Ambiguities Multiply\nLook in the large crate  in the kitchen by the door\nPP存在修饰歧义\n\n\n\n\n\\text{Look in } \\underset{NP}{\\underbrace{\\text{the large crate}}} \\underset{PP}{\\underbrace{\\text{ in the kitchen}}} \\underset{PP}{\\underbrace{\\text{ by the door}}}\n通常我们把一个句子的核心词称为head，修饰部分为modification\n\n\n\nPrepositional Phrase Attachment Ambiguity\nSan Jose cops kill man with knife\n\n\n\n\n\n不同的是，中文由于语法习惯不一样不存在相同的歧义。\n\n\n\n\nA key parsing decision is how we ‘attach’ various constituents\nPPs , adverbial or participial phrases, infinitives, coordinations\n\n2.3 Coordination Scope Ambiguity\n定语修饰范围出现错误\n\nShuttle veteran and longtime NASA executive Fred Gregory appointed to board\n\n[Shuttle veteran] and [longtime NASA executive Fred Gregory ]appointed to board\n[Shuttle veteran and longtime NASA executive] Fred Gregory appointed to board\n\n\nDoctor:No heart, cognitive issues\nDoctor:No [heart, cognitive issues]\nDoctor:[No heart], [cognitive issues]\n\n\n\n2.4 Adjectival/Adverbial Modifier Ambiguity\nStudents get first hand job experience\nStudents get [first hand] [job experience]\nStudents get [first [hand job] experience]\n\n\n\n2.5 Verb Phrase (VP) attachment ambiguity\nMultilated body washes up on Rio beach to be used for Olympics beach volleyball\nMultilated body washes up on Rio beach to be used for Olympics beach volleyball\nMultilated body washes up on Rio beach to be used for Olympics beach volleyball\n\n\n\n3. Dependency Grammar and Dependency Structure3.1 Def\n\nDependency syntax postulates that syntactic structure consists of relations between lexical items, normally binary asymmetric relations (“arrows”) called dependencies\n研究紧挨着的两个词的关系，并且会给边打上相关的标签\n\n\nThe arrows arecommonly typed with the name of grammatical relations (subject,prepositional object,apposition, etc.)\nAn arrow connects a head(governor, superior,regent) with a dependent(modifier, inferior,subordinate)\nUsually, dependencies form a tree (a connected,acyclic, single-root graph)\n\n\n处于支配地位的成分称为支配者head,governor,regent，而处于被支配地位的成分称为从属者 modifier,subordinate,dependency\n\n值得一提的是，我们都会在句子解析结构加上根节点以保证树状结构\n\n\n3.2 Dependency Relations\n3.3 Dependency Grammar and Dependency StructureROOT\nWhat are the sources of information for dependency parsing?\n\nBilexical affinities 同词性之间的依赖关系​ \n\nThe dependency [discussion  issues] is plausible\n\n\nDependency distance 依赖距离\n\nMost dependencies are between nearby words\n\n\nIntervening material   \n\nDependencies rarely span intervening verbs or punctuation\n依赖关系很少跨越中间的动词或标点符号\n\n\nValency of heads \n\nHow many dependents on which side are usual for a head?\n\n\n\n\n\nA sentence is parsed by choosing for each word what other word (including ROOT) it is a dependent of\nUsually some constraints:\nOnly one word is a dependent of ROOT (single headed)\nDon’t want cycles  (acyclic)\n\n\nThis makes the dependencies a tree\nFinal issue is whether arrows can cross (be non-projective) or not\n是否允许交叉将使得建树的方法和结果有所不同\n\n\n\n\n3.4 Methods of Dependency Parsing\n3.4.1 Greedy transition based parsing\nA simple form of greedy discriminative dependency parser\nThe parser does a sequence of bottom up actions\n\nRoughly like “shift” or “reduce” in a shift-reduce parser, but the “reduce” actions are specialized to create dependencies with head on left or right\n\n\nThe parser has:\n\na stack , written with top to the right\nwhich starts with the ROOT symbol\n用一个栈存放代操作的字符，后进先出\n\n\na buffer , written with top to the left\nwhich starts with the input sentence\n用一个队列存放剩余需要操作的句子，先进先出\n\n\na set of dependency \nwhich starts off empty\n有一个集合存放所有弧的关系\n\n\na set of actions\n用一个集合存放所有操作\n\n\n\n\n\nBasic Transition based Dependency Parser\nArc standard Transition based Parser\n(there are other transition schemes …) Analysis of  “I ate fish“\n\n\n\nExample\nLeft-Arc Precondition: ​ ROOT\n\n意思是当所在的stack中相邻的两个symbol的左依赖关系没有在集合里时，可以进行操作\n操作时，我们比较栈顶和队首的两个词，如果存在Left Arc，那么我们只pop掉栈顶的词\n\n\nRight-Arc ​\n\n操作时，我们比较栈顶和队首的两个词，如果存在right Arc，我们就pop掉队首并且push进栈顶\n这里我理解是，Right arc的两个词都有可能产生新的依赖关系，最具标志性的就是ROOT连接句子的第一个修饰的词\n\n\nReduce Precondition: ​\n\n意思是当队列为空，且所在的stack中相邻的两个symbol的左依赖关系有在集合里时，那么直接pop栈顶\n\n\nShift ​\n\n当栈顶和队首不存在任何依赖关系时，且队列不为空时，直接将队首的词加入栈顶\n\n\n\n\nMaltParser\n\n可以用SVM来判断应该做什么action\n\n可以使用beam search的方法来优化搜索\n\n\n3.4.2 Conventional Feature Representation\n3.5 Evaluation of Dependency Parsing: (labeled)dependency accuracy\n\n\n没有标签：UAS\n有标签：LAS\n\n3.6 Indicator Features Revisited\nProblem #1: \nsparse\n\n\nProblem#2: \nincomplete\n\n\nProblem#3: \nexpensive computation\n\n\nMore than 95% of parsing time is consumed by feature computation\n\n\n3.7 Why do we gain from a neural dependency parser?\ndense vector\ncomplete\ntrainable, low computation\n\n\n4. Further developments in transition based neural dependency parsing\nGraph based Dependency Parsers\nCompute a score for every possible dependency for each word\n\n\n\n权重最大的组成依赖关系\n\n","categories":["nlp"]},{"title":"Bert","url":"/2021/08/15/nlp%20learning/Chapter13_Bert/","content":"Bert\n\n1. Seq2Seq\n机器翻译、语音翻译、对话翻译、对话、QA、句法树（括号对齐）、多分类问题、图像识别、词向量\n位置编码：有些词出现的位置是有规律了，所以位置是非常重要的\n\n\nBert与CBOW的差异\nBERT相比CBOW更复杂\n\n\n语义对齐\n\n2. Multimodal\n视觉信息和文字信息理解不同\n对于同一个事物，我们有多种模态来表示\n\n","categories":["nlp"]},{"title":"Language Modeling","url":"/2021/08/15/nlp%20learning/Chapter3_n-gram_model/","content":"Language Modeling\n\nThe Language Modeling Problem\nSetup: assume a (finite) vocabulary of words:\n\n\\mathcal{V}=\\{ \\text{the, a, man, telescope, Beckham, two, Madrid, ...}\\}\nWe can construct an (infinite) set of strings:\n\n\\mathcal{V}^{\\dagger}=\\{ \\text{the, a, the a, the fan, the man, the man with the telescope, ...} \\}\nData: given a training set of example sentences \n\nProblem: estimate a probability distribution\n\n\n\nProbabilistic Language Modeling\nGoal\n\nassign a probability  to a sentence \n\n\nP(W) = P(w_1,w_2,w_3,...,w_n)\nRelated task\n\nprobability of an upcoming word\n\n\nP(w_5 \\mid w_1,w_2,w_3,w_4)\nA model that computes either of these is called a language model or LM\n\n\nHow to compute \nP(w_1,w_2,...,w_n) = \\prod_{i}^n P(w_i \\mid w_1,w_2,...,w_{i-1})e.g.\n\n\\begin{align}\nP(\\text{its water is so trasparent}) \n&=P(\\text{its}) \\\\\n&\\times P(\\text{water} \\mid \\text{its})\\\\\n&\\times P(\\text{is} \\mid \\text{its water}) \\\\\n&\\times P(\\text{so} \\mid \\text{its water is}) \\\\\n&\\times P(\\text{transparent} \\mid \\text{its water is so}) \\\\\n\\end{align}Markov Assumption\nFirst-order markov processes\n\n\n  P(\\text{the} \\mid \\text{its water is so transparent that}) = P(\\text{the} \\mid \\text{that})\nSecond-order markov processes\n\n\nP(\\text{the} \\mid \\text{its water is so transparent that}) = P(\\text{the} \\mid \\text{transparent that})\nwe approximate each component in the product\n\n\nP(w_1,w_2,...,w_n) = \\prod_{i}^n P(w_i \\mid w_{i-k},...,w_{i-1}) \\\\\nP(w_i \\mid w_1,...,w_{i-1}) =  P(w_i \\mid w_{i-k},...,w_{i-1})Unigram model\nP(w_1,w_2,...,w_n) = \\prod_{i}^n P(w_i)Problem\nP(\\text{the the the the}) \\gg P(\\text{I like ice cream})Bigram model\nP(w_i \\mid w_1,...,w_{i-1}) = P(w_i \\mid w_{i-1}) \\\\\nP(w_1,w_2,...,w_n) = \\prod_{i}^n P(w_i \\mid w_{i-1})Estimating\nP(w_i \\mid w_{i-1}) = \\frac{count(w_i,w_{i-1})}{count(w_{i-1})}= \\frac{c(w_i,w_{i-1})}{c(w_{i-1})}e.g.\n\n&lt;s&gt; I am Sam &lt;/s&gt;,&lt;s&gt; Sam am I &lt;/s&gt;,&lt;s&gt; I do not like green eggs an ham &lt;/s&gt;\n\n\n\\begin{array}{lll}\nP(\\text{I} \\mid \\text{}) = 0.67 & P(\\text{Sam} \\mid \\text{})=0.33 & P(\\text{am} \\mid \\text{I})=0.67 \\\\\nP(\\text{} \\mid \\text{Sam})=0.5 & P(\\text{Sam} \\mid \\text{am})=0.5 & P(\\text{do} \\mid \\text{I})=0.33\n\\end{array}\n\nRaw bigram counts\nRaw unigram counts\nRaw bigram probabilities\nExample: $P(\\text{ I want english food})$\nGiven\n\n\n\\begin{array}{ll}\nP(\\text{i} \\mid \\text{})=0.25 & P(\\text{english} \\mid \\text{want})=0.0011 \\\\\nP(\\text{food} \\mid \\text{english})=0.5 & P(\\text{}\\mid \\text{food})=0.68\n\\end{array}\nGet\n\n\nP(\\text{ I want english food})=\\begin{aligned}\nP(I \\mid)&\\times P(\\text {want} \\mid I) \n\\times P(\\text{english} \\mid \\text{want}) \n\\times P(\\text{food} \\mid \\text{english}) \n\\times P(\\text{}\\mid \\text {food)} \n=0.000031\n\\end{aligned}Practical Issue\nWe do everything in log space\nAvoid underflow\nadding is faster than multiplying\n\n\n\n\n\\log(p_1 \\cdot p_2 \\cdot p_3 \\cdot p_4) = \\log p_1 + \\log p_2 + \\log p_3 + \\log p_4N-gram model\nWe can extend to trigrams, 4 grams, 5 grams\nIn general this is an insufficient model of language\nbecause language has long distance dependencies\ne.g. The computer which I had just put into the machine room on the fifth floor crashed\n\n\n\n\nBut we can often get away with N-gram models\n\nHow to evaluate language models: Perplexity 困惑度\nPerplexity is the inverse probability of the test set, normalized by the number of words\nLower perplexity  Better model\n\n\nPP(W) = P(w_1w_2...w_n)^{-\\frac{1}{N}}\nChain rule\n\n\nPP(W) = (\\prod_{i=1}^{N}\\frac{1}{P(w_i \\mid w_1,...,w_{i-1})})^{\\frac{1}{N}}\nBigrams\n\n\nPP(W) = (\\prod_{i=1}^{N}\\frac{1}{P(w_i \\mid w_{i-1})})^{\\frac{1}{N}}SmoothingAdd-one estimation / Laplace smoothed\nPretend we saw each word one more time than we did\n\n\nP_{Add-1}(w_i \\mid w_{i-1}) = \\frac{c(w_{i-1},w_i)+1}{c(w_{i-1})+V}\\\\\nP_{Add-k}(w_i \\mid w_{i-1}) = \\frac{c(w_{i-1},w_i)+k}{c(w_{i-1})+kV}\n\nAdvanced smoothing algorithms\nGood-Turing\n\nReplace the empty frequency with those things we’ve seen only  times\n\n\n\nKneser-Ney\n\n\nInterpolation\nSimple interpolation\n\n\n\\begin{aligned}\n\\hat{P}\\left(w_{n} \\mid w_{n-1} w_{n-2}\\right)&= \\lambda_{1} P\\left(w_{n} \\mid w_{n-1} w_{n-2}\\right) \\\\\n&+\\lambda_{2} P\\left(w_{n} \\mid w_{n-1}\\right) \\\\\n&+\\lambda_{3} P\\left(w_{n}\\right)\n\\end{aligned}\n\\quad \\sum_i \\lambda_i = 1\nChoose  to maximize the probability of validation data\nFix the N-gram probabilities without smoothing (on the training data)\nThen search for λs that give largest probability to validation set\nCan use any optimization technique (line search or EM usually easiest)\n\n\n\nUnknown words\nIf we know all the words in advanced\nVocabulary V is fixed\nClosed vocabulary task\n\n\nOften we don’t know this\nOut Of Vocabulary = OOV words\nOpen vocabulary task\n\n\nInstead: create an unknown word token \nTraining of \\ probabilities\nCreate a fixed lexicon L of size V\nAt text normalization phase, any training word not in L changed to \\\nNow we train its probabilities like a normal word\n\n\nAt decoding time\nIf text input: Use UNK probabilities for any word not in training\n\n\n\n\n\n","categories":["nlp"]},{"title":"BOW","url":"/2021/08/15/nlp%20learning/Chapter4_BOW/","content":"BOW\n\n1. Language model has a history of over one hundred yearsPast:\n𝑛-gram Language Model\n\nPresent:\nNeural Language Model\n\nPretrained Language Model\n\n\nFuture:\nBrain-Inspired Language Model\n\n2. Brain-Inspired Language Model\n\nHumans’ language system in their brains can be divided into three regions including storing languages, sentiment, and representation,\nWhen people see the sentence, they will think of it as a picture in their brain. So though the two sentences above are very similar, the images may be different.\n\n3. Text Classification\nAssigning subject categories, topics, or genres\nSpam detection\nAuthorship identification\nAge/gender identification\nLanguage Identification\nSentiment analysis\n\n3.1 input\na document \na fixed set of classes  \n\n3.2 Output\na predicted class \n\n3.3 methodsRules-based on combinations of words or other features\n\nspam: black-list-address OR (“折扣” AND “降价”)\n\nAccuracy can be high\n\nIf rules are carefully refined by expert\n\nBut building and maintaining these rules is expensive\n\n\n\n\n3.4 The Bag of Words Representation\n\nCount the words of the document, and get a dictionary about the document. Then we can get the frequency of each word from the document.\nSometimes, it’s useless for us to count all words of the document. So we usually calculate the useful words of the document.\n\n\n\n4. How to learn the classifier 4.1 Let’s start with Naive Bayes(Simple “naïve” classification method based on Bayes rule)\n4.1.1 Imagine two people Alice and Bob whose word usage pattern you know：Alice often uses words:  love, great, wonderful\nBob  often uses words:  dog, ball, wonderful\nAlice words probabilities: love(0.1), great(0.8), wonderful(0.1)\nBob words probabilities: love(0.3), ball(0.2), wonderful(0.5)\nCan you guess who sends: “wonderful love”?\n\\begin{array} \\\\\nP(Alice|\\text{\"wonderful love\"})=0.1\\times 0.1=0.01 \\\\\nP(Bob|\\text{\"wonderful love\"})=0.5\\times 0.3=0.15 \\end{array}\nSo Bob do it\n\n4.1.2 Suppose there are two bowls of cookies：\nBowl 1 contains 30 vanilla cookies and 10 chocolate cookies. \n\nBowl 2 contains 20 of each.\n\n\n\nNow suppose you choose one of the bowls at random and, without looking, select a cookie at random. The cookie is vanilla. \nWhat is the probability that it came from Bowl 1? \n\n\\begin{array}\\\\\nP(Bowl_1|vanilla)\n=\\frac{P(Bowl_1, vanilla)}{P(vanilla)}=\\frac{P(vanilla|Bowl_1)\\times P(Bowl_1)}{\\sum_{i=1}^2 P(vanilla|Bow_i)\\times P(Bow_i)}\\\\\n=\\frac{\\frac{1}{2}\\times\\frac{3}{4}}{\\frac{1}{2}\\times\\frac{3}{4}+\\frac{1}{2}\\times\\frac{1}{2}}=\\frac{3}{5}\n\\end{array}\nP(c \\mid x)=\\frac{P(x \\mid c) P(c)}{P(x)}\nP(c|x) is the posterior probability of class c given features.\nP(c) is the probability of class.\nP(x|c) is the likelihood which is the probability of features given class.\nP(x) is the prior probability of features.\n\n4.1.3\nBased on our training set we can also say the following:\n\nFrom 500 bananas 400 (0.8) are Long, 350 (0.7) are Sweet and 450 (0.9) are Yellow\n\nOut of 300 oranges, 0 are Long, 150 (0.5) are Sweet and 300 (1) are Yellow\n\nFrom the remaining 200 fruits, 100 (0.5) are Long, 150 (0.75) are Sweet and 50 (0.25) are Yellow\n\n\n\nP\\left(x_{1}, x_{2}, \\ldots, X_{n} \\mid c\\right)=\\prod_{x \\in X} P(x \\mid c)\n\\begin{gathered}\nP\\left(\\frac{\\text { Banana }}{\\text { Long, } \\text { Sweet }, \\text { Yellow }}\\right)=\\frac{P\\left(\\frac{\\text { Long }}{\\text { Banana }}\\right) \\times P\\left(\\frac{\\text { Sweet }}{\\text { Banana }}\\right) \\times P\\left(\\frac{\\text { Yellow }}{\\text { Banana }}\\right) \\times P(\\text { Banana })}{P(\\text { Long }) P(\\text { Sweet }) P(\\text { Yellow })} \\\\\nP\\left(\\frac{\\text { Banana }}{\\text { Long }, \\text { Sweet }, \\text { Yellow }}\\right)=(0.8) \\times(0.7) \\times(0.9) \\times(0.5)\\times\\alpha=0.252\\alpha\n\\end{gathered}\n\\begin{gathered}\nP\\left(\\frac{\\text { Orange }}{\\text { Long, } \\text { Sweet }, \\text { Yellow }}\\right)=0\\end{gathered}\n\\begin{gathered}\nP\\left(\\frac{\\text { Other }}{\\text { Long, Sweet, Yellow }}\\right)=\\frac{P\\left(\\frac{\\text { Long }}{\\text { Other }}\\right) \\times P\\left(\\frac{\\text { Sweet }}{\\text { Other }}\\right) \\times P\\left(\\frac{\\text { Yellow }}{\\text { Other }}\\right) \\times P \\text { (Other) }}{P(\\text { Long }) P(\\text { Sweet }) P(\\text { Yellow })}\\\\\n={P\\left(\\frac{(0.5) \\times(0.75) \\times(0.25) \\times(0.2)}{\\text { Long, Sweet, Yellow }}\\right)=(0.5) \\times(0.75) \\times(0.25) \\times(0.2)\\times\\alpha=0.01875\\alpha}\n\\end{gathered}\nSo it’s banana\n\n4.2 Naive Bayes Classifier\n\\begin{aligned}\n&c_{M A P}=\\underset{c \\in C}{\\operatorname{argmax}} P(c \\mid d)\\\\\n&=\\underset{c \\in C}{\\operatorname{argmax}} \\frac{P(d \\mid c) P(c)}{P(d)}\\\\\n&=\\underset{c \\in C}{\\operatorname{argmax}} P(d \\mid c) P(c)\n\\end{aligned}\n\\left.c_{M A P}=\\underset{c \\in C}{\\arg \\max } P(d / c)\\right\\}(c)例：给定好评，对应评论的概率? 是否感到很奇怪?\n\nc_{M A P}=\\arg \\max _{c \\in C}[P(d / c) p(c)\n=\\underset{c \\in C}{\\operatorname{argmax}} P\\left(x_{1}, x_{2}, \\ldots, x_{n} \\mid c\\right) P(c)\n parameters\n\n\nP\\left(x_{1}, x_{2}, \\ldots, x_{n} \\mid c\\right)=\\prod_{x \\in X} P(x \\mid c)\nAssume that conditionally independent\n\n\nc_{N B}=\\underset{c \\in C}{\\operatorname{argmax}} P\\left(c_{j}\\right) \\prod_{x \\in X} P(x \\mid c)5. Learning the Naive Bayes Model\n\nSimply use the frequencies in the data (maximum likelihood estimates)\n\n\n\\begin{gathered}\n\\hat{P}\\left(c_{j}\\right)=\\frac{\\operatorname{doccount}\\left(C=c_{j}\\right)}{N_{d o c}} \\\\\n\\hat{P}\\left(w_{i} \\mid c_{j}\\right)=\\frac{\\operatorname{count}\\left(w_{i}, c_{j}\\right)}{\\sum_{w \\in V} \\operatorname{count}\\left(w, c_{j}\\right)}\n\\end{gathered}\n is equal to the likelihood of documents from class \n is equal to the likelihood of the word  in class \n\nCreate mega-document for topic j by concatenating all docs in this topic\n\nUse frequency of w in mega-document\n\n\n\n5.1 Laplace (add-1) smoothing\n\\begin{aligned}\n\\hat{P}\\left(w_{i} \\mid c\\right) &=\\frac{\\operatorname{count}\\left(w_{i}, c\\right)+1}{\\left.\\sum_{w \\in V}(\\operatorname{count}(w, c))+1\\right)} \\\\\n&=\\frac{\\operatorname{count}\\left(w_{i}, c\\right)+1}{\\left.\\sum_{w \\in V} \\operatorname{count}(w, c)\\right)+|V|}\n\\end{aligned}\n5.2 unknown wordAdd one extra word to the vocabulary, the “unknown word” \n\n\\begin{aligned}\n\\hat{P}\\left(w_{u} \\mid c\\right) &=\\frac{\\operatorname{count}\\left(w_{u}, c\\right)+1}{\\left(\\sum_{w \\in V} \\operatorname{count}(w, c)\\right)+|V+1|} \\\\\n&=\\frac{1}{\\left(\\sum_{w \\in V} \\operatorname{count}(w, c)\\right)+|V+1|}\n\\end{aligned}6. Try again with Textual examples\n\n\\begin{array}{r}\n\\hat{P}(c)=\\frac{N_{c}}{N} \\\\\n\\hat{P}(w \\mid c)=\\frac{\\operatorname{count}(w, c)+1}{\\operatorname{count}(c)+|V|}\n\\end{array}Priors\n\n\\begin{array}\\\\\nP(c)=\\frac{3}{4}\\\\\nP(j)=\\frac{1}{4}\n\\end{array}Conditional Probabilities:\n\n\\begin{array}\\\\\nP(\\text{Chinese|c}) = \\frac{5+1}{8+6}=\\frac{3}{7}\\\\\nP(\\text{Tokyo|c}) = \\frac{0+1}{8+6}=\\frac{1}{14}\\\\\nP(\\text{Japan|c}) = \\frac{0+1}{8+6}=\\frac{1}{14}\\\\\nP(\\text{Chinese|j}) = \\frac{1+1}{3+6}=\\frac{2}{9}\\\\\nP(\\text{Tokyo|j}) = \\frac{1+1}{3+6}=\\frac{2}{9}\\\\\nP(\\text{Japan|j}) = \\frac{1+1}{3+6}=\\frac{2}{9}\\\\\n\\end{array}Choosing a class:\n\n\\begin{aligned}\n\\mathrm{P}(\\mathrm{c} \\mid \\mathrm{d} 5) \\propto 3 / 4\\times (3 / 7)^{3} \\times 1 / 14 \\times 1 / 14 \n& \\approx 0.0003 \\\\\n\\mathrm{P}(\\mathrm{j} \\mid \\mathrm{d} 5) \\propto 1 / 4\\times(2 / 9)^{3} \\times 2 / 9 \\times 2 / 9 \n& \\approx 0.0001\n\\end{aligned}7. Sentiment Classification: Dealing with Negation 否定词\nI really like this movie\n\nI really don’t like this movie\n\n\nNegation changes the meaning of “like” to negative.\nNegation can also change negative to positive-ish \n\nDon’t dismiss this film\nDoesn’t let us get bored\n\n7.1 Sentiment Classification: Dealing with NegationDas, Sanjiv and Mike Chen. 2001. Yahoo! for Amazon: Extracting market sentiment from stock message boards. In Proceedings of the Asia Pacific Finance Association Annual Conference (APFA).\nBo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques. EMNLP-2002, 79—86.\n\n8. Naïve Bayes and Language ModelingNaïve bayes classifiers can use any sort of feature\n\nURL, email address, dictionaries, network features\n\nBut if, as in the previous slides\n\nWe use only word features \nwe use all of the words in the text (not a subset)\n\nThen\n\nNaive bayes has an important similarity to language modeling.\n\nEach class = a unigram language model\nAssigning each word: P(word | c)\nAssigning each sentence: P(s|c)=Π P(word|c)\n\n\n9. Evaluation\nprecision just represent the rate of positive example predicted by the LM, so it can’t be an evidence that the model is a good model\n\n\n\\begin{gathered}\nF_{\\beta}=\\frac{\\left(\\beta^{2}+1\\right) P R}{\\beta^{2} P+R} \\\\\n\\mathrm{~F}_{1}=\\frac{2 P R}{P+R}\n\\end{gathered}10.New Generation\n\n","categories":["nlp"]},{"title":"hmm","url":"/2021/08/15/nlp%20learning/Chapter7_hmm/","content":"hmm\n\n1. Generation and Discreminent Model\n for    input  to labels \n\n对于判别模型：条件概率分布\n\n对于生成模型：联合概率分布\n\n其中表示先验，表示似然\nwhere \n\n\n预测：对于判别式模型\n​            对于生成式模型\n\n\n2. Definition HMM\nHMM有两个独立性假设：\n\n观测序列之间是独立的 (A B 独立 有P(A, B) = P(A)P(B)，所以计算联合概率的时候才能用叠乘 )\n当前状态仅依赖于先前的状态\n\n\nNumber of states = K, Number of observations = M\n\n：Initial probabilities over states (K*K matrix)\n\n：Transition probabilities (K*M matrix)\n\n\n\nInput , Output ​ that corresponds to \n\n\nargmax_\\bold{y}P(\\bold{y}|\\bold{x},\\pi,A,B)=argmax_\\bold{y}P(\\bold{y},\\bold{x},\\pi,A,B)\nMaximun a posterior inference(MAP inference)\n\n\n\n\\begin{array}{ll}\nP(\\bold{y},\\bold{x})\n&=P(y_1,\\ldots,y_n,x_1,\\ldots,x_n)\\\\\n&=P(\\bold{x}|\\bold{y})P(\\bold{y})\\\\\n&=(\\prod^n_{i=1}P(x_i|y_i))\\\\\n&\\text{注释：这里用了条件独立假设，以及可见符号的概率只与当前状态有关}\\\\ \n&\\times P(y_1)P(y_2|y_1)P(y_3|y_1,y_2)\\ldots P(y_n|y_1,\\ldots,y_{n-1})\\\\\n&=(\\prod^n_{i=1}P(x_i|y_i))P(y_1)\\prod^n_{i=2}P(y_i|y_1,\\ldots,y_{i-1})\\\\\n&=(\\prod^n_{i=1}P(x_i|y_i))P(y_1)\\prod^n_{i=2}P(y_i|y_{i-1})\\\\\n&\\text{注释：这里用了隐马尔可夫一阶假设，即当前状态的概率只与上一个状态有关}\\\\\n&=P(y_1)\\prod^n_{i=2}P(y_i|y_{i-1})\\prod^n_{i=1}P(x_i|y_i)\n\\end{array}\nDP推导：\n\n\n\\begin{array}{ll}\n\\max_{y_1,\\ldots,y_n} P(\\bold{y},\\bold{x})\n&=\\max_{y_1,\\ldots,y_n}P(y_1,\\ldots,y_n,x_1,\\ldots,x_n)\\\\\n&=\\max_{y_1,\\ldots,y_n}P(y_1)\\prod^n_{i=2}P(y_i|y_{i-1})\\prod^n_{i=1}P(x_i|y_i)\\\\\n&=\\max_{y_1,\\ldots,y_n}P(y_n|y_{n-1})y(x_n|y_n)\\ldots P(y_2|y_1)P(x_2|y_2)P(y_1)P(x_1|y_1)\\\\\n&=\\max_{y_2,\\ldots,y_n}P(y_n|y_{n-1})y(x_n|y_n)\\ldots\\\\& P(y_3|y_2)P(x_3|y_3)\\max_{y_1}P(y_2|y_1)P(x_2|y_2)P(y_1)P(x_1|y_1)\\\\\n&=\\max_{y_2,\\ldots,y_n}P(y_n|y_{n-1})y(x_n|y_n)\\ldots\\\\& P(y_3|y_2)P(x_3|y_3)\\max_{y_1}P(y_2|y_1)P(x_2|y_2)Score(y_1)\\\\\n&\\text{Initialize: } score_1(y_1)=P(y_1)P(x_1|y_1)\\\\\n&=\\max_{y_3,\\ldots,y_n}P(y_n|y_{n-1})y(x_n|y_n)\\ldots\\\\& P(y_4|y_3)P(x_4|y_4) \\max_{y_2}(P(y_3|y_2)P(x_3|y_3)\\max_{y_1}P(y_2|y_1)P(x_2|y_2)Score(y_1))\\\\\n&=\\max_{y_3,\\ldots,y_n}P(y_n|y_{n-1})y(x_n|y_n)\\ldots\\\\& P(y_4|y_3)P(x_4|y_4)\\max_{y_2}(P(y_3|y_2)P(x_3|y_3)Score(y_2))\\\\\n&\\text{Update: }score_2(y_2)=\\max_{y_1}P(y_2|y_1)P(x_2|y_2)Score(y_1)\\\\\n&=max_{y_n}score(y_n)\\\\\n\\end{array}\nscore(y_i)=\n\\left\\{\n\\begin{array}{cc}\nP(y_i)P(x_i|y_i),&\\text{i=1}\\\\\n\\max_{y_{i-1}}P(y_i|y_{i-1})P(x_i|y_i)Score(y_{i-1}),&\\text{i=2,$\\ldots$,n}\n\\end{array}\n\\right.\n\n\n不用动态规划前，算法复杂度为​，即要遍历​的路径数量，使用动态规划后变为​，即对于每次迭代只需要计算k个联合概率，每个联合概率需要计算k次乘法，而迭代数为n次，所以时间复杂度如上\n\n3. Vitebri Algorithm\n​\n\nInitialization\n\nfor each hidden  ​​​\n\n\nRecursion\n\nfor t = 2 to n, for each :\n\n即t时刻隐藏状态j的联合概率为上一个状态转移的最大值所激发可见符号的概率\n\n\n​\n找到路径\n\n\n\n\nEnd\n\n\n​​\nfor t=n-1 to 1(path back tracking)\n$w^(t)=\\psi_{w^(t+1)}(t+1)$\n\n\nend\n\n4. Supervised learning details\n can be estimated separately just by counting\n\ns denotes label, x denotes word, n denotes the number of total words\n\nInitial prob\n\n\n\\pi_s=\\frac{count(start\\rightarrow s)}{n}\nTransition prob\n\n\nA_{s',s}=\\frac{count(s\\rightarrow s')}{count(s)}\nEmission prob\n\n\nB_{s,x}=\\frac{count\\left(\n\\begin{array}{c}\ns\\\\\n\\downarrow\\\\\nx\n\\end{array}\n\\right)}{count(s)}5. tri-gram markov\n\\begin{array}{ll}\nP(\\bold{y},\\bold{x})\n&=P(y_1,\\ldots,y_n,x_1,\\ldots,x_n)\\\\\n&=P(\\bold{x}|\\bold{y})P(\\bold{y})\\\\\n&=(\\prod^n_{i=1}P(x_i|y_i))\\\\\n&\\text{注释：这里用了条件独立假设，以及可见符号的概率只与当前状态有关}\\\\ \n&\\times P(y_1)P(y_2|y_1)P(y_3|y_1,y_2)\\ldots P(y_n|y_1,\\ldots,y_{n-1})\\\\\n&=(\\prod^n_{i=1}P(x_i|y_i))P(y_1)P(y_2|y_1)\\prod^n_{i=3}P(y_i|y_1,\\ldots,y_{i-1})\\\\\n&=(\\prod^n_{i=1}P(x_i|y_i))P(y_1)P(y_2|y_1)\\prod^n_{i=3}P(y_i|y_{i-2},y_{i-1})\\\\\n&\\text{注释：这里用了隐马尔可夫二阶假设，即当前状态的概率只与上一个状态有关}\\\\\n&=P(y_1)P(y_2|y_1)\\prod^n_{i=3}P(y_i|y_{i-2},y_{i-1})\\prod^n_{i=1}P(x_i|y_i)\n\\end{array}\n\\begin{array}{ll}\n\\max_{y_1,\\ldots,y_n} P(\\bold{y},\\bold{x})\n&=\\max_{y_1,\\ldots,y_n}P(y_1,\\ldots,y_n,x_1,\\ldots,x_n)\\\\\n&=\\max_{y_1,\\ldots,y_n}P(y_1)P(y_2|y_1)\\prod^n_{i=3}P(y_i|y_{i-2},y_{i-1})\\prod^n_{i=1}P(x_i|y_i)\\\\\n&=\\max_{y_1,\\ldots,y_n}P(y_n|y_{n-2},y_{n-1})y(x_n|y_n)\\ldots \\\\&P(y_3|y_1,y_2)P(x_3|y_3)P(y_2|y_1)P(x_2|y_2)P(y_1)P(x_1|y_1)\\\\\n&=\\max_{y_2,\\ldots,y_n}P(y_n|y_{n-2},y_{n-1})y(x_n|y_n)\\ldots P(y_4|y_2,y_3)P(x_4|y_4)\\\\&\\max_{y_1}P(y_3|y_1,y_2)P(x_3|y_3)P(y_2|y_1)P(x_2|y_2)P(y_1)P(x_1|y_1)\\\\\n&=\\max_{y_2,\\ldots,y_n}P(y_n|y_{n-2},y_{n-1})y(x_n|y_n)\\ldots P(y_4|y_2,y_3)P(x_4|y_4)\\\\&\\max_{y_1}P(y_3|y_1,y_2)P(x_3|y_3)Score(y_1,y_2)\\\\\n&\\text{Initialize: } score_1(y_1,y_2)=P(y_2|y_1)P(x_2|y_2)P(y_1)P(x_1|y_1)\\\\\n&=\\max_{y_3,\\ldots,y_n}P(y_n|y_{n-2},y_{n-1})y(x_n|y_n)\\ldots P(y_5|y_3, y_4)P(x_5|y_5) \\\\&\\max_{y_2}P(y_4|y_2,y_3)P(x_4|y_4)\\max_{y_1}P(y_3|y_1,y_2)P(x_3|y_3)Score(y_1,y_2)\\\\\n&=\\max_{y_3,\\ldots,y_n}P(y_n|y_{n-2},y_{n-1})y(x_n|y_n)\\ldots P(y_5|y_3,y_4)P(x_5|y_5)\\\\&\\max_{y_2}P(y_4|y_2,y_3)P(x_4|y_4)Score(y_2,y_3))\\\\\n&\\text{Update: }score(y_2,y_3)=\\max_{y_1}P(y_3|y_1,y_2)P(x_3|y_3)Score(y_1,y_2)\\\\\n&\\vdots\\\\\n    &=\\max_{y_{n-2},y_{n-1},y_n}P(y_n|y_{n-2},y_{n-1})P(x_n|y_n)\\\\&\\max_{y_{n-3}}P(y_{n-1}|y_{n-3},y_{n-2})P(x_{n-1}|y_{n-1})Score(y_{y_{n-3}},y_{n-2})\\\\\n&\\text{Update: }score(y_{n-2},y_{n-1})=\\max_{y_{n-3}}P(y_{n-1}|y_{n-3},y_{n-2})P(x_{n-1}|y_{n-1})Score(y_{y_{n-3}},y_{n-2})\\\\\n&=\\max_{y_{n-2},y_{n-1},y_n}P(y_n|y_{n-2},y_{n-1})P(x_n|y_n)score(y_{n-2},y_{n-1})\\\\\n&=\\max_{y_{n-1},y_n}\\max_{y_{n-2}}P(y_n|y_{n-2},y_{n-1})P(x_n|y_n)score(y_{n-2},y_{n-1})\\\\\n&=\\max_{y_{n-1},y_{n}}score(y_{n-1},y_{n})\n\\end{array}\nscore(y_{i},y_{i+1})=\n\\left\\{\n\\begin{array}{cc}\nP(y_2|y_1)P(x_2|y_2)P(y_1)P(x_1|y_1),&\\text{i=1}\\\\\n\\max_{y_{i-1}}P(y_{i+1}|y_{i-1},y_{i})P(x_{i+1}|y_{i+1})score(y_{i-1},y_{i}),&\\text{i=2,$\\ldots$,n-1}\n\\end{array}\n\\right.5.1 Vitebri Algorithm\n\n每次需要计算k*k种组合的隐藏状态概率，每次计算需要k次乘法，最终复杂度为\n\n","categories":["nlp"]},{"url":"/2022/02/02/github_skills/github_push/","content":"\ntitle:github_pushdate:2022-02-08description:github push\ncategories:github_skillsgithub push\n\n\n使用VS Code推送代码到GitHub - cheney-yang - 博客园 (cnblogs.com)\n\n关于pull冲突问题\n\ngithub远程仓库和本地仓库修改后的push问题_Wind_waving的博客-CSDN博客\n\n\n"},{"url":"/2022/02/08/github_skills/teamwork_tips/","content":"\ntitle:pull-requestdate:2022-02-08description:pull requestcategories:github_skills\n\n团队协作\n\n\n【原创】Github团队协作之Pull请求_weixin_34410662的博客-CSDN博客\ngit 团队协作 使用GitHub push pull clone - SKPrimin - 博客园 (cnblogs.com)\n\n"},{"title":"Gray-Level Grouping","url":"/2021/08/15/Image%20processing/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1/GLG%E7%AE%97%E6%B3%95/","content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.https://blog.csdn.net/linyacool/article/details/74982487\n思想概述本文提出了一个应用在灰度图上的图像对比度增强算法，能够在不需要人工改变算法参数的情况下自动增强图像的对比度，提高显示效果。算法操作的对象集中在灰度直方图上，对比度较低的图像的直方图灰度大多集中在某一区域，算法的目标就是让灰度级更合理的分布在直方图上。振幅小的灰度级应当距离相对较近，振幅大的灰度级则应当相距较远。\n算法概述\n对于一幅灰度图像，首先得到它的统计直方图 ​, 把强度值非0的灰度级作为初始的组, 初始化 ​​ 和, 分别作为每个组灰度级大小的左边界和右边界。\n对于每一次循环：\n(1)找到所有组中最小的强度值, 记录其下标，在该组左右两边寻找较小的组与其组成一个新的组, 更新 、、 。\n(2)计算灰度级转换表。计算组间间距N, 首先将两端的灰度级拉伸到该灰度级的端点，然后按照组间间距 , 组内间距在  内均匀分布的原则, 计算所有灰度级的转换表  。\n(3)将转换函数应用到原始图像上，得到新的直方图，计算像素间的平均距离D。循环直到组数为2。寻找最大的像素平均距离D所对应的转换函数 , 并将原始图像转换为最终的对比度增强了的图像。\n\n\n\n\n\n\n\n​ 为当前组数，k为灰度阶数\n\n初始值 ​\n\n更新\n\n\nG_{n-1}(i)= \\begin{cases}G_{n}(i), & \\text { for } i=1,2, \\ldots, i^{\\prime}-1 \\\\ a+b, & \\text { for } i=i^{\\prime} \\\\ G_{n}(i+1), & \\text { for } i=i^{\\prime}+1, i^{\\prime}+2, \\ldots, n-1\\end{cases}\n\nb=\\min \\left\\{G_{n}\\left(i_{a}-1\\right), G_{n}\\left(i_{a}+1\\right)\\right\\}\n\ni^{\\prime}= \\begin{cases}i_{a}-1, & \\text { for } G_{n}\\left(i_{a}-1\\right) \\leq G_{n}\\left(i_{a}+1\\right) \\\\ i_{a}, & \\text { otherwise. }\\end{cases}\n\n\n\n\n​​\n\n左/右边界 保存组内序号\n\n初始值\n\n\nL_{n}(i)=R_{n}(i) =k, \\text { for } H_{n}(k) \\neq 0 \\\\\nk=0,1,2, \\ldots, M-1, \\quad i=1,2,3, \\ldots, n\n\n\n\n\n\n\n灰度阶数最小的组的\n\n\n\n\ntransformation fuction\n\n\n\n\n\nN_{n-1}=\\frac{M-1}{n-1-\\alpha}\n防止当最左侧组只有一个灰度阶数的时候 这一个灰度阶数占太多空间 从而不平均\n\n\n\n\n\n像素的平均距离\n\n\nD_{n-1}=\\frac{1}{N_{p i x}\\left(N_{p i x}-1\\right)} \\sum_{i=0}^{M-2} \\sum_{j=i+1}^{M-1} H_{n-1}(i) H_{n-1}(j)(j-i)\n\n\n图片中的总像素数\n\n\n找到最好的​使得最大\n\n\n\n\n代码实现python版本2.7\n# coding=utf-8import numpy as npimport cv2import scipy.miscimport os#import matplotlib.pyplot as pltimport warningsfrom argparse import ArgumentParserwarnings.filterwarnings(\"ignore\")ALPHA = 0.8M = 256Threshold = 10def build_parser():    parser = ArgumentParser()    parser.add_argument('--image',                        dest = 'img', help = 'input image',                        metavar = 'INPUT_IMAGE.jpg', required = True)    parser.add_argument('--result',                        dest='res', help='output image',                        metavar='OUTPUT_IMAGE.jpg', required=True)    return parserdef Trans_and_CalcD(H = [],T = []):    M = len(H)    Tar = np.zeros(M+1)    for i in range(M):        if H[i] != 0:            Tar[T[i]] = H[i]    D = 0    for i in range(0,M-1):        for j in range(i+1,M):            D = D + Tar[i] * Tar[j] * (j - i)    return Ddef ten(img):    height, width = np.shape(img)    '''    ix = [-1,0,1   iy = [1,2,1          -2,0,2         0,0,0          -1,0,1]        -1,-2,-1]    '''    ans = 0    for i in range(1,height-1):        for j in range(1,width-1):            Sx = img[i-1][j+1] + 2 * img[i][j+1] + img[i+1][j+1] - (img[i-1][j-1] + 2 * img[i][j-1] + img[i+1][j-1])            Sy = img[i-1][j-1] + 2 * img[i-1][j] + img[i-1][j+1] - (img[i+1][j-1] + 2 * img[i+1][j] + img[i+1][j+1])            temp = Sx * Sx + Sy * Sy            if temp &gt; Threshold:                ans = ans + temp    return ansdef glg(img):    height,width = np.shape(img)    Npix = height * width    scipy.misc.imsave('original_img.jpg', img)    hist = cv2.calcHist([img], [0], None, [M], [0.0, 255.0])    #show histogram of the original image    '''    bins = np.arange(257)    item = img[:, :]    hist, bins = np.histogram(item, bins)    width = 0.7 * (bins[1] - bins[0])    center = (bins[:-1] + bins[1:]) / 2    plt.bar(center, hist, align='center', width=width)    plt.show()    '''    temp = [0]    temp_gray_level = np.zeros(M)    cnt = 1    for i in range(M):        if hist[i] != 0:            temp.append(hist[i])            temp_gray_level[cnt] = i            cnt = cnt + 1    n = len(temp) - 1    G = [[0] for i in range(n+2)]    gray_level = [[0 for _ in range(n+1)] for __ in range(n+1)]    G[n] = temp    gray_level[n] = temp_gray_level    L = [[0] for i in range(n+2)]    R = [[0] for i in range(n+2)]    for k in range(M):        if hist[k] != 0:            L[n].append(k)            R[n].append(k)    N = np.zeros(n+2).astype(float)    T = [[0 for k in range(M+1)] for i in range(n+2)]    D = np.zeros(n+2)    maxD = 0    Iopt = n - 1    while n &gt;= 3:        #compute Gn-1,Ln-1,Rn-1,i'        a = min(G[n][1:n+1])        ia = G[n].index(a)        left = True        if ia == 1:            b = G[n][ia+1]            left = False        elif ia == n:            b = G[n][ia-1]        else:            if G[n][ia-1] &lt;= G[n][ia+1]:                b = G[n][ia-1]                left = True            else:                b = G[n][ia+1]                left = False        if left:            ii = ia - 1        else:            ii = ia        for i in range(1,ii):            G[n-1].append(G[n][i])            gray_level[n-1][i] = gray_level[n][i]        G[n-1].append(a+b)        gray_level[n-1][ii] = gray_level[n][ii]        for i in range(ii+1,n):            G[n-1].append(G[n][i+1])            gray_level[n-1][i] = gray_level[n][i+1]        for i in range(1,ii+1):            L[n-1].append(L[n][i])        for i in range(ii+1,n):            L[n-1].append(L[n][i+1])        for i in range(1,ii):            R[n-1].append(R[n][i])        for i in range(ii,n):            R[n-1].append(R[n][i+1])        if L[n-1][1] != R[n-1][1]:            N[n-1] = (M - 1)/float(n - 1)        else:            N[n-1] = (M - 1)/float(n - 1 - ALPHA)        for k in range(0,M):            if k &lt;= L[n-1][1]:                T[n - 1][k] = 0                continue            if k &gt;= R[n-1][n-1]:                T[n-1][k] = M - 1                continue            i = 0            for x in range(1,n):                if k &gt;= L[n-1][x] and k &lt; R[n-1][x]:                    i = x                    if i &gt; 0 and L[n-1][i] != R[n-1][i]:                        if L[n-1][1] == R[n-1][1]:                            ans = int((i - ALPHA - (R[n - 1][i] - k) / float(R[n - 1][i] - L[n - 1][i])) * float(N[n - 1]) + 1 + 0.5)                            T[n - 1][k] = ans                        else:                            ans = int((i - (R[n - 1][i] - k) / float(R[n - 1][i] - L[n - 1][i])) * float(N[n - 1]) + 1 + 0.5)                            T[n - 1][k] = ans                    elif i &gt; 0 and L[n-1][i] == R[n-1][i]:                        if L[n-1][1] == R[n-1][1]:                            T[n - 1][k] =int(((i - ALPHA) * float(N[n - 1])) + 0.5)                        else:                            T[n - 1][k] =int((i * float(N[n - 1])) + 0.5)                elif k == R[n-1][x]:                    i = x                    if L[n-1][1] == R[n-1][1]:                        T[n - 1][k] = int(((float (i) - ALPHA) * float(N[n - 1])) + 0.5)                    else:                        T[n - 1][k] = int((i * float(N[n - 1])) + 0.5)             #can be deleted                if i == 0:                    T[n-1][k] = T[n-1][k-1]        D[n-1] = Trans_and_CalcD(hist,T[n-1])        if D[n - 1] &gt; maxD:            maxD = D[n - 1]            Iopt = n - 1        #print n - 1, D[n - 1]        n = n - 1    return T[Iopt],D[Iopt]/(float (Npix) * (Npix - 1))def main():    parser = build_parser()    options = parser.parse_args()    if not os.path.isfile(options.img):        parser.error(\"Image %s does not exist.)\" % options.network)    res = options.res    img = cv2.imread(options.img, cv2.IMREAD_GRAYSCALE)    Trans,PixDist = glg(img)    height, width = np.shape(img)    #reconstruct the enhangced image    image = np.copy(img)    for i in range(0,height):        for j in range(0,width):            image[i][j] = Trans[img[i][j]]    scipy.misc.imsave(res,image)    print \"The PixDist is %.1lf\" %PixDistif __name__ == '__main__':    main()\n","categories":["ImageProcessing"]},{"title":"Structure from motion (SFM)","url":"/2021/08/15/cv/10.%20Structure%20from%20motion%20(SFM)/","content":"Structure from motion (SFM)\n\nStructure from motion (SFM)1. Affine structure from motion\nGiven a set of corresponding points in two or more images, compute the camera parameters and the 3D point coordinates\n\n从图像中找一些点，再恢复到3D空间\n\n\n\n1.1 Recall: Corresponding point detection\nDetect SIFT features\n\n\n\nMatch features between each pair of images\n\n\n\n何种方法可能会出现错误\nUse RANSAC to estimate fundamental matrix between each pair\n不断找8个点去估计F矩阵，对于正确的F，一定可以使得这些点满足矩阵变换关系，那么当F保留内点最多时，则F正确\n这样估计出来的F可以用于去除一些错误的匹配\n\n\nKeep only the matches at are “inliers” with respect to the best fundamental matrix\n只保留与最佳基本矩阵相关的“内联”匹配\n\n\n\n1.2 Structure from motion\nGiven: $m$ images of $n$ fixed 3D points\n\n\\mathbf{x}_{i j}=\\mathbf{P}_{i} \\mathbf{X}_{j}, i=1, \\ldots, m, \\quad j=1, \\ldots, n\nestimate $m$ projection matrices $\\mathbf{P}_{i}$ motion\n\nestimate $n$​ 3D points $\\mathbf{X}_{j}$​ from the $m n$​ correspondences $\\mathbf{x}_{i j}$​ structure\n\n\n\n1.3 Orthographic Projection\n\n\\left[\\begin{array}{llll}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny \\\\\nz \\\\\n1\n\\end{array}\\right]=\\left[\\begin{array}{l}\nx \\\\\ny \\\\\n1\n\\end{array}\\right] \\Rightarrow(x, y)1.4 Affine cameras\nA general affine camera combines the effects of an affine transformation of the $3 D$ space, orthographic projection, and an affine transformation of the image:\n\n\n\\begin{array}{l}\n\\mathbf{P}=[3 \\times 3 \\text { affine }]\\left[\\begin{array}{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right][4 \\times 4 \\text { affine }]\\\\\n=\\left[\\begin{array}{cccc}\na_{11} & a_{12} & a_{13} & b_{1} \\\\\na_{21} & a_{22} & a_{23} & b_{2} \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\mathbf{A} & \\mathbf{b} \\\\\n\\mathbf{0} & 1\n\\end{array}\\right]\n\\end{array}\nAffine projection is a linear mapping + translation in non-homogeneous coordinates (in Euclidean space))\n转换为非齐次坐标系\n\n\n\n\n\n\nGiven: $m$ images of $n$ fixed 3D points:\n\\mathbf{x}_{i j}=\\mathbf{A}_{i} \\mathbf{X}_{j}+\\mathbf{b}_{i}, \\quad i=1, \\ldots, m, j=1, \\ldots, n\nProblem: use the $m n$ correspondences $x_{i j}$ to estimate $m$ projection matrices $A_{i}$ and translation vectors $b_{i}$, and $n$ points $X_{j}$\n\nCentering: subtract the centroid of the image points in each view\n\n对于每张图片做一个中心化处理\n这种变换等价于在空间上先做中心化，在进行投影\n\n\n\n\n\\begin{aligned}\n\\hat{\\mathbf{x}}_{i j} &=\\mathbf{x}_{i j}-\\frac{1}{n} \\sum_{k=1}^{n} \\mathbf{x}_{i k}=\\mathbf{A}_{i} \\mathbf{X}_{j}+\\mathbf{b}_{i}-\\frac{1}{n} \\sum_{k=1}^{n}\\left(\\mathbf{A}_{i} \\mathbf{X}_{k}+\\mathbf{b}_{i}\\right) \\\\\n&=\\mathbf{A}_{i}\\left(\\mathbf{X}_{j}-\\frac{1}{n} \\sum_{k=1}^{n} \\mathbf{X}_{k}\\right)=\\mathbf{A}_{i} \\hat{\\mathbf{X}}_{j}\n\\end{aligned}\n\\hat{\\mathbf{x}}_{i j}=\\mathbf{A}_{i}\\left(\\mathbf{X}_{j}-\\frac{1}{n} \\sum_{k=1}^{n} \\mathbf{X}_{k}\\right)=\\mathbf{A}_{i} \\hat{\\mathbf{X}}_{j}\nFor simplicity, set the origin of the world coordinate system to the centroid of the 3D points\n以世界坐标系的原点为中心点，这样我们就可以简化表达式\n\n\nAfter centering, each normalized $2 D$ point is related to the $3 D$ point $X_{j}$​ by\n\n\n\\hat{\\mathbf{x}}_{i j}=\\mathbf{A}_{i} \\mathbf{X}_{j}\nLet’s create a $2 m \\times n$ data (measurement) matrix:\n\n\n\\mathbf{D}=\\left[\\begin{array}{cccc}\n\\hat{\\mathbf{x}}_{11} & \\hat{\\mathbf{x}}_{12} & \\cdots & \\hat{\\mathbf{x}}_{1 n} \\\\\n\\hat{\\mathbf{x}}_{21} & \\hat{\\mathbf{x}}_{22} & \\cdots & \\hat{\\mathbf{x}}_{2 n} \\\\\n& & \\ddots & \\\\\n\\hat{\\mathbf{x}}_{m 1} & \\hat{\\mathbf{x}}_{m 2} & \\cdots & \\hat{\\mathbf{x}}_{m n}\n\\end{array}\\right]=\\underset{cameras(2m ×3)}{\\left[\\begin{array}{c}\n\\mathbf{A}_{1} \\\\\n\\mathbf{A}_{2} \\\\\n\\vdots \\\\\n\\mathbf{A}_{m}\n\\end{array}\\right]}\\underset{points (3×n)}{\\left[\\begin{array}{cccc}\n\\mathbf{X}_{1} & \\mathbf{X}_{2} & \\cdots & \\mathbf{X}_{n}\n\\end{array}\\right]}\n$x_{ij}$​是21，$A_i$​是2\\3,$X_i$​是3*1\n\nThe measurement matrix $\\mathbf{D}=\\mathbf{M S} $​ must have rank 3 !\n\n\n1.5 Factorizing the measurement matrix\nSingular value decomposition of D:\n\n\n\nObtaining a factorization from SVD:\n奇异值分解后，我们选取特征值最大的三个值所对应的$U$和$V$\n\n\n\n\n\n因为恢复存在歧义性所以恢复出来的三维结构可能会失去深度信息，即会发生变形\n$HS$为仿射变换，所以会发生变形\n\n\n\n\nD=MS=MH^{-1}HS\nThe decomposition is not unique. We get the same D by using any $3 \\times 3$​ matrix $\\mathrm{C}$​ and applying the transformations $\\mathrm{M} \\rightarrow \\mathrm{MC}, \\mathrm{S} \\rightarrow \\mathrm{C}^{-1} \\mathrm{~S}$​\nThat is because we have only an affine transformation and we have not enforced any Euclidean constraints (like forcing the image axes to be perpendicular, for example)\n\n\n\n一种解决方法：加入先验\n第二种通过灯箱实验，加入适当约束\n\n2. Projective structure from motion\nGiven: $m$ images of $n$ fixed 3 D points\n\n\nx_{i j}=P_{i} X_{j}, i=1, \\ldots, m, \\quad j=1, \\ldots, n\nProblem: estimate $m$​ projection matrices $\\boldsymbol{P}_{i}$​ and $n$​ 3D points $\\boldsymbol{X}_{j}$​ from the $m n$​ correspondences $x_{i j} $​\n\n\n\nWith no calibration info, cameras and points can only be recovered up to a $4 \\times 4$​ projective transformation $\\boldsymbol{H}$​​ :\n加入标定信息的变换\n\n\n\n\n\\begin{array}{cc}\nx=P X & P=K[R T] \\\\\n\\downarrow & \\downarrow \\\\\nH X & P H^{-1} \\\\\nx=P X=P H^{-1} H X &\n\\end{array}2.1 Problem\nFactorization method (by SVD) Assume all points are visible. \nThis not true if: occlusions occur \n如果出现遮挡，会出现缺失点，导致一些点用不了\n\n\nfailure in establishing correspondences\n其次如果点的对应关系无法找到也用不了\n\n\n\n\n\n2.2 Algebraic approach : Two-camera case\nCompute fundamental matrix $\\mathbf{F}$ between the two views From at least 8 point correspondences, compute $F$ associated to camera 1 and 2\nUse $\\mathrm{F}$​ to estimate projective cameras\n\n\n\\mathrm{F}->P_{1}, P_{2}\nUse these cameras to triangulate and estimate points in 3D\n\n\nX_{j}^{*}=\\arg \\min \\left(d^{2}\\left(\\mathbf{x}_{1 j}, \\mathbf{P}_{1} \\mathbf{X}_{j}\\right)+d^{2}\\left(\\mathbf{x}_{2 j}, \\mathbf{P}_{2} \\mathbf{X}_{j}\\right)\\right)\n3. Bundle adjustment\nNon-linear method for refining structure and motion \nMinimize reprojection error\n\n\n\\sum_{i=1}^{m} \\sum_{j=1}^{n} D\\left(\\mathbf{x}_{i j}-\\mathbf{P}_{i} \\mathbf{X}_{j}\\right)^{2}\nD is the nonlinear mapping\nNonlinear minimization: Newton Method\n\n\n4. What is deep learning? How does it work?\nFully connect feedforward neural network\nLoss function\nOptimization of deep neural network\n\n4.1 Perceptron\nMathematical model\n\n\n\nDistance from a incorrect classified point to the line\n\n\n\n\nGoal: make L(w, b) as small as possible\nL(w, b)=-\\sum_{x \\in M} y_{i}\\left(w \\cdot x_{i}+b\\right)\nMethod: optimize $w$ and $b$ through gradıent descent\n\n\n\nLet’s optimize w as an example\n\n\nQuestion\nCan we solve the problem below with perceptron?\n\n\n\nGoal: classifying samples to class A and class B\n\n4.2 Idea: the Two-Layer Perceptron\nTwo-Layer Perceptron\n\nFor the XOR problem, draw two, instead, of one lines\n\n\nWhat did it do?\n\n\n\nL4.3 et’s consider more general cases\nRecall: Machine Learning ≈ Looking for a function f\n\nSpeech Recognition\n\n\n\n\nImage Recognition\n\n\n\nPlaying Go\n\n\n\nDialogue System\n\n\n4.4 Framework\n\n\n\n4.5 Neuron\n\nz=a_{1} w_{1}+\\cdots+a_{k} w_{k}+\\cdots+a_{K} w_{K}+b4.6 Other activation functions\nRectified Linear Unit (ReLU)\ny=\\max (\\mathbf{0}, z)\nLeaky ReLU\n\n\ny= \\begin{cases}z & z>0 \\\\ \\alpha z & \\text { otherwise }\\end{cases}\nSoftplus\n\n\ny=\\ln \\left(1+e^{z}\\right)4.7 Neural Network\n\nNetwork parameter 𝜃𝜃:all the weights and biases in the “neurons”\n\n4.8 Fully Connect Feedforward Network\n\n4.9 Matrix Operation\n\n4.10 Output Layer as Multi-Class Classifier\n\nSoftmaxlayer as the output layer\n为了更好的可解释性\n\n\n\n\n\n4.11 Example Application\n4.12 Loss for an Example\n4.13 Total Loss\n4.14 Gradient Descent\n\n\n4.15 Local Minima\n\nDo we really minimize the total loss in engineering?\n\n4.16 Shuffle the samples for each epoch\n4.17 Mini-batch Epoch\n\nStep #1: Randomly initialize network parameters\n\n\n\nMini-batch is Faster\n\n\n4.18 Vanishing gradient and ReLU\n\n梯度压缩，导致梯度反向传播时只能影响靠近输出的layer的参数\n\n4.19 Rectified Linear Unit (ReLU)\n\n\n4.20 ReLU -variant\n4.21 Learnable activation function -Maxout\nLearnable activation function [Ian J. Goodfellow, ICML’13]\n\n\n\nYou can have more than 2 elements in a group\n\nReLu是Maxout的特俗情况\n\n\n\n\n因为maxout会随着w变化，所以是可学习的激活函数\n\nHow many pieces depending on how many elements in a group\n\n\n\n\n不同的样本会产生不同的网络架构，从而改变模式和参数都达到调整\n\nGiven a training data x, we know which z would be the max\n\n\n\n\nGiven a training data x, we know which z would be the max\n\n\n\nTrain this thin and linear network1x2x\n\nDifferent thin and linear network for different examples\n\n\n4.22 Hard to find optimal parameters\n4.23 In physical world……\n4.24 Review: Vanilla Gradient Descent\nStart at position $\\theta^{0}$\nCompute gradient at $\\theta^{0}$\nMove to $\\theta^{1}=\\theta^{0}-\\eta \\nabla L\\left(\\theta^{0}\\right)$\nCompute gradient at $\\theta^{1}$\nMove to $\\theta^{2}=\\theta^{1}-\\eta \\nabla L\\left(\\theta^{1}\\right)$\nStop until $\\nabla L\\left(\\theta^{t}\\right) \\approx 0$\n\n\n4.25 Momentum\nStart at point $\\theta^{0}$​\nMovement $\\mathrm{v}^{0}=0$​\nCompute gradient at $\\theta^{0}$​\nMovement $\\mathrm{v}^{1}=\\lambda \\mathrm{v}^{0}-\\eta \\nabla L\\left(\\theta^{0}\\right)$​\nMove to $\\theta^{1}=\\theta^{0}+\\mathrm{v}^{1}$​\nCompute gradient at $\\theta^{1}$​\nMovement $\\mathrm{v}^{2}=\\lambda \\mathrm{v}^{1}-\\eta \\nabla L\\left(\\theta^{1}\\right)$​\nMove to $\\theta^{2}=\\theta^{1}+\\mathrm{v}^{2}$​\n\nMovement not just based on gradient, but previous movement\n\n\n\n\n4.26 Dropout\n回忆一下，3.8节 (多层感知机) 的图3.3描述了一个单隐藏层的多层感知机。其中输入个数为 4 ，隐藏单元个数为 5 ，且隐藏单元 $h_{i}(i=1, \\ldots, 5)$ 的计算表达式为\n\n\nh_{i}=\\phi\\left(x_{1} w_{1 i}+x_{2} w_{2 i}+x_{3} w_{3 i}+x_{4} w_{4 i}+b_{i}\\right)\n这里 $\\phi$ 是激活函数， $x_{1}, \\ldots, x_{4}$ 是输入，隐藏单元 $i$ 的权重参数为 $w_{1 i}, \\ldots, w_{4 i}$ ，偏差参数为 $b_{i}$ 。当对该隐藏层使用丟弃法时，该层的隐藏单元将有一定概率被至弃掉。设丟弃概率为 $p$ ，那么有 $p$ 的概率 $h_{i}$ 会被 清零，有 $1-p$ 的概率 $h_{i}$ 会除以 $1-p$ 做拉伸。丟弃概率是丟弃法的超参数。具体来说，设随机变量 $\\xi_{i}$ 为 0 和 1 的概率分别为 $p$ 和 $1-p_{\\text {。 }}$ 使用丟弃法时我们计算新的隐藏单元 $h_{i}^{\\prime}$\n\n\nh_{i}^{\\prime}=\\frac{\\xi_{i}}{1-p} h_{i}\n由于 $E\\left(\\xi_{i}\\right)=1-p$ ，因此\n\n\nE\\left(h_{i}^{\\prime}\\right)=\\frac{E\\left(\\xi_{i}\\right)}{1-p} h_{i}=h_{i}\n即丟弃法不改变其输入的期望值。让我们对图3.3中的隐藏层使用丟弃法，一种可能的结果如图3.5所示，其中 $h_{2}$ 和 $h_{5}$ 被清零。这时输出值的计算不再依赖 $h_{2}$ 和 $h_{5}$ ，在反向传播时，与这两个隐藏单元相关的权重 的梯度均为 0 。由于在训练中隐藏层神经元的丢弃是随机的，即 $h_{1}, \\ldots, h_{5}$ 都有可能被清零，输出层的计算无法过度依赖 $h_{1}, \\ldots, h_{5}$ 中的任一个，从而在训练模型时起到正则化的作用，并可以用来应对过拟 合。在测试模型时，我们为了拿到更加确定性的结果，一般不使用丟弃法。\n\ndef dropout(X, drop_prob):    X = X.float()    assert 0 &lt;= drop_prob &lt;= 1    keep_prob = 1 - drop_prob    # 这种情况下把全部元素都丢弃    if keep_prob == 0:        return torch.zeros_like(X)    mask = (torch.rand(X.shape) &lt; keep_prob).float()    return mask * X / keep_prob\n\n\nEach time before updating the parameters\nEach neuron has p% to dropout\nThe structure of the network is changed.Thinner!\n\n\nUsing the new network for training\nFor each mini-batch, we resample the dropout neurons\n\n\n\n\nWhy the weights should multiply (1-p)% (dropout rate) when testing?\n\n\n\nDropout is a kind of ensemble\n\n\n\nTrain a bunch of networks with different structures\n\n\n\n\n","categories":["CV"]},{"title":"semi-supervised learning","url":"/2021/08/15/cv/12.%20semi-supervised%20learning/","content":"semi-supervised learning\n\n1. What is semi-supervised learning?\n\nHumans learn in semi-supervised way\n\n1.1 Why semi-supervised learning helps?\n\nThe distribution of the unlabeled data tell us something.\n\n\n1.2 Low-density Separation Assumption\n\n希望分开的类内差距尽量大\n\nGiven: labelled data set $=\\left\\{\\left(x^{r}, \\hat{y}^{r}\\right)\\right\\}_{r=1}^{R}$, unlabeled data set $=\\left\\{x^{u}\\right\\}_{u=1}^{U}$\n\nRepeat:\n\n\n\nHard label vs Soft label\nConsidering using neural network $𝜃^∗$(network parameter) from labelled data\n\n\n\n\n\n软标记对训练没有影响，所以应该使用硬标记\n\n1.3 Entropy-based Regularization\n\n我们希望我们的分类是非黑即白的\n可以通过信息熵来判断是否分类成功，对于无标记数据我们希望其分类越集中越好\n\n1.4 Smoothness Assumption\nAssumption: “similar” $x$​ has the same $\\hat{y}$​\n数据相似带来标签相似\n\n\nMore precisely:\n$\\mathrm{x}$ is not uniform.\nIf $x^{1}$ and $x^{2}$ are close in a high density region, $\\hat{y}^{1}$ and $\\hat{y}^{2}$ are the same.\n\n\n\n\n\nConnected by a high density path\n\n\n\n我们可以在训练数据库中插入多个2，使得可以左边的2可以通向右边的2\n\nClassify astronomyvs. travelarticles\n\n\n\n\n可以找到一条连通区域，从而进行分类\n\n1.5 Graph-based Approach\n$\\text { How to know } x^{1} \\text { and } x^{2} \\text { are connected by a high density path? }$\n\n\n\nDefine the similarity $s\\left(x^{i}, x^{j}\\right)$ between $x^{i}$ and $x^{j}$\n\nAdd edge:\n\nK Nearest Neighbor\ne-Neighborhood\n\n\n\n\n\n\nEdge weight is proportional to $s\\left(x^{i}, x^{j}\\right)$ Gaussian Radial Basis Function:\n\n\ns\\left(x^{i}, x^{j}\\right)=\\exp \\left(-\\gamma\\left\\|x^{i}-x^{j}\\right\\|^{2}\\right)\n\nThe labelled data influence their neighbors. Propagate through the graph\n图上的标记会随着路径传播\n\n\n\n\n\n\n不一定有效\n\n\n\nDefine the smoothness of the labelson the graph\nw是特征空间的相似度，S越小越平滑\n\n\n\n\n\n\nDefine the smoothness of the labels on the graph\n\n\nS=\\frac{1}{2} \\sum_{i, j} w_{i, j}\\left(y^{i}-y^{j}\\right)^{2}=y^{T} L y\n$y:(R+U)-\\operatorname{dim}$​ vector\n在标记传播过程中，会先初始化标签，R表示有标签，U表示原来无标签\n\n\n\n\ny=\\left[\\cdots y^{i} \\cdots y^{j} \\cdots\\right]^{T}\n$L:(R+U) \\times(R+U)$​​ matrix\nD是行和放于对角线\n\n\n\n\n不同层都可以加如smooth，即传播后会返回每一层的输出以便于计算loss\n\n\n2. Unsupervised Neural Network2.1  Recall: Unsupervised learningData: xJust data, no labels!Goal: Learn some underlyinghidden structure of the dataExamples: Clustering, dimensionality reduction, density estimation, etc.\n\nK-means clustering\n\n\n2.2 Auto-encoder\n希望编码器可以自动凝练特征，编码后又可以恢复\n\n\n\nOutput of the hidden layer is the code\n\n\n2.3 Deep Auto-encoder\n\n\n深度网络更具表征更具判别性\n\n\n\nDe-noising auto-encoder\n希望有噪音的图像能够恢复为无噪图像，即希望自编码器能够自主去噪\n\n\n\n\n\n2.4 Auto-encoder –Text Retrieval\n\nThe documents talking about the same thing will have close code.\n\n\n2.5 Auto-encoder –Similar Image Search\n2.6 Auto-encoder for CNN\n2.7 CNN -Unpooling\n2.8 CNN -Deconvolution\n\nGreedy Layer-wise Pre-training\n逐层进行训练，训练完后的参数freeze\n\n\n\n\n\n\n\n最后进行微调\n\n\n2.9 Why VAE (Variational Auto-Encoders)?\n对编码进行插值能否采样？\n不会\n\n\n\n\n\n但我们希望编码能够一定线性插值得到新的图片\n将确定性的向量变为一个分布，即对编码进行加噪\n\n\n\n\n\ne为来自高斯分布的采样权值，$\\sigma$为标准差\n\n\n2.10 Pokémon Creation\n\n垂直方向控制大小，水平方向控制方向\n\n\n2.11 Problems of VAE\nIt does not really try to simulate real images\n\n\n\n有其重构函数是像素级别的，所以不一定完全相近\n\n3. Generative Adversarial Network (GAN)3.1 Basic Idea of GAN\n$\\text { The data we want to generate has a distribution } P_{\\text {data }}(x)$\n\n\n\nA generator G is a network. The network defines a probability distribution.\n不考虑原始数据的分布\n\n\n\n\n3.2 Generative adversarial networks\nTrain two networks with opposing objectives:\nGenerator:learns to generate samples\nDiscriminator:learns to distinguish between generated and real samples\n\n\n两者互相博弈，最后越来越好\n\n\n3.3 Evolution\n\nGenerator\n每一维度都觉了图像某一特征\n\n\n\n\n\n\nDiscriminator\n\n\n3.4 The evolution of generation\n固定一个更新另一个，从而迭代更新\n\n\n\nThe discriminator $D(x)$ should output the probability that the sample $x$ is real\nThat is, we want $D(x)$ to be close to 1 for real data and close to 0 for fake\nExpected conditional log likelihood for real and generated data:\n对于判别器，我们希望区分出真假\n而生成器则相反，希望他越小越好\nV(G, D)=\\mathbb{E}_{x \\sim p_{\\text {data }}} \\log D(x)+\\mathbb{E}_{z \\sim p} \\log (1-D(G(z)))\n\n\nWe seed the generator with noise $z$ drawn from a simple distribution $p$(Gaussian or uniform)\n\n3.5 GAN objective\nV(G, D)=\\mathbb{E}_{x \\sim p_{\\text {data }}} \\log D(x)+\\mathbb{E}_{z \\sim p} \\log (1-D(G(z)))\nThe discriminator wants to correctly distinguish real and fake samples:\nD^{*}=\\arg \\max _{D} V(G, D)\nThe generator wants to fool the discriminator:\nG^{*}=\\arg \\min _{G} V(G, D)\nTrain the generator and discriminator jointly in a minimax game\n\nUpdate discriminator:\n\nRepeat for $k$ steps:\nSample mini-batch of noise samples $z_{1}, \\ldots, z_{m}$ and mini-batch of real samples $x_{1}, \\ldots, x_{m}$\n\n3.6 Training algorithm in practice\nUpdate parameters of $D$​ by stochastic gradient ascent on\nRepeat for $k$ steps:\nSample mini-batch of noise samples $z_{1}, \\ldots, z_{m}$ and mini-batch of real samples $x_{1}, \\ldots, x_{m}$\nUpdate parameters of $D$ by stochastic gradient ascent on\n\\frac{1}{m} \\sum_{m}\\left[\\log D\\left(x_{m}\\right)+\\log \\left(1-D\\left(G\\left(z_{m}\\right)\\right)\\right)\\right]\n\n\n\n\nUpdate generator:\nSample mini-batch of noise samples $z_{1}, \\ldots, z_{m}$\nUpdate parameters of $G$ by stochastic gradient ascent on\n\\frac{1}{m} \\sum_{m} \\log D\\left(G\\left(z_{m}\\right)\\right)\n\n\nRepeat until happy with results\n\nUpdate discriminator: push $D\\left(x_{\\text {data }}\\right)$ close to 1 and $D(G(z))$ close to 0\n\nThe generator is a “black box” to the discriminator\nThe generator is exposed to real data only via the output of the discriminator (and its gradients)\n\n\n\n\n\nTest time –the discriminator is discarded\n\n\n3.7 Original GAN results\n原始GAN比较模糊，因为这样能够难以分类\n\n\n3.8 Problems with GAN training\nStability\n\nParameters can oscillate or diverge, generator loss does not correlate with sample quality\nBehavior very sensitive to hyperparameter selection\n\n\n只能模仿几个模式而无法生成实际的多模态\n\nMode collapse\n\nGenerator ends up modeling only a small subset of the training data\n\n\n\n\n3.9 DCGAN\nEarly, influential convolutional architecture for generator\n使用卷积，且不用池化，即用stride代替\n\n\n\n\n\nEarly, influential convolutional architecture for generator\nDiscriminator architecture (empirically determined to give best training stability):\nDon’t use pooling, only strided convolutions\nUse Leaky ReLU activations (sparse gradients cause problems for training)\nUse only one FC layer before the softmax output\nUse batch normalization after most layers (in the generator also)\n降低对超参敏感程度\n\n\n\n\n\n3.10 DCGAN results\nInterpolation between different points in the z space\n即是连续的\n\n\n\n\n\nVector arithmetic in the z space\n\n\n\nPose transformation by adding a “turn” vector\n\n\n4. Conditional generation\nTo condition the generation of samples on discrete side information (label) 𝑦, we need to add 𝑦 as an input to both generator and discriminator\n加入类的标签，加入限制\n\n\n\n\n4.1 BigGAN\nClass-conditional generation of ImageNet images up to\n\n\n\n对Z空间进行截断，防止由于分布带来的模糊，因为只取了一部分作为编码空间，从而提高分辨率\n但也有可能降低保真度，所以需要tradeoff\n\n5. Image-to-image translation\n\nProduce modified image $y$ conditioned on input image $x$(note change of notation)\nGenerator receives $x$ as input\nDiscriminator receives an $x, y$ pair and has to decide whether it is real or fake\n\n\n\n\n\n作为一个对照来进行判别，以增加条件\n即希望鞋的形状一致\n\n\n\n5.1 Translating between maps and aerial photos\n\nDay to night\n\n\n\nEdges to photos\n\n\n5.2 Unpaired image-to-image translation\n有时候我们并不能得到成对的样本\n\nGiven two unordered image collections 𝑋 and 𝑌, learn to “translate” an image from one into the other and vice versa\n\n\n\n\n5.3 CycleGAN\nGiven: domains $X$ and $Y$​\n就是我们希望X可以变为Y，Y经过反变换后还可以生成Y\n就可以限制Y的形状类似X\n\n\nTrain two generators $F$ and $G$ and two discriminators $D_{X}$ and $D_{Y}$\n$G$ translates from $X$ to $Y, F$ translates from $Y$ to $X$\n$D_{X}$ recognizes images from $X, D_{Y}$ from $Y$\nCycle consistency: we want $F(G(x)) \\approx x$ and $G(F(y)) \\approx y$\n\n\n\n\n\nIllustration of cycle consistency:\n\n\n\nTranslation between maps and aerial photos\n\n\n\nTasks for which paired data is unavailable\n\n\n5.4 CycleGAN: Limitations\nCannot handle shape changes (e.g., dog to cat)\n\nCan get confused on images outside of the training domains (e.g., horse with rider)\n\n不能对训练数据以外的做拟合\n\n\nCannot close the gap with paired translation methods\n\n5.5 Multimodal image-to-image translation5.5.1 Human generation conditioned on pose\n","categories":["CV"]},{"title":"Image Feature Extraction","url":"/2021/08/15/cv/4.1%20Image%20Feature%20Extraction/","content":"Image Feature Extraction \n\n1. Why computer vision is challenging?\nViewpoint variation 视觉角度\nIllumination 光照\nOcclusion 遮挡\nScale 尺度\nDeformation 姿态变形\nBackground clutter 背景杂乱\nLocal ambiguity 局部迷惑\nObject intra class variation 目标内部变换\n\n2. Motivation for using local features\nOcclusions\nArticulation 关节\n\n\n\nIntra category variations 局部不变\n\n\n3. General Approach\nFind a set of distinctive keypoints 找到一系列关键点\nDefine a region around each keypoint 在关键点周围定义对应的区域\nExtract and normalize the region content 从区域中提取内容并归一化\nCompute a local descriptor from the normalized region 用特征算子提取特征\nMatch local descriptors 匹配局部描述信息\n\n4. Characteristics of good features\nCompactness and efficiency 紧凑且高效，要求关键点远少于图片pixel，且提取得算法是高效的，存储量小的\nSaliency 具有判别力\nEach feature is distinctive\n\n\nLocality 局部特征不会变\nRepeatability 可重复性\n\n5. Keypoint extraction: Corners\n\n\n平滑部分区域的各个方向梯度都为零\n边缘区域有一个方向的梯度不为零\n角点位置至少有两个方向的梯度值不为零\n\n6. Corner Detection\n最终结果是一个窗口大小的像素，不是只有一个像素\n\n6.1 Derivation\nE(u,v)=\\sum_{(x, y)\\in W}[I(x+u, y+v)-I(x, y)]^{2}\n窗口移动前后像素值变化\n类似于欧式距离\n\n\n\n\nE(u, v)=\\sum_{( x, y )\\in W} w(x,y)[I(x+u, y+v)-I(x, y)]^{2}\nwindow function 取窗口为矩形波或者高斯窗口\n\n\n\nFirst-order Taylor approximation for small motions [u, v]:\n\n\nI(x+u, y+v) \\approx I(x, y)+I_{x} u+I_{y} v\n一维泰勒：\n\n\nf(x_0+h)=f(x_0)+\\frac{f'(x_0)}{1!}h+\\frac{f''(x_0)}{2!}h^2+o(h^2)\n二维泰勒：\n\n\nf(x_0+h,y_0+k)=f(x_0,y_0)+\\frac{f'_x(x_0,y_0)h+f'_y(x_0,y_0)k}{1!}+\\frac{f_{xx}''(x_0,y_0)}{2!}h^2+\\frac{f_{xy}''(x_0,y_0)} {2!}hk+\\frac{f_{yx}''(x_0,y_0)}{2!}hk+\\frac{f_{yy}''(x_0,y_0)}{2!}k^2+...\n代入$E(u,v)$：\n\n\nE(u, v)=\\sum_{(x, y) \\in W}\\left(I_{x} u+I_{y} v\\right)^{2}=\\sum_{(x, y) \\in w} I_{x}^{2} u^{2}+2 I_{x} I_{y} u v+I_{y}^{2} v^{2}\nE(u, v) \\approx u^{2} \\sum_{x, y} I_{x}^{2}+2 u v \\sum_{x, y} I_{x} I_{y}+v^{2} \\sum_{x, y}I_{y}^{2}\n\nE(u, v) \\approx[u, v]\\left[\\begin{array}{cc}\n\\sum_{x,y\\in w} I_{x}^{2} & \\sum_{x,y\\in w} I_{x} I_{y} \\\\\n\\sum_{x,y\\in w} I_{x} l_{y} & \\sum_{x,y\\in w} I_{y}^{2}\n\\end{array}\\right]\\left[\\begin{array}{l}\nu \\\\\nv\n\\end{array}\\right]6.2 研究$M$​矩阵\nA horizontal slice ” of 𝐸(𝑢,𝑣)is given by the equation of an ellipse: 做垂直切片，即令E=const\n\n\n\\left[\\begin{array}{ll}\nu & v\n\\end{array}\\right] M\\left[\\begin{array}{l}\nu \\\\\nv\n\\end{array}\\right]=\\mathrm{const}\n\nM =\\left[\\begin{array}{cc}\n\\sum_{x,y\\in w} I_{x}^{2} & \\sum_{x,y\\in w} I_{x} I_{y} \\\\\n\\sum_{x,y\\in w} I_{x} l_{y} & \\sum_{x,y\\in w} I_{y}^{2}\n\\end{array}\\right]6.2.1 假设窗口只朝x，y方向移动\n$M$​矩阵为对角矩阵，记为：\n\n\nM =\\left[\\begin{array}{cc}\na & 0\\\\\n0 & b\n\\end{array}\\right]\n即没有旋转角度的影响，角点刚好和x，y方向平行\n\n\n[u, v]\\left[\\begin{array}{cc}\na & 0\\\\\n0 & b\n\\end{array}\\right]\\left[\\begin{array}{l}\nu \\\\\nv\n\\end{array}\\right]=1\n\nau^2+bv^2=1\n\\frac{u^2}{(a^{-\\frac{1}{2}})^2}+\\frac{v^2}{(b^{-\\frac{1}{2}})^2}=1\n这个式子说明，$a,b$都不接近0，则为角点，这说明E随u,v呈现至少两个方向的变化。    \n当a或b其中一个接近于0，则为边缘，因为此时椭圆坍塌为一条直线，这说明E随着u,v变化只有一个方向。\n当a和b都很接近于零，则说明，E随着u,v基本不变，则说明此时该部分为平滑区域。\n\n6.2.2 假设窗口只朝有一定转角\n由于：\n\n\nM =\\left[\\begin{array}{cc}\n\\sum_{x,y\\in w} I_{x}^{2} & \\sum_{x,y\\in w} I_{x} I_{y} \\\\\n\\sum_{x,y\\in w} I_{x} l_{y} & \\sum_{x,y\\in w} I_{y}^{2}\n\\end{array}\\right]\n我们可以知道$M$是对称矩阵，所以$M$可以进行正交分解：\n\n\nM=R^{-1}\\left[\\begin{array}{cc}\n\\lambda_{1} & 0 \\\\\n0 & \\lambda_{2}\n\\end{array}\\right] R\n不难发现，$R$是决定旋转的旋转矩阵，$\\lambda_1,\\lambda_2$为$M$的特征值，所以真正决定该处是否为角点的是$M$的特征值。\n\n\n\n6.3 改进\nR=\\operatorname{det}(M)-\\alpha \\operatorname{trace}(M)^{2}=\\lambda_{1} \\lambda_{2}-\\alpha\\left(\\lambda_{1}+\\lambda_{2}\\right)^{2}\n用这种方法，可以避免计算矩阵的特征值。\n\n\n\n如果$\\lambda_1,\\lambda_2$可以比拟，且均不接近零，可以推出$R&gt;0$，则为角点：\n\n\n\\begin{array}{cc}\nR=\\lambda_{1} \\lambda_{2}-\\alpha\\left(\\lambda_{1}+\\lambda_{2}\\right)^{2} \\\\\n=\\lambda^2-\\alpha(4\\lambda^2)\\\\\n=0.84\\lambda^2 > 0\n\\end{array}\n如果$\\lambda_1,\\lambda_2$​可以比拟，且均接近零，可以推出$|R|\\approx 0$​​​，则为平滑区域：\n\n\n\\begin{array}{cc}\nR=\\lambda_{1} \\lambda_{2}-\\alpha\\left(\\lambda_{1}+\\lambda_{2}\\right)^{2} \\\\\n=\\lambda^2-\\alpha(4\\lambda^2)\\\\\n=0.84\\lambda^2 \\approx 0\n\\end{array}\n如果$\\lambda_1\\gg\\lambda_2$，则可以推出$R&lt;0$,为边缘处：\n\n\nR=\\lambda_1\\lambda_2-\\alpha\\lambda_1^2","categories":["CV"]},{"title":"Image Segmentation","url":"/2021/08/15/cv/6.%20Segmentation/","content":"Image Segmentation\n\n1. Image Segmentation1.1 Def\nldentify groups of pixels that go together\n把图像分割为几个不相交的单连通区域\n\n\n\n\n1.2 The Goals of Segmentation1.2.1 Separate image into coherent(相干) “objects”\n\nGroup together similar-looking pixels for efficiency offurther processing\n\n\n1.3 Some Problems\n过分割：分割力度过细\n欠分割：分割过于粗糙\n\n2.  Clustering\nOne way to think about “segmentation“ is Clustering.\nClustering: group together similar data points and represent them with asingle token\n\n2.2 Methords\nBottom up clustering\n\ntokens belong together because they are locally coherent. (像素级别，像素特征上类似)\n\n\nTop down clustering\n\ntokens belong together because they lie on the same visual entity(object, scene…) （语义级别，同样的马、鼻子等）\n\n\nThese two are not mutually exclusive\n\n3. lnspiration from psychology\nMuller-Lyer illusion\n下面的直线感觉会更长\n\n\n\n\n3.1 The Gestalt Theory\nGestalt: whole or group\nWhole is greater than sum of its parts\nRelationships among parts can yield new properties/features\n通过组合形成新的关系\n\n\n\n\n\n\n\nPsychologists identified series of factors that predispose(倾向) set of elements to be grouped (by human visual system)\nstand at the window and see a house, trees, sky.Theoretically l might say there were 327 brightnessesand nuances of colour. bo l have “327”? No. I have sky,house, and trees.”                   Max Wertheimer\n\n\n那么如何找到这种整体的关系呢？\n\n3.2 Gestalt Factors\n\n共同命运：即有相同运动趋势\n\n3.3 Continuity through Occlusion Cues 遮挡3.4 Figure-Ground Discrimination\n图像背景区别\n\n\n3.5 Grouping phenomena in real life\n4. Clustering for Summarization\n\n算相邻点到中心点的距离，进行分类\nGoal: choose three “centers” as the representative intensities, and label every pixel according to which of these centers it is nearest to.\nBest cluster centers are those that minimize Sum of SquareDistance (SSD) between all points and their nearest cluster center $c_i$​\n最小化所有点到所有中心距离的加权和\n\n\n\n\nc^{*}, \\delta^{*}=\\underset{c, \\delta}{\\arg \\min } \\frac{1}{N} \\sum_{j}^{N} \\sum_{i}^{K} \\delta_{i j}\\left(c_{i}-x_{j}\\right)^{2}\n$\\delta_{i j}$: whether $x_j $​is assigned to $c_i$\n\n5. K-means clustering\nBasic idea: randomly initialize the k cluster centers, and iterate between the two steps we just saw.\n\nstep1: Randomly initialize the cluster centers, $c_1,..,c_K$​​​\nstep2: Given cluster centers, determine points in each cluster\nstep3: For each point $x_j$, find the closest $c$;. Put $x_j$into cluster $i_3$. Given points in each cluster, solve for $c$\nstep4: Set $c$ to be the mean of points in cluster $i_4$. lf c; have changed, repeat Step 2-3.\n\n\nAn iterative clustering algorithm\n\nPick K random points ascluster centers (means)- Alternate:\nAssign data instances to closest mean\nAssign each mean to the average of its assigned points\n\n\n\n5.1 Problem\n很依赖于初始值\n\nA local optimum:\n\n\n\n5.2 Pros  and cons5.2.1 Pros\nSimple, fast to compute\n\n5.2.2 Cons/issues\nConverges to local minimum of within-cluster squared error\n\nSetting k?\n\nk 是个超参\n\n\nSensitive to initial centers. \n\n对一开始初始化的中心很敏感\n\n\nSensitive to outliers\n\n\n\n\nDetects spherical(球形) clusters \n\n\n\nAssuming means can be computed\n如果一些特性不能用均值\n\n\n\n5.3 Qestions5.3.1 local minimum\nWill K-means converge?\n是收敛的，可以证明\n\n\nTo a global optimum?\n不能\n\n\n\nSolution: Kmeans++\nCan we prevent arbitrarily bad local minima?\nRandomly choose first center.\nPick new center with prob. proportional to (r-c)(Contribution of $x$ to total error) 以距离为概率选择下一个中心点\nRepeat until $k$​​ centers.\n\n\n\n\n\n下面结合一个简单的例子说明K-means++是如何选取初始聚类中心的。数据集中共有8个样本，分布以及对应序号如下图所示：\n\n\n\n假设经过图2的步骤一后6号点被选择为第一个初始聚类中心，那在进行步骤二时每个样本的D(x)和被选择为第二个聚类中心的概率如下表所示：\n\n\n\n其中的P(x)就是每个样本被选为下一个聚类中心的概率。最后一行的Sum是概率P(x)的累加和，用于轮盘法选择出第二个聚类中心。方法是随机产生出一个0~1之间的随机数，判断它属于哪个区间，那么该区间对应的序号就是被选择出来的第二个聚类中心了。例如1号点的区间为[0,0.2)，2号点的区间为[0.2, 0.525)。\n从上表可以直观的看到第二个初始聚类中心是1号，2号，3号，4号中的一个的概率为0.9。而这4个点正好是离第一个初始聚类中心6号点较远的四个点。这也验证了K-means的改进思想：即离当前已有聚类中心较远的点有更大的概率被选为下一个聚类中心。可以看到，该例的K值取2是比较合适的。\n当K值大于2时，每个样本会有多个距离，需要取最小的那个距离作为D(x)。一开始只有一个中心，之后会有下一个，那么此时每个点都要遍历两个中心，则距离取最小值即可，物理意义就是希望下一个中心尽可能离这两个中心远一点\n下面是选择中心点的代码\n\n# coding: utf-8import mathimport randomfrom sklearn import datasetsdef euler_distance(point1: list, point2: list) -&gt; float:    &quot;&quot;&quot;    计算两点之间的欧拉距离，支持多维    &quot;&quot;&quot;    distance = 0.0    for a, b in zip(point1, point2):        distance += math.pow(a - b, 2)    return math.sqrt(distance)def get_closest_dist(point, centroids):    min_dist = math.inf  # 初始设为无穷大    for i, centroid in enumerate(centroids):        dist = euler_distance(centroid, point)        if dist &lt; min_dist:            min_dist = dist    return min_distdef kpp_centers(data_set: list, k: int) -&gt; list:    &quot;&quot;&quot;    从数据集中返回 k 个对象可作为质心    &quot;&quot;&quot;    cluster_centers = []    cluster_centers.append(random.choice(data_set))    d = [0 for _ in range(len(data_set))]    for _ in range(1, k):        total = 0.0        for i, point in enumerate(data_set):            d[i] = get_closest_dist(point, cluster_centers) # 与最近一个聚类中心的距离            total += d[i]        total *= random.random()        for i, di in enumerate(d): # 轮盘法选出下一个聚类中心；            total -= di            if total &gt; 0:                continue            cluster_centers.append(data_set[i])            break    return cluster_centersif __name__ == &quot;__main__&quot;:    iris = datasets.load_iris()    print(kpp_centers(iris.data, 4))\n5.3.2 spherical cluster\nWill it always find the true patterns in the data?\n不一定，他只能找到球状分布\nif the patterns are very clear?\n\n\n\nSolution: GMMA probabilistic variant of Kimeans:\n\nE step: “soft assignment”of points to clusters   （标记属于某个类的概率）\nestimate probability that a point is in a cluster \n先随机初始化一个概率分布，就可以计算每个点归类的概率\n\n\n\n\nMstep: update cluster parameters\nmean and variance info (covariance matrix)\nmaximizes the likelihood of the points given the clusters\n\n\n\n\n\n理论上足够多的高斯分布可以拟合任意数据点\n\n详细见参考阅读GMM\n\n\n5.3.3 How to choose the number of clusters?\nTry different numbers of clusters in a validation set and look at performance.\nWe can plot the objective function values for k equals 1 to 6…\nThe abrupt change at k = 2, is highly suggestive of two clustersin the data. This technique for determining the number of clusters is known as “knee finding or “elbow finding.\n\n\n6. Smoothing Out Cluster Assignments\n聚类目标内部不连续\n\n\n\n引入空间坐标的信息\nK-means clustering based on intensity or color is essentially vector quantization（量化） of the image attributes\n基于强度或颜色的K均值聚类本质上是矢量量化(量化） 图像属性的分类\n\n\nClusters don’t have to be spatially coherent （空间相关性）\n\n\n6.1 Spatial Coherence\nWay to encode both similarity and proximity.\n实际上就是在原本三维向量的基础上，多加两维空间信息\n\n\n7. Mean-Shift Segmentation\nAn advanced and versatile technique for clustering-based segmentation\n\n7.1 Algorithm 找到区域空间密度的极大值\n随机初始化一个圆，然后找到点的重心，再以该重心为中心做圆，从而达到漂移的效果\n\n\n\n在一个 n 维空间内，存在一个点 x，以该点为球心，以 h 为半径，生成一个球 $S_h$，计算球心到球内所有点生成的向量的均值，显然这个均值也是一个向量，这个向量就是 mean shift 向量\n\n公式如下:\n\n\n\nM_{h}(x)=\\frac{1}{k} \\sum_{x_{i} \\in S_{h}}\\left(x_{i}-x\\right)\n可以这么理解以上公式，我们定义了一个叫mean-shift的东西，它用于计算漂移距离\n\n\n7.2 shift point\n我给起个名字，叫 偏移点；\n注意，几乎没有资料专门提到这个概念，我为什么要讲呢？因为我们需要把 shift point 和 mean shift 区分开，这俩可不是一回事；\n而在我们写算法时，需要的是 shift point，而不是 mean shift\n\n\n\\hat{M}_n(X)=\\frac{1}{k}(x_i),\\text{shift point}\n7.3 基本思想\n对于 样本中的每一个点 $x$，做如下操作\n\n以 x 为起点，计算他的 $\\text{shift point } x’$​，然后把 该点 “移动” 到 x’.【注意不是真的移动点，而是把 x 标记成 x’】\n\n以 $x’ $为新起点，计算他的 shift point\n\n重复 前两步，直至 前后两次 的 mean shift 向量满足条件，如 距离很近【这一步才用到 mean shift，也就是 前后两个 shift point 相减得到 向量，再计算向量的模】\n\n把 $x$ 标记为 最终的 shift point，即为对应的类\n\n遍历计算所有点\n\n\n过程大致如下图：\n\n\n从上图可以看到，mean shift 向量指向了更密集的区域，也就是说 mean shift 算法是在寻找 最密集 的区域，作为最后的类别\n\n存在问题\n\n在计算 mean shift 向量时，圆圈内所有点的贡献是一样的 即1/k，而实际上离圆心越远可能贡献越小，\n为此 mean shift 算法引入核函数来表达这种贡献，代替 1/k\n\n\n\n7.4 引入核函数\n此处以 高斯核函数 为例\n\n\nN(x)=\\frac{1}{\\sqrt{2 \\pi} h} e^{-\\frac{x^{2}}{2 h^{2}}}\n其中 h 代表核函数的带宽(bandwidth)【这个 h 和 高斯分布 里的 标准差σ 类似，但它不是 标准差，而是 人工指定的，但是起到的作用和 标准差一样】\nh 越小，衰减为 0 的速度就越快，也就是说 mean shift 向量对应的球 Sh 越小，稍微远点就没有贡献了；\n于是，引入 核函数 的 mean shift 向量变成如下样子，此时的 Sh 可以为整个数据集（原因为上句）\n\n\nM_{h}(x)=\\frac{\\sum_{i=1}^{n} G\\left(\\frac{x_{i}-x}{h_{i}}\\right)\\left(x_{i}-x\\right)}{\\sum_{i=1}^{n} G\\left(\\frac{x_{i}-x}{h_{i}}\\right)}\n代码见：Mean shift 【1】- 基本原理 - 努力的孔子 - 博客园 (cnblogs.com)\n\n证明：\n\n\n7.4 如何聚类\nCluster: all data points in the attraction basin of amode\n模式吸引区中的所有数据点\n\n\nAttraction basin (吸引池): the region for which all trajectories（轨迹） lead to the same mode\n对于指定一片区域，该区域中所有点最终会漂移到同一个点，这个区域被称为吸引区\n遍历所有数据点，最后数据点最终漂移到哪一个中心，那么最后就是属于哪一个类\n\n\n\n\n7.5 如何在数字图像上应用？\nFind features (color, gradients, texture, etc)\nInitialize windows at individual pixel locations 在各个像素位置初始化window\nPerform mean shift for each window until convergence \nMerge windows that end up near the same “peak” or mode 合并峰值点\n可以理解为，虽然一开始生成了很多window，但是最终对于峰值接近的重心点，我们可以进行合并\n\n\n\n\n7.6 speedups\n遍历所有点，速度太慢\n\n\n\n我们可以定义对于圆内r/c部分的点和中心点都会最终漂移到同一个点，那么这部分点就不用再遍历\n\n\n\n一开始初始化多个圆，然后依次进行漂移，从而实现聚类\n\n7.7 Summary Mean-Shift\nPros\n\nGeneral, application-independent tool\nModel-free, does not assume any prior shape (spherical, elliptical, etc.)on data clusters\nJust a single parameter (window size h)\nh has a physical meaning (unlike k-means)· \n\n\nFinds variable number of modes\nRobust to outliers\n\n\nCons\n\nOutput depends on window size\nComputationally (relatively) expensive\n\n\n\n8. Graph-based segmentation\n8.1 Efficient graph-based segmentation\n\nRuns in time nearly linear in the number of edges \n时间复杂度和边数相关\n\n\nEasy to control coarseness（粗糙度） of segmentations\nResults can be unstable\n\n8.2 Segmentation by graph cuts\n\n断掉某些边，把没连通的集合分为两类\n\n\n\nA graph cut is a set of edges whose removal disconnects the graph\n图割是一个从原图断掉联系的边集，去掉该边集，会使得原图不在连通即不相连\n\n\nCost of a cut: sum of weights of cut edges\n即图割中所有边的权值和\n\n\n\n\n\n断掉相似度最小的边\n\n8.3 Measuring affinity（亲和力） （定义相似度）\nOne possibility:\n特征上的距离差异，与相似度成反比\n\n\n\n\na f f(x, y)=\\exp (-\\frac{\\|x-y\\|^{2}}{2 \\sigma_{d}^{2}})\n\n8.4 Cuts in a graph: Min cut 最小割\n\nLink Cut: set of links whose removal makes a graph disconnected\ncost of a cut:\n\n\\operatorname{cut}(A, B)=\\sum_{p \\in A, q \\in B} w_{p, q}Find minimum cut：\n\ngives you a segmentation\n\nfast algorithms exist for doing this\n断掉的权重值之和最小\n\n\n\n8.5 Problem\nMinimum cut tends to cut off very small, isolated components\n容易造成孤立点分割，因为这些点可能和其他点的相似度都很低，所以自然就被切割走了·\n\n\n\n\n8.6 Cuts in a graph: Normalized cut\nNormalized Cut\nfix bias of Min Cut by normalizing for size of segments:\nN \\operatorname{cut}(A, B)=\\frac{\\operatorname{cut}(A, B)}{\\operatorname{assoc}(A, V)}+\\frac{\\operatorname{cut}(A, B)}{\\operatorname{assoc}(B, V)}\n$\\operatorname{assoc}(\\mathrm{A}, \\mathrm{V})=$​ sum of weights of all edges that touch $\\mathrm{A}$​\n\n即除以割开后，分别处以两个图的权重之和，再相加\n这样可以使得，分割后的连个cluster的权重之和尽可能均衡\n\n\nNcut value small when we get two clusters with many edges with high weights, and few edges of low weight between them\n\n尽可能使得同一类多边\n\n\nApproximate solution for minimizing the Ncut value : generalized eigenvalue problem.\n求解方法：广义特征值\n\n\n\n8.7 Normalized cuts: Pro and conPro\nGeneric framework, can be used with many different features and affinity formulations\n\nCon\nHigh storage requirement and time complexity:\ninvolves solving a generalized eigenvalue problem of size $n \\times n$​, where n is the number of pixels\n\n\n\n9. Feature Space\nDepending on what we choose as the feature space, we can group pixels in different ways.\nGrouping pixels based on color similarity\n\n\n\ncolor, brightness, position alone are not enough to distinguish all regions…\n\n9.1 Recall: texture representation example\n分别计算一个小窗口里水平方向梯度的均值以及垂直方向梯度的均值，作为这个小窗口的特征，再进行聚类\n\n\n\n9.2 Segmentation with texture features\nFind “textons” （文本） by clustering vectors of filter bank outputs. \n查找“内容” 通过对滤波器组输出向量进行聚类。\n\n\nDescribe texture in a window based on texton histogram\n对于每个小窗口，我们可以得到多个filter在该小窗口的多个值，我们可以以filtter为横坐标建立直方图\n\n\n\n\n9.3 lmage segmentation example\n\n利用多特征融合，实现通过不同特征分割的手段，融合出较好的图像分割结果\n\n9.4 Segments as primitives for recognition\n\n一辆车可能方位形状不一样，且经常有遮挡，但我们也要完成这个任务\n\n10. Summary\nSegmentation to find object boundaries or mid-level regions,tokens.\nBottom-up segmentation via clustering\nGeneral choices — features, affinity functions, and clustering algorith ms\nGrouping also useful for quantization, can create new featuresummaries\nTexton histograms for texture within local region· \nExample clustering methods\nK-means\nMean shift\nGraph cut, normalized cuts\n\n\n\n","categories":["CV"]},{"title":"fitting","url":"/2021/08/15/cv/5.%20fitting/","content":"fitting\n\n1. Def\nChoose a parametric model to represent a set of features\n\n2. Challenges\nNoise in the measured feature locations\n\nExtraneous data（外点）: clutter (outliers), multiple lines 交叉干扰\n\nMissing data: occlusions 遮挡\n\n\n3. Overview\nIf we know which points belong to the line, how do we find the “optimal”line parameters?\nLeast squares\n\n\nWhat if there are outliers?\nRobust fitting, RANSAC\n\n\nWhat if there are many lines?\nVoting methods: RANSAC, Hough transform\n\n\nWhat if we’re not even sure it’s a line?\nModel selection: Snake (not covered)\n\n\n\n4. Least squares line fitting\nData: $\\left(x_{1}, y_{1}\\right), \\ldots,\\left(x_{n}, y_{n}\\right)$\nLine equation: $y_{i}=m x_{i}+b$\nFind $(m, b)$ to minimize\n\n\nE=\\sum_{i=1}^{n}\\left(y_{i}-m x_{i}-b\\right)^{2}\nE=\\|Y-X B\\|^{2} \\text { where } Y=\\left[\\begin{array}{c}\ny_{1} \\\\\n\\vdots \\\\\ny_{n}\n\\end{array}\\right] \\quad X=\\left[\\begin{array}{cc}\nx_{1} & 1 \\\\\n\\vdots & \\vdots \\\\\nx_{n} & 1\n\\end{array}\\right] \\quad B=\\left[\\begin{array}{c}\nm \\\\\nb\n\\end{array}\\right]\n\n是样本点均匀分布在直线两边\n\n4.1 Least squares line fitting\n\\begin{array}{l}\nE=\\| Y-X B\\|^{2}\\\\\n=(Y-X B)^{T}(Y-X B)\\\\\n=Y^{T}-2(X B)^{T} Y+(X B)^{T}(X B)\n\\end{array}\n\\begin{aligned}\n&\\frac{d E}{d B}=2 X^{T} X B-2 X^{T} Y=0 \\\\\n&X^{T} X B=X^{T} Y\n\\end{aligned}\nNormal equations: least squares solution to B\n\n\nB=\\left(X^{T} X\\right)^{-1} X^{T} Y4.2 Problem\nNot rotation-invariant\nFails completely for vertical lines\n\n\n4.3 Total least squares\nDistance between point $(x_i, y_i)$​​ and line  利用点直线的距离$ax+by=d (a^2+b^2=1): |ax_i + by_i – d|$​​​\n\n\nax+by=d\n\\text{distance}=\\frac{|ax_i+by_i-d|}{\\sqrt{a^2+b^2}}=|ax_i+by_i-d|\nFind $ (a, b, d) $​ to minimize the sum of squared perpendicular distances\n\n\n\nE=\\sum_{i=1}^{n}\\left(a x_{i}+b y_{i}-d\\right)^{2}\n\\frac{\\partial E}{\\partial d}=\\sum_{i=1}^{n}-2\\left(a x_{i}+b y_{i}-d\\right)=0\nd=\\frac{a}{n} \\sum_{i=1}^{n} x_{i}+\\frac{b}{n} \\sum_{i=1}^{n} y_{i}=a \\bar{x}+b \\bar{y}\n\\begin{array}{ll}\\\\\nE=\\sum_{i=1}^{n}\\left(a\\left(x_{i}-\\bar{x}\\right)+b\\left(y_{i}-\\bar{y}\\right)\\right)^{2}\\\\\n\\ \\ \\ =\\left\\|\\left[\\begin{array}{cc}\nx_{1}-\\bar{x} & y_{1}-\\bar{y} \\\\\n\\vdots & \\vdots \\\\\nx_{n}-\\bar{x} & y_{n}-\\bar{y}\n\\end{array}\\right]\\left[\\begin{array}{c}\na \\\\\nb\n\\end{array}\\right]\\right\\|^{2}\\\\\n\\ \\ \\ =(U N)^{T}(U N) \n\\end{array}\n\\frac{d E}{d N}=2\\left(U^{T} U\\right) N=0\n$A$ is a $n \\times n$ matrix, $x$ is a non-zero vector, if\n\n\nA x=\\lambda x, \\quad x \\neq 0\n$x$​ is one eigenvectors of $A$​, and $\\lambda$​ is one of eigenvalues.\n\n\n\\frac{d E}{d N}=2\\left(U^{T} U\\right) N=0\nSolution to $(U^TU)N=0$, s.t. $|N|^2=1$:\n\n\nU=\\left[\\begin{array}{cc}\nx_{1}-\\bar{x} & y_{1}-\\bar{y} \\\\\n\\vdots & \\vdots \\\\\nx_{n}-\\bar{x} & y_{n}-\\bar{y}\n\\end{array}\\right]\nN=\\left[\\begin{array}{cc}\na\\\\\nb\n\\end{array}\\right]\neigenvector of $U^T U$​​ associated with the smallest eigenvalue. 即对应最小特征值的特征向量为其解\n\n\n\n$(a,b)^T\\dotproduct(x_i-\\bar{x},y_i-\\bar{y})$相当于投影距离\n\n4.5 Least squares as likelihood maximization\nGenerative model: line points are sampled independently and corrupted by Gaussian noise in the direction perpendicular(垂直的) to the line\n即每个点会被独立采样，且其到直线的距离符合高斯分布\n\n\n\n\n\n\nLikelihood of points given line parameters (a, b, d):\n\n\n\\varepsilon=\\frac{(ax_{i}+by_{i}-d)^2}{\\sqrt{a^{2}+b^{2}}} \\Rightarrow \\varepsilon=(a x_{i}+b y_{i}-d)^2\\sim N\\left(0,\\sigma^{2}\\right)\n\\begin{aligned}\nP\\left(x_{1}, y_{1}, \\ldots, x_{n}, y_{n} \\mid a, b, d\\right) &=\\prod_{i=1}^{n} P\\left(x_{i}, y_{i} \\mid a, b, d\\right) \\\\\n& \\propto \\prod_{i=1}^{n} \\exp \\left(-\\frac{\\left(a x_{i}+b y_{i}-d\\right)^{2}}{2 \\sigma^{2}}\\right)\n\\end{aligned}\nLog-likelihood:\n\n\nL\\left(x_{1}, y_{1}, \\ldots, x_{n}, y_{n} \\mid a, b, d\\right)=-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(a x_{i}+b y_{i}-d\\right)^{2}4.6 Problem:\nProblem: squared error heavily penalizes outliers \n\n\n5. Robust estimators\nGeneral approach: find model parameters $\\theta$ that minimize\n\n\n\\sum_{i} \\rho\\left(u_{i}\\left(x_{i}, \\theta\\right) ; \\sigma\\right)\n$u_{i}\\left(x_{i}, \\theta\\right)-$​​ residual of ith point w.r.t. model parameters $\\theta$​​ $\\rho$​​ - robust function with scale parameter $\\sigma$​​​\n可以理解为给原先的距离加了个robust function\n\n\n\n\n\nThe robust function ρ behaves like squared distance for small values of the residual u but saturates(平滑) for larger values of u\n\nAttention：\n\nRobust fitting is a nonlinear optimization problem that must besolved iteratively\nLeast squares solution can be used for initialization \nScale of robust function should be chosen carefully\n\n\n\\rho(u ; \\sigma)=\\frac{u^{2}}{\\sigma^{2}+u^{2}}\n\n$\\sigma$​​ too big: The error value is almost the same for every point and the fit is very poor\n\n\n\n$\\sigma$ too large: Behaves much the same as least squares\n\n6. RANSAC\nRobust fitting can deal with a few outliers 离群值 —— what if we have very many?\nRandom sample consensus (RANSAC):Very general framework for model fitting in the presence of outliers \nOutline\nChoose a small subset of points uniformly at random. 随机均匀采样\nFit a model to that subset 拟合模型\nFind all remaining points that are “close” to the model and reject the rest asoutliers(去除外点) 记录模型的内点数\nDo this many times and choose the best model 超过内点数阈值的model\n\n\nLeast squares fit\n\n\n\nRandomly select minimal subset of points:\n\n\n\nHypothesize a model 假设模型\n\n\n\nCompute error function 计算剩余点到直线的距离\n\n\n\nSelect points consistent with model 记录内点（设置距离阈值）\n\n\n\nRepeat hypothesize and verify loop\n\n\n\nUncontaminated(未污染) sample\n\n\n\n从而得到内点最多的模型为我们需要的模型\n\nSteps:\n\nRepeat $N$ times:\nDraw $s$ points uniformly at random.Fit line to these s points\nFind inliers to this line among the remaining points (i.e., points whose distance from the line is less than $t$)\nlf there are $d$​ or more inliers, accept the line and refit using allinliers 如果内点足够多，则用这些内点重新拟合，并得到这个model\n有k个模型，内点数目大于d，这些都可以输出\n\n6.1 Choosing the parameters\nlnitial number of points $s$\nTypically minimum number needed to fit the model. \n\n\nDistance threshold $t$​\nNumber of iterations $N$\n\nChoose $N$ so that, with probability $p$​, at least one random sample is free from outliers （没有异常值） (e.g. p=0.99) (outlier ratio: e)\n有0.99的置信度，在N次迭代后，进行随机采样中至少有一个点不是外点\n\n\nN 如何选？假设100次迭代后，我们采样得到的是outlier，那么可以用公式表示\n\n\n\n\\begin{array}{l}\n\\left(1-(1-e)^{s}\\right)^{N}=1-p \\\\\nN=\\log (1-p) / \\log \\left(1-(1-e)^{s}\\right)\n\\end{array}\n$(1-e)^s$表示随机采样s个点，都是内点，$(1-(1-e)^s)$​一次迭代中随机采样的点有外点\n\nProblem:\n\nOutlier ratio e is often unknown a priori, so pick worst case, e.g. 50%,and adapt if more inliers are found, e.g. 80% would yield e=0.2 即以最坏情况找到的内点百分比，来计算e\n\nAdaptive procedure:\n\n$N=\\infty$, sample_count $=0$​ 一开始初始迭代无穷次，采样0次\n\nWhile $N&gt;$​ sample_count 当当前的迭代数大于已经采样数，则继续运算\n\nChoose a sample and count the number of inliers\n\n采样一个样本，去拟合模型，算出内点\n\nIf inlier ratio is highest of any found so far, set\n $\\mathrm{e}=1-($​ number of inliers $) /($​​ total number of points)\n\n如果内点率升高，则更新e，因为这说明e应该更小\n\n\nRecompute $N$ from $e$​​​ : 更新N\n\n\n\nN=\\log (1-p) / \\log \\left(1-(1-e)^{s}\\right)\nIncrement the sample_count by 1\n\n\n\n\nsample count指的是总的拟合次数，即对每个N进行搜索，N随着拟合会逐渐下降，直到降到总拟合次数以下\n\nlnitial number of points s\n\nTypically minimum number needed to fit the model\n\n\nDistance threshold t\n\nChoose t so probability for inlier is p (e.g.0.95). Zero-mean Gaussian noise with std.dev. o: t2=3.84?.\n\n\nNumber of samples N\n\nChoose N so that, with probability p, at least one random sample is free from outliers (e.g.p=0.99) (outlier ratio: e)\n\n\nConsensus set size d 判断该模型是否正确\n\nShould match expected inlier ratio\n\n\n\n6.2 RANSAC pros and cons\nPros\nSimple and general\nApplicable to many different problems\nOften works well in practice\n\n\n\n\nCons\nLots of parameters to tune 参数过多\nDoesn’t work well for low inlier ratios (too many iterations,or can fail completely)  内点少则很难收敛\nCan’t always get a good initialization of the model based on the minimum number of samples 很难得到好的初始值\nRefine by Least Squares\n\n\n\n6.3 平面拟合\n\n\\left[\\begin{array}{cc}\nx_a\\\\\ny_a\n\\end{array}\\right]=\n\\left[\\begin{array}{cc}\na & b & c\\\\\nd & e & f\n\\end{array}\\right]\n\\left[\\begin{array}{cc}\nx_b\\\\\ny_b\\\\\n1\n\\end{array}\\right]\n至少要三个点才能拟合\n\n\n7. Hough Transform7.1 Voting schemes\nLet each feature vote for all the models that are compatible（兼容） with it\nHopefully the noise features will not vote consistently for any single model 噪点不会在单独的模型下拟合\nMissing data doesn’t matter as long as there are enough features remaining to agree on a good model 数据足够\n\n\n\n7.2 An early type of voting scheme\nDiscretize parameter space into bins\nFor each feature point in the image, put a vote in every bin in the parameter space that could have generated this point\nFind bins that have the most votes\n\n\n\n在参数空间进行投票，每个特征点都要在参数空间投票，投票最多的bins，为所需要的参数\n\n7.3 Parameter space representation\nA line in the image corresponds to a point in Hough space\n\n\n\nWhat does a point $(x_0 , y_0 )$ in the image space map to in theHough space?\n\nAnswer: the solutions of $y_0 = m x_0 + b$​\nThis is a line in Hough space\n\n可以理解为，过图像中的一点，在参数空间的表达式为一条直线\n\n\n\n\n\n\nWhere is the line that contains both $(x_0 , y_0 ) $and $(x_1 , y_1)$​？\nIt is the intersection of the lines $b = x_0 m + y_0$​ and $b = x_1 m + y_1$\n\n\n\n\n\nProblems with the ( m,b )\n\nUnbounded parameter domains\nVertical lines require infinite m （m，b）是无限的，所以（m,b）的范围无法限定\n\n\nAlternative: polar（极坐标） representation\n\n\n\n\nEach point $(x,y)$ will add a sinusoid(正弦波) in the $(\\theta,\\rho)$ parameter space\n\n7.4 Algorithm outline\nInitialize accumulator（累加器） H to all zeros\n\nFor each feature point ( x,y ) in the image\n\nFor θ = 0 to 180\n$\\rho=xcos\\theta + ysin\\theta$\n$H(\\theta,\\rho)=H(\\theta,\\rho)+1$\n\n\nend\n\n\nend\n\n遍历图像每个数据点与$\\theta$​，得到多个$\\rho$​，从而对bins进行投票\n\n\n\n7.5 Basic illustration\n\\rho=\\rho_0cos(\\theta+\\theta_0)\\ \\ s.t.\\theta_0=\\text{斜率}\n\\rho_0=\\frac{\\rho} {cos(\\theta+\\theta_0)}\\ for\\ any\\ line\\ through (\\rho,\\theta)\n所以我们可以发现，一条直线映射后是一个点\n而一个点映射后是sec\n\n\n\nOther shapes：\n\n\n\nA more complicated image\n\n\n\n具体过程是先识别出边缘点，再利用边缘点进行拟合\n\n7.6 Effect of noise\n\nPeak gets fuzzy and hard to locate\n由于一个点拟合后是余割，那么当有噪音时，会导致，这多个点拟合的余割无法准确交于一个点，造成难以定位\n\nNumber of votes for a line of 20 points with increasing noise:\n\n随着噪音增加，那么投票在一个bin的值就会减少\n\n7.7 Random points\n\nUniform noise can lead to spurious peaks in the array\n\n均匀噪声会导致阵列中出现杂散峰值\n\n\nAs the level of uniform noise increases, the maximum number ofvotes increases too:\n\n\n\n\n当随机噪声增加，会导致最大投票值也增加，这是因为噪声是均匀分布的，相当于以一定比例增加参数空间所有投票\n\n7.8 Dealing with noise\nChoose a good grid / discretization 利用稍微扩大的网格增加容错率\nToo coarse（粗）: large votes obtained when too many different lines correspond to a single bucket 有可能使得许多不同的直线投票到同一个桶，原因在于格子过粗，会导致丢失原本图像中的信息\nToo fine（细）: miss lines because some points that are not exactly collinear cast votes for different buckets 可以这么理解，就是刻画过于精细，导致无法容忍一点噪音，从而无法找到交点\n\n\nIncrement neighboring bins (smoothing in accumulator array) \n其意思是，在对一个格子投票时，同时对其领域进行投票，但是其投票总和还是1，如下图：\n\n\n\n\n\nTry to get rid of irrelevant features \nE.g.take only edge points with significant gradient magnitude\n\n\n\n7.9 Incorporating（合并） image gradients\nWhen we detect an edge point, we also know its gradient orientation\nBut this means the line is uniquely determined!\nModified Hough transform:\n\n因为对于一个边缘点，其实我们是可以唯一确定他的直线方向的，那么我们就不用对$\\theta$​进行量化了\n只需利用梯度的角度值，进行计算相应的$\\rho$，并对$\\rho$进行量化即可\n\n\nFor each edge point ( x,y )\n\nθ = gradient orientation at ( x,y）\nρ = x cos θ + y sin θ\nH(θ, ρ ) = H(θ, ρ ) + 1\n\n\nend\n\n7.10 Hough transform for circles\nHow many dimensions will the parameter space have?\nGiven an unoriented edge point, what are all possible bins that it canvote for?\nWhat about an oriented edge point?\n\n\n\n对于定义一个圆我们需要一个圆心和半径坐标\n\n\n\n而对于图中任意一个点，它映射到参数空间是一个圆锥\n\n\n(x_0-a)^2+(y_0-b)^2=r^2\n给定点$(x_0,y_0)$​，由于我们即不知道方向，也不知道半径，所以形成一个圆锥方程，其过$(x_0,y_0,0)$这一点：\n\n\n(a-x_0)^2+(b-y_0)^2=r^2\n如果我们知道半径大小，则退化为一个圆\n\n对于一个确定方向的点，映射为两个射线：\n\n\n\n\\left\\{ \\begin{array}{cc}\nx_0=a+rcos\\theta\\\\\ny_0=b+rsin\\theta\n\\end{array}\\right.\ntan\\theta=\\frac{y_0-b}{x_0-a} \\text{ is a constant}\n所以是两条过$(x_0,y_0)$的射线，\n\n\n\n\n显然，这是因为一个点梯度的垂直方向有两边，即可以确定两个圆\n我们已知每个边缘点的梯度，我们就可以以此计算出每个边缘点对应的圆心\n\n\n\\left\\{\\begin{array}{r}\na=x_0-rcos\\theta\\\\\nb=y_0-rsin\\theta\n\\end{array}\\right.\n再进行投票即可\n\n7.11 Application in recognition\nWe want to find a template defined by its reference point (center) and several distinct types of landmark points in stable spatial configuration\n学习特征点与中心点相对的关系\n\n\n\n7.10 Hough transform: Pros and cons\nPros\nCan deal with non locality and occlusion 可以处理非局部性和遮挡\nCan detect multiple instances of a model\nSome robustness to noise: noise points unlikely to contribute consistently to any single bin\n\n\nCons\nComplexity of search time increases exponentially with the number of model parameters\nNon target shapes can produce spurious （虚假的） peaks in parameter space 非目标形状可能会产生虚假形状(虚假的） 参数空间中的峰值\nIt’s hard to pick a good grid size\n\n\n\n","categories":["CV"]},{"title":"Image Segmentation","url":"/2021/08/15/cv/7.%20Recognition%20&%20Detection/","content":"Image Segmentation\n\nRecognition &amp; Detection1.  Introduction to recognition\n\n1.1 Activity recognition\nWhat are these people doing?\n\n\n\nwalking \nshopping\nrolling a cartsitting\ntalking\n…\n\n1.2 Categorization vs Single instance recognitionWhere is the crunchy（松脆的） nut?\n\n1.3 Visual Recognition\nDesign algorithms that have the capability to:\nClassify images or videos\nDetect and localize objects\nEstimate semantic and geometrical attributes. \nClassify human activities and events\n\n\n\n1.4 Why is it difficult?\nWant to find the object despite possibly large changes inscale, viewpoint, lighting and partial occlusion\n\n\n2. The machine learning framework\n\nTraining: given a training set of labeled examples${(x_1,y_1),…. , (x_N,y_N)}, $​estimate the prediction function $f$ by minimizing the prediction error on the training set\nTesting: apply $f$ to a never before seen test example $x$ and output the predicted value $y = f(x)$​\n\nApply a prediction function to a feature representation of the image to get the desired output:\n\n\n\n2.1 A simple pipeline - Training\n2.2 “Classic”recognition pipeline\n\nHand-crafted feature representation\n\n纹理\n边缘\n角点\n\n\nOff-the-shelf trainable classifier\n\n\n3. Bag of words\n3.1 Origin3.1.1 Origin 1: Texture Recognition\nTexture is characterized by the repetition of basic elements or textons\n\n\n\n\n统计图像中包含纹理基元的频率，从而作为特征向量\n\n3.1.2 Origin 2: Bag-of-words models\nOrderless document representation: frequencies of words from a dictionary salton &amp; McGill (1983)\n\n\n3.1.3 Bags of features for object recognition\n\nWorks pretty well for image-level classification and for recognizing object instances\n\n3.2 Bag of features\nFirst, take a bunch of images, extract features, and build up a”dictionary” or “visual vocabulary”——a list of common features\nGiven a new image, extract features and build a histogram - for each feature, find the closest visual word in the dictionary\n\n3.2.1 Bag of features: outline\nExtract features\n\n\n\nLearn “visual vocabulary”\n只选取有代表性的特征基元\n\n\n\n\n\nQuantize features using visual vocabulary\n\n算相似度，投影bins\n\n\nRepresent images by frequencies of “visual words”\n\n\n\n3.2.2 Feature extraction\nRegular grid\nVogel &amp; Schiele, 2003\nFei-Fei &amp;Perona,2005\n\n\n\n\n\nlnterest point detector\nCsurka et al. 2004\nFei-Fei&amp; Perona,2005Sivic et al. 2005\n\n\n\n\n\nOther methods\nRandom sampling (Vidal-Naquet &amp; Ullman, 2002). \nSegmentation-based patches (Barnard et al.2003)\n\n\n\n3.3 Learning the visual vocabulary\n\n把特征点进行聚类，得到聚类中心，从而得到视觉词汇\n对于SIFT，则将10000个128维向量缩小为3个128维，进而通过直方图，最后变成一个三维向量\n\n\n\n3.3.1 From clustering to vector quantization\nClustering is a common method for learning a visual vocabulary or codebook\n\nUnsupervised learning process\nEach cluster center produced by k-means becomes a codevector \nProvided the training set is representative, the codebook will be “universal”\n\n\nThe codebook is used for quantizing features\n\nA vector quantizer takes a feature vector and maps it to the index of the nearest codevector in a codebook\nCodebook = visual vocabulary\nCodevector= visual word\n简言之，就是通过聚类得到的聚类中心就是我们需要的code vector，然后code vector 组成codebook，对于后续图片的表达，只要借助于量化，即可\n\n\n\n\n\n3.3.2 Visual vocabularies\n3.3.3 Visual vocabularies: lssues\nHow to choose vocabulary size?\n\nToo small: visual words not representative of all patches\nToo large: quantization artifacts, overfitting\n\n\nComputational efficiency\n\nVocabulary trees(Nister &amp; Steweniu:s, 2006)\n\n\n\n\n\n训练阶段，进行树状分类，即递归调用k-means；测试阶段先进行粗分类\n\n\n3.3.4 Large-scale image matching\nBag-of-words models have been useful in matching an image to a large database of object instances\n\n\n3.3.5 Bags of features for object recognition\n3.4 What about spatial information?\n\n\n单纯使用上诉表示方法，会失去空间信息的特征\n\n3.4.1 Spatial pyramids （Spatial Pyramid Matching）\n\n\n将图像分成若干块(sub-regions)，分别统计每一子块的特征，最后将所有块的特征拼接起来，形成完整的特征。\n\n简介：\n假设存在两个点集$X$和$Y$（ 每个点都是$D$维的，以下将它们所在的空间称作特征空间）。将特征空间划分为不同的尺度$0,…,L$，在尺度$l$下特征空间的每一维划出$2^l$个cells，那么d维的特征空间就能划出$D=2^{dl}$​个bins；\n两个点集中的点落入同一个bin就称这两个点Match。在一个bin中match的总数定义为 $min(X_i, Y_i)$​​，其中$X_i$​​和$Y_i$​​分别是两个点集中落入第$i$​个bin的点的数目；\n统计各个尺度下match的总数$\\mathcal{I}^l$​（就等于直方图相交)。由于细粒度的bin被大粒度的bin所包含，为了不重复计算，每个尺度的有效Match定义为match的增量$\\mathcal{I}^l-\\mathcal{I}^{l+1}$\n不同的尺度下的match应赋予不同权重，显然大尺度的权重小，而小尺度的权重大，因此定义权重为$\\frac{1}{2^{L-l}}$​\n最终，两点集匹配的程度定义为：\n\n\n\\begin{aligned}\n\\kappa^{L}(X, Y) &=\\mathcal{I}^{L}+\\sum_{\\ell=0}^{L-1} \\frac{1}{2^{L-\\ell}}\\left(\\mathcal{I}^{\\ell}-\\mathcal{I}^{\\ell+1}\\right) \\\\\n&=\\frac{1}{2^{L}} \\mathcal{I}^{0}+\\sum_{\\ell=1}^{L} \\frac{1}{2^{L-\\ell+1}} \\mathcal{I}^{\\ell} \n\\end{aligned}\n我觉得要特别说明一下的就是这里的特征空间与前面两个点集的点所被描述的空间之间的关系——-没有关系，对，我觉得是没有关系，因此就有作者的SPM：\n将图像空间用构造金字塔的方法分解为多个scale的bins（通俗地说就是切分成不同尺度的方形）\n像BOW一样构造一本大小为M的dictionary，这样每个特征都能投影到dictionary中的一个word上。其中字典的训练过程是在特征空间中完成。论文中的特征利用的dense SIFT。\n统计每个bin中各个words的数目，最终两幅图像的匹配程度定义为：\n\n\n\n\nK^{L}(X, Y)=\\sum_{m=1}^{M} \\kappa^{L}\\left(X_{m}, Y_{m}\\right)\n注意，当L=0时，模型就退化成为BOW了。\n\nSPM介绍了两幅图像匹配的方法。如要用于场景分类，注意(2)式就等于$M(L+1)$​个直方图相交运算的和，其实也就等于一个更大的向量直接进行直方图相交运算而已。而这个向量，就等于每个被划分的图像子区域上的visual words直方图连在一起。这个特征，就是用来分类的特征。\n\n作者在实验中表明，不同L下，M从200取到400对分类性能影响不大，也就是降低了码书的大小对分类效果的影响。\n\n在本文最开始也提到了，这个方法可以作为一个模板，每个sub-region中统计的直方图可以多种多样，简单的如颜色直方图，也可以用HOG，这就形成了PHOG。SPM的matlab代码也可以从作者的主页上下载到(here)。只不过这种空间分类信息仍然有局限性——-一幅相同的图像旋转90度，匹配的结果就不会太高了。所以模型隐含的假设就是图像都是正着存储的（人都是站立的，树都是站立的…….）。另外空间Pyramid的分块方法也没有考虑图像中object的信息（仅仅是利用SIFT特征来描述了Object），这也是作者在文中承认的缺点。DPM，应该是考虑了这个问题的吧。\n\n\n4. “Classic” recognition pipeline\n4.1 Recall: Many classifiers to choose from\nK-nearest neighbor\nSVM\nNeural networks\nNaive Bayes\nLogistic regression\nRandomized Forests\nEtc.\n\n4.2 Generalization\n\nHow well does a learned model generalize from the data it was trained on to a new test set?\n\n4.2.1 Bias-Variance Trade-off\nModels with too few parameters are inaccurate because of a large bias (not enough flexibility).\n\n\n\nModels with too many parameters are inaccurate because of a large variance(too much sensitivity to the sample).\n\n\n4.2.2 Bias versus variance\nComponents of generalization error\nBias: how much the average model over all training sets differ from the true model?\nError due to inaccurate assumptions/simplifications made by the model \n\n\nVariance: how much models estimated from different training sets differ fron each other\n\nUnderfitting: \n\nmodel is too “simple”to represent all the relevant classcharacteristics\nHigh bias and low variance\nHigh training error and high test error\n\n\n\nOverfitting: \n\nmodel is too “complex” and fits irrelevant characteristics(noise) in the data\n\nLow bias and high variance\n\nLow training error and high test error\n\n\n\nNo classifier is inherently(天生的) better than any other: you need to make assumptions to generalize\nErrors\nBias: due to over-simplifications\nVariance: due to inability to perfectlyestimate parameters from limited data\n\n\n\n4.2.3 How to reduce variance?\nChoose a simpler classifier\nRegularize the parameters\nGet more training data\n\n4.3 Remarks\nKnow your data:\n\nHow much supervision do you have?\nHow many training examples can you afford?\nHow noisy?\n\n\nKnow your goal (i.e. task):\n\nAffects your choices of representation\nAffects your choices of learning algorithms\nAffects your choices of evaluation metricss\n\n\nUnderstand the math behind each machine learning algorithm under consideration!\n\n\n5. Object detection5.1 From image classification to object detection\n\n5.2 Window-based detection models\nBuilding an object model\nGiven the representation, train a binary classifier\n\n\n\n使用不同窗口进行遍历整个图像\n\n5.3 Window-based object detection: recap\n大致思想是生成很多window，然后用学习到的分类器遍历每个window的图像，看分类正确得分\n\nTraining:\n\nObtain training data\n\nDefine features\n\nDefine classifier\n\nGiven new image:\n\nSlide window\n\nScore by classifier\n\n\n\n5.4 Challenges\nlmages may contain more than one class, multiple instances from the same class\nBounding box localization\n位置精度影响分类\n\n\nEvaluation\n评价标准不一样\n\n\n\n\n5.5 Object detection evaluation\nAt test time, predict bounding boxes, class labels, and confidence scores\n\nFor each detection, determine whether it is a true or false positive\n\nPASCAL criterion: Area(GT ∩ Det)/ Area(GT U Det)&gt;0.5\n就是交并比$IoU$\n\n\nFor multiple detections of the same ground truth box, only one considered a true positive\n\n\nFor each class, plot Recall-Precision curve and compute Average Precision (area under the curve)\n\n\n\n\nPrecision:指的是无误检\n保证检测出来的是正确的\n\n\n\n\n\\text { Precision }=\\frac{T P}{T P+F P}=\\frac{1}{1+\\frac{FP}{TP}}\nRecall:表示无漏检\n\n保证不漏检测正确的，例如不希望任何有缺陷的样品漏掉\n\n下式可以理解为在所有正确的样本中，你预测为正确的样本占的比重，所以最大化召回率会使得你尽可能预测到所有的正例样本\n\n\n\n\n\n\\text { Recall }=\\frac{T P}{T P+F N}\n两者需要trade-off\n\n\n\nAUC的物理意义就是权衡这两者的度量\n\n6. Face detection\n\nSlide a window across the image and evaluate a detection model at each location\nThousands of windows to evaluate: efficiency and low false positive rates are essential\nFaces are rare: 0-10 per image\n一张图像不可能出现很多张人脸\n\n\n\n6.1 Viola-Jones face detector\n6.2 Boosting intuition\n\n对于复杂的特征，可能需要复杂的曲线去分类\n有没有什么办法，简化模型复杂度\n用多条直线拟合曲线\n\n\n\n\n\n给分错的点，给一个较大的权重\n就能在下一次分类分对\n\n\n\n\n\n经过数次分类\n就可以得到多个弱分类器\n\n\n\n\n6.3 Boosting: training\nlnitially, weight each training example equally\nln each boosting round:\nFind the weak learner that achieves the lowest weighted training error\nRaise weights of training examples misclassified by current weak learner\n\n\nCompute final classifier as linear combination of all weaklearners (weight of each learner is directly proportional toits accuracy)\n\nExact formulas for re-weighting and combining weak learners depend on the particular boosting scheme.\n\n\n6.4 Viola-Jones face detectorMain idea:\n\nRepresent local texture with( efficienily computable “rectangular” features within window of interest. \nSelect discriminative features to be weak classifiers\n\nUse boosted combination of them as final classifier\n\nForm a cascade（串联） of such classifiers, rejecting clear negatives quickly\n\n6.5 Viola-Jones detector: features6.5.1 “Rectangular” filters\nFeature output is difference between adjacent regions\n左边所有像素点的值减去右边所有像素点的值\n\n\n\n\n\n计算量很大，因为既要考虑框的尺度以及位置(遍历所有像素点)\n这些特征都很简单，就是分别将白色和黑色区域中的所有像素相加，然后做差。例如图1中的A特征，首先计算两个区域像素和$Sum(white),Sum(black).$\n\n然后计算:\n\n\n\nfeature=Sum(white)-Sum(black)\n但是考虑到多尺度问题，即利用不同大小的扫描窗口去检测不同大小的人脸，这个特征feature应该需要归一化。即最终特征：\n\n\nfeature'=\\frac{\\text{feature}}{\\text{pixel\\_num}}\n$pixel_num$​是黑色/白色区域的像素点个数。这样一来，即使扫描窗口的大小不一样，得到的人脸对应位置的特征值也能基本一致。另外，说一下为啥这个叫haar-like。因为在haar-wavelet中，haar基函数是下面这样一个东西。\n\n\n\\psi(x) \\equiv \\begin{cases}1 & 0 \\leq x","categories":["CV"]},{"title":"Gaussian mixture model","url":"/2021/08/15/cv/GMM%E5%8F%82%E8%80%83%E9%98%85%E8%AF%BB/","content":"Gaussian mixture model\n\n详解EM算法与混合高斯模型(Gaussian mixture model,GMM)1 单高斯模型(Gaussian single model, GSM）\n简单回顾一下概率论讲过的高斯模型。\n高斯模型是一种常用的变量分布模型，在数理统计领域有着广泛的应用（……好吧读了这么多年书没白费，教科书般的话语已植入骨髓）。一维高斯分布的概率密度函数如下\n\n\nf(x)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}\\right)\n$\\mu$和 $\\sigma^2$ 分别是高斯分布的均值和方差。\n譬如将男生身高视为变量X, 假设男生的身高服从高斯分布，则$X\\sim N(\\mu,\\sigma^2)$​​,女生亦如此。只是男女生身高分布可能具有不同的均值和方差。图1是从谷歌图库中搜索到的男女生身高分布图，来源不清，个人觉得男生的均值身高虚高……四个记号分别表示$3\\sigma$​准则。\n\n\n\n多维变量 $X=\\left(x_{1}, x_{2}, \\ldots x_{n}\\right)$ 的联合概率密度函数为;\n\n\nf(X)=\\frac{1}{(2 \\pi)^{d / 2}|\\Sigma|^{1 / 2}} \\exp \\left[-\\frac{1}{2}(X-u)^{T} \\Sigma^{-1}(X-u)\\right], X=\\left(x_{1}, x_{2} \\ldots x_{n}\\right)\n其中：d: 变量维度。对于二维高斯分布，有 $\\mathrm{d}=2$​;$u=\\left(\\begin{array}{c}u_{1} \\ u_{2} \\ \\ldots \\ u_{n}\\end{array}\\right):$​ 各维变量的均值；\n$\\Sigma$​ : 协方差矩阵，描述各维变量之间的相关度。对于二维高斯分布，有:\n\n\n\\Sigma=\\left[\\begin{array}{ll}\n\\delta_{11} & \\delta_{12} \\\\\n\\delta_{21} & \\delta_{22}\n\\end{array}\\right]\n\n图 2 是二维高斯分布产生的数据示例，参数设定为: $u=\\left(\\begin{array}{c}0 \\ 0\\end{array}\\right), \\Sigma=\\left[\\begin{array}{cc}1 &amp; 0.8 \\ 0.8 &amp; 5\\end{array}\\right]$. 关于二 维高斯分布的参数设定对为高斯曲面的影响，这篇文章二维高斯分布（Two-dimensional Gaussian distribution) 的参数分析有提及，主要是为了下文理解混合高斯分布做铺垫。服从二维高斯分布的数 据主要集中在一个䧎圆内部, 服从三维的数据集中在一个椭球内部。\n\n2 混合高斯模型（Gaussian mixture model, GMM）2.1 为什么要有混合高斯模型\n先来看一组数据。\n\n\n\n如果我们假设这组数据是由某个高斯分布产生的，利用极大似然估计（后文还会提及）对这个高斯分布做参数估计，得到一个最佳的高斯分布模型如下。\n\n\n\n有什么问题吗？一般来讲越靠近椭圆的中心样本出现的概率越大，这是由概率密度函数决定的，但是这个高斯分布的椭圆中心的样本量却极少。显然样本服从单高斯分布的假设并不合理。单高斯模型无法产生这样的样本。\n实际上，这是用两个不同的高斯分布模型产生的数据。\n\n\n\n正当单高斯模型抓耳挠腮的时候，混合高斯模型就大摇大摆地进场了。它通过求解两个高斯模型，并通过一定的权重将两个高斯模型融合成一个模型，即最终的混合高斯模型。这个混合高斯模型可以产生这样的样本。\n更一般化的描述为：假设混合高斯模型由K个高斯模型组成（即数据包含K个类），则GMM的概率密度函数如下\n\n\np(x)=\\sum_{k=1}^{K} p(k) p(x \\mid k)=\\sum_{k=1}^{K} \\pi_{k} N\\left(x \\mid u_{k}, \\Sigma_{k}\\right)\n其中， $p(x \\mid k)=N\\left(x \\mid u_{k}, \\Sigma_{k}\\right)$ 是第 $\\mathrm{k}$ 个高斯模型的概率密度函数，可以看成选定第 $\\mathrm{k}$ 个模型后， 该模型产生x的概率； $p(k)=\\pi_{k}$ 是第 $\\mathrm{k}$ 个高斯模型的权重，称作选择第 $\\mathrm{k}$ 个模型的先验概率，且满足 $\\sum_{k=1}^{K} \\pi_{k}=1$\n\n所以，混合高斯模型并不是什么新奇的东西，它的本质就是融合几个单高斯模型，来使得模型更加复杂，从而产生更复杂的样本。理论上，如果某个混合高斯模型融合的高斯模型个数足够多，它们之间的权重设定得足够合理，这个混合模型可以拟合任意分布的样本。\n\n\n2.2 直观上理解混合高斯模型\n下面通过几张图片来帮助理解混合高斯模型。\n首先从简单的一维混合高斯模型说起。\n\n\n\n在图6中，$y_1,y_2$​和$y_3$​分别表示三个一维高斯模型，他们的参数设定如图所示。y4表示将三个模型的概率密度函数直接相加，注意的是这并不是一个混合高斯模型，因为不满足$\\sum^K_{k=1}\\pi_k = 1$​​的条件。而$y_5$​和$y_6$​分别是由三个相同的高斯模型融合生成的不同混合模型。由此可见，调整权重将极大影响混合模型的概率密度函数曲线。另一方面也可以直观地理解混合高斯模型可以更好地拟合样本的原因：它有更复杂更多变的概率密度函数曲线。理论上，混合高斯模型的概率密度函数曲线可以是任意形状的非线性函数。\n\n\n下面再给出一个二维空间3个高斯模型混合的例子。\n\n\n\n\n\n(a) 图表示的是3个高斯模型的截面轮廓图，3个模型的权重系数已在图中注明，由截面轮廓图可知3个模型之间存在很多重叠区域。其实这也正是混合高斯模型所希望的。因为如果它们之间的重叠区域较少，那么生成的混合高斯模型一般较为简单，难以生成较为复杂的样本。\n设定好了3个高斯模型和它们之间的权重系数之后，就可以确定二维混合高斯分布的概率密度函数曲面，如图©所示。图(b)是对于图©概率密度曲面的截面轮廓线。从图7也可以看出，整个混合高斯分布曲面相对比于单高斯分布曲面已经异常复杂。实际上，通过调整混合高斯分布的系数$(\\pi ,\\mu ,\\Sigma )$，可以使得图©的概率密度曲面去拟合任意的三维曲面，从而采样生成所需要的数据样本。\n\n3 极大似然估计(Maximum Likehood Estimate, MLE)（最大化对数似然函数）\n最大化对数似然函数（log-likelihood function）的意义。首先直观化地解释一下最大化对数似然函数要解决的是什么问题。假设我们采样得到一组样本$y_t$​​ ,而且我们知道变量Y服从高斯分布（本文只提及高斯分布，其他变量分布模型类似），数学形式表示为$Y \\sim N(\\mu ,\\Sigma )$​采样的样本如图8所示，我们的目的就是找到一个合适的高斯分布（也就是确定高斯分布的参数$(\\mu ,\\Sigma)$,使得这个高斯分布能产生这组样本的可能性尽可能大。\n\n\n\n那怎么找到这个合适的高斯分布呢（在图8中的表示就是1~4哪个分布较为合适）？这时候似然函数就闪亮登场了。\n\n似然函数数学化：设有样本集 $Y=y_{1}, y_{2} \\ldots y_{N}$$ p\\left(y_{n} \\mid \\mu, \\Sigma\\right)$ 是高斯分布的概率分布函数，表示 变量 $Y=y_{n}$ 的概率。假设样本的抽样是独立的，那么我们同时抽到这 $\\mathrm{N}$ 个样本的概率是抽到每个样 本概率的乘积，也就是样本集Y的联合概率。此联合概率即为似然函数：\n\nL(\\mu, \\Sigma)=L\\left(y_{1}, y_{2} \\ldots y_{N} ; \\mu, \\Sigma\\right)=\\prod_{n=1}^{N} p\\left(y_{n} ; \\mu, \\Sigma\\right)\n对式子(4)进行求导并令导数为0（即最大化似然函数，一般还会先转化为对数似然函数再最大化），所求出的参数就是最佳的高斯分布对应的参数。\n\n所以最大化似然函数的意义就是：通过使得样本集的联合概率最大来对参数进行估计，从而选择最佳的分布模型。\n对于图8产生的样本用最大化似然函数的方法，最终可以得到序号1对应的高斯分布模型是最佳的模型\n\n4. EM算法（最大化Q函数）4.1 为什么要有EM算法（EM算法与极大似然估计分别适用于什么问题）\n尝试用极大似然估计的方法来解GMM模型\n解GMM模型，实际上就是确定GMM模型的参数 $(\\mu ,\\Sigma ,\\pi )$，使得由这组参数确定的GMM模型最有可能产生采样的样本。\n先试试看用极大似然估计的方法来解GMM模型会出现什么样的问题。\n如第3小节所述，要利用极大似然估计求解模型最重要的一步就是求出似然函数，即样本集出现的联合概率。而对于混合高斯模型，如何求解某个样本$y_t$的概率？显然我们得先知道这个样本来源于哪一类高斯模型，然后求这个高斯模型生成这个样本的概率$p(y_t)$​\n但是问题来了：我们只有样本。不知道样本到底来源于哪一类的高斯模型。那么如何求解样本的生成概率$p(y_t)$?\n先引入一个隐变量$\\gamma$​​。它是一个K维二值随机变量，在它的K维取值中只有某个特定的元素$\\gamma _k$的取值为1，其它元素的取值为0。实际上，隐变量描述的就是：每一次采样，选择第k个高斯模型的概率，故有：\n\n\np\\left(\\gamma_{k}=1\\right)=\\pi_{k}\n当给定了$\\gamma$的一个特定的值之后（也就是知道了这个样本从哪一个高斯模型进行采样），可以得到样本y的条件分布是一个高斯分布，满足：\n\n\np\\left(y \\mid \\gamma_{k}=1\\right)=N\\left(y \\mid \\mu_{k}, \\Sigma_{k}\\right)\n而实际上，每个样本到底是从这K个高斯模型中哪个模型进行采样的，是都有可能的。故样本y的概率为\n\n\np(y)=\\sum_{\\gamma} p(\\gamma) p(y \\mid \\gamma)=\\sum_{\\mathrm{k}=1}^{K} \\pi_{k} N\\left(y \\mid \\mu_{k}, \\Sigma_{k}\\right)\n样本集Y(n个样本点)的联合概率为：\n\n\n\\begin{array}{l}\nL(\\mu, \\Sigma, \\pi)\\\\\n=L\\left(y_{1}, y_{2} \\ldots y_{N} ; \\mu, \\Sigma, \\pi\\right)\\\\\n=\\prod_{n=1}^{N} p\\left(y_{n} ; \\mu, \\Sigma, \\pi\\right)\\\\\n=\\prod_{n=1}^{N} \\sum_{\\mathrm{k}=1}^{K} \\pi_{k} N\\left(y_{n} \\mid\\mu_{k}, \\Sigma_{k}\\right)\n\\end{array}\n对数似然函数表示为：\n\n\n\\ln L(\\mu, \\Sigma, \\pi)=\\sum_{n=1}^{N} \\ln \\sum_{\\mathrm{k}=1}^{K} \\pi_{k} N\\left(y_{n} \\mid \\mu_{k}, \\Sigma_{k}\\right)\n好了，然后求导，令导数为0，得到模型参数$(\\mu ,\\Sigma ,\\pi)$。貌似问题已经解决了，喜大普奔。\n然而仔细观察可以发现，对数似然函数里面，对数里面还有求和。实际上没有办法通过求导的方法来求这个对数似然函数的最大值。\nMLE（极大似然估计）略显沮丧。这时候EM算法走过来，安慰着说：兄弟别哭，老哥帮你。\n\n5. 极大似然估计与EM算法适用问题分析\n下面先阐述一下极大似然估计与EM算法分别适用于解决什么样的问题。\n\n\n\n\n如果我们已经清楚了某个变量服从的高斯分布，而且通过采样得到了这个变量的样本数据，想求高斯分布的参数，这时候极大似然估计可以胜任这个任务；而如果我们要求解的是一个混合模型，只知道混合模型中各个类的分布模型（譬如都是高斯分布）和对应的采样数据，而不知道这些采样数据分别来源于哪一类（隐变量），那这时候就可以借鉴EM算法。EM算法可以用于解决数据缺失的参数估计问题（隐变量的存在实际上就是数据缺失问题，缺失了各个样本来源于哪一类的记录）。\n下面将介绍EM算法的两个步骤：E-step(expectation-step，期望步)和M-step (Maximization-step,最大化步);\n\n4.2 E-step\n我们现有样本集$Y=(y_1,y_2,…y_T)$，通过隐变量$\\gamma_{t,k}$（表示$y_t$这个样本来源于第k个模型）的引入，可以将数据展开成完全数据：\n\n\n\\left(y_{t}, \\gamma_{t, 1}, \\gamma_{t, 2} \\ldots \\gamma_{t, K}\\right), t=1,2 \\ldots T\n所谓的完全数据，就是不缺失的数据。只有样本集 $Y=\\left(y_{1}, y_{2} \\ldots y_{T}\\right)$ 的数据是不完整的，存在信息 缺失的。若 $y_{t}$ 由第1类采样而来，则有 $\\gamma_{t, 1}=1, \\gamma_{t, 2}=0 \\ldots \\gamma_{t, K}=0$ ，表示为 $\\left(y_{t}, 1,0, \\ldots 0\\right)$ 。\n所以要求能采到这组数据的可能性，需要分两步走：(1)第 $\\mathrm{t}$ 个样本由哪一类产生? (2)如果第 $\\mathrm{t}$ 个样 本由第k类产生，那么第k类产生第t个样本的概率为多少?\n综合考虑上面两步，有了完全数据的似然函数：\n\n\n\\begin{aligned}\np(y, y \\mid \\mu, \\Sigma, \\pi)=& \\prod_{t=1}^{T} p\\left(y_{t}, \\gamma_{t, 1}, \\gamma_{t, 2} \\ldots \\gamma_{t, K} \\mid \\mu, \\Sigma, \\pi\\right) \\\\\n&=\\prod_{t=1}^{T} \\prod_{k=1}^{K}\\left(\\pi_{k} N\\left(y_{t} ; \\mu_{k}, \\Sigma_{k}\\right)\\right)^{\\gamma_{t, k}} \\\\\n&=\\prod_{k=1}^{K} \\pi_{k}^{\\sum_{t=1}^{T} \\gamma_{t, k}} \\prod_{t=1}^{T}\\left(N\\left(y_{t} ; \\mu_{k}, \\Sigma_{k}\\right)\\right)^{\\gamma_{t, k}}\n\\end{aligned}\n第 1 个等号到第 2 个等号的理解：若 $y_{t}$ 由第 1 类采样而来，则有 $\\gamma_{t, 1}=1, \\gamma_{t, 2}=0 \\ldots \\gamma_{t, K}=0$,\n\n\n\\begin{array}{l}\np(y_t,\\gamma_{t,1},\\gamma_{t,2}...,\\gamma_{t,K}\\mid \\mu,\\Sigma,\\pi)\\\\\n=\\prod_{i=1}^{K} \\left(\\pi_{k} N\\left(y_{t} ; \\mu_{k}, \\Sigma_{k}\\right)\\right)^{\\gamma_{t, k}} \\\\\n=\\left(\\pi_{1} N\\left(y_{t} ; \\mu_{1}, \\Sigma_{1}\\right)\\right)^{\\gamma_{t, 1}}\\left(\\pi_{2} N\\left(y_{t} ; \\mu_{2}, \\Sigma_{2}\\right)\\right)^{\\gamma_{t, 2}} \\left(\\pi_{K}\\left(\\left(y_{t} ; \\mu_{K}, \\Sigma_{K}\\right)\\right)^{\\gamma_{t, K}}\\right. \\\\\n=\\left(\\pi_{1} N\\left(y_{t} ; \\mu_{1}, \\Sigma_{1}\\right)\\right)^{1}\\left(\\pi_{2} N\\left(y_{t} ; \\mu_{2}, \\Sigma_{2}\\right)\\right)^{0} \\ldots\\left(\\pi_{K} N\\left(y_{t} ; \\mu_{K}, \\Sigma_{K}\\right)\\right)^{0} \\\\\n=\\pi_{1} N\\left(y_{t} ; \\mu_{1}, \\Sigma_{1}\\right)\n\\end{array}\n\\begin{array}{l}\n\\ln p(y, \\gamma \\mid \\mu, \\Sigma, \\pi)\\\\\n=\\sum_{k=1}^{K}\\left(\\sum_{t=1}^{T} \\gamma_{t, k}\\right) \\ln \\pi_{k}\\\\\n+\\sum_{t=1}^{T} \\gamma_{t, k}\\left(-\\ln (2 \\pi)-\\frac{1}{2} \\ln \\left|\\Sigma_{k}\\right|\\right.\\\\\n\\left.-\\frac{1}{2}\\left(y_t-\\mu_{t}\\right)^{T}\\left(\\Sigma_{k}\\right)^{-1}\\left(y_{t}-\\mu_{t}\\right)\\right)\n\\end{array}\n这一步应该没啥问题吧。。。注意的是，此处考虑的是二维高斯分布的情况，对应于式子(2)中的d=2。\n\n我们的目标就是找出一组参数 $(\\mu , \\Sigma , \\pi *)$ 使得 $\\ln p(y, \\gamma \\mid \\mu, \\Sigma, \\pi)$ 最大。那么问题来了: $\\ln p(y, \\gamma \\mid \\mu, \\Sigma, \\pi)$ 中含有隐变量 $Y$ ， $Y$ 的存在使得我们没法最大化 $\\ln p(y, \\gamma \\mid \\mu, \\Sigma, \\pi)$ 。如果我们知道了 $Y$, 那么最大化 $\\ln p(y, \\gamma \\mid \\mu, \\Sigma, \\pi)$ 就显得水到渠 成。\n\n但是坑忩的就是: 我们只有采样数据 $y_{t}, \\quad$ 末知。那么怎么办呢? 对 $/$ 来一个估计。\n猜想我们给了一组起始参数 $\\left(\\mu^{0}, \\Sigma^{0}, \\pi^{0}\\right)$ 或者优化过的第次迭代的参数 $\\left(\\mu^{i}, \\Sigma^{i}, \\pi^{i}\\right)$， 也就是说每一个高斯分布的参数我们都有了， $Y$ 做的事不就是决定每个样本由哪一个高斯分布产生的嘛，有了每个高斯分布的参数那我们就可以猜想每个样本最有可能来源于哪个高斯分布没错吧! Done!\n为此我们不最大化\\operatornamen $p(y, y \\mid \\mu, \\Sigma, \\pi)$ (也无法最大化它)，而是最大化Q函数。Q函 数如下:\n\n\n\\begin{aligned}\n&Q\\left(\\mu, \\Sigma, \\pi, \\mu^{i}, \\Sigma^{i}, \\pi^{i}\\right)=E_{\\gamma}\\left[\\ln p(y, \\gamma \\mid \\mu, \\Sigma, \\pi) \\mid Y, \\mu^{i}, \\Sigma^{i}, \\pi^{i}\\right] \\\\\n&=E_{\\gamma}\\left[\\sum_{k=1}^{K}\\left(\\sum_{t=1}^{T} \\gamma_{t, k} \\mid y_{t}, \\mu^{i}, \\Sigma^{i}, \\pi^{i}\\right) \\ln \\pi_{k}\\right.\\\\\n&+\\left.\\sum_{t=1}^{T}\\left(y_{t, k} \\mid y_{t}, \\mu^{i}, \\Sigma^{i}, \\pi^{i}\\right)(-\\ln (2 \\pi)-\\right.\\left.\\left.\\frac{1}{2} \\ln \\left|\\Sigma_{k}\\right|-\\frac{1}{2}\\left(y_{t}-\\mu_{t}\\right)^{T}\\left(\\Sigma_{k}\\right)^{-1}\\left(y_{t}-\\mu_{t}\\right)\\right)\\right] \\\\\n&=\\sum_{k=1}^{K}\\left(\\sum_{t=1}^{T} E\\left(\\gamma_{t, k} \\mid y_{t}, \\mu^{i}, \\Sigma^{i}, \\pi^{i}\\right) \\ln \\pi_{k}\\right.\\\\\n&+\\left.\\sum_{t=1}^{T} E\\left(y_{t, k} \\mid y_{t}, \\mu^{i}, \\Sigma^{i}, \\pi^{i}\\right)(-\\ln (2 \\pi)-\\right.\\left.\\left.\\frac{1}{2} \\ln \\left|\\Sigma_{k}\\right|-\\frac{1}{2}\\left(y_{t}-\\mu_{t}\\right)^{T}\\left(\\Sigma_{k}\\right)^{-1}\\left(y_{t}-\\mu_{t}\\right)\\right)\\right)\n\\end{aligned}\n其中, $ E\\left(\\gamma_{t, k} \\mid y_{t}, \\mu^{i}, \\Sigma^{i}, \\pi^{i}\\right) $​就是对 $\\gamma$的估计: \n\n\n\\begin{aligned}\n&E\\left(\\gamma_{t, k} \\mid y_{t}, \\mu^{i}, \\Sigma^{i}, \\pi^{i}\\right)=p\\left(\\gamma_{t, k}=1 \\mid y_{t}, \\mu^{i}, \\Sigma^{i}, \\pi^{i}\\right)\\\\\n&=\\frac{p\\left(\\gamma_{t, k}=1, y_{t} \\mid \\mu^{i}, \\Sigma^{i}, \\pi^{\\prime}\\right)}{p\\left(y_{t}\\right)}\\\\\n&=\\frac{p\\left(y_{t} \\mid y_{t, k}=1, \\mu^{2}, \\Sigma^{2}, \\pi^{2}\\right) p\\left(\\gamma_{t, k}=1 \\mid \\mu^{2}, \\Sigma^{2}, \\pi^{i}\\right)}{\\sum_{k=1}^{K} p\\left(y_{t} \\mid \\gamma_{t, k}=1, \\mu^{i}, \\Sigma^{i}, \\pi^{i}\\right) p\\left(\\gamma_{t, k}=1 \\mid \\mu^{i}, \\Sigma^{i}, \\pi^{i}\\right)}\\\\\n&=\\frac{\\pi_{k}^{i} N\\left(y_{t} ; \\mu_{k}^{i}, \\Sigma_{k}^{i}\\right)}{\\sum_{k=1}^{K} \\pi_{k}^{i} N\\left(y_{t} ; \\mu_{k}^{i}, \\Sigma_{k}^{i}\\right)}\n\\end{aligned}\n这公式是不是很可怕? ? 别急，带上几点声明，再去看公式就很好理解了!\nQ函数描述的其实就是在给定 $\\left(\\mu^{i}, \\Sigma^{i}, \\pi^{i}\\right)$ 参数下，先对样本 $Y$ 做一个最有可能的划 分 (每个样本来源于各个类的可能性，即对 $\\gamma$ 的估计 $E\\left(\\gamma_{t, k} \\mid y_{t}, \\mu^{i}, \\Sigma^{i}, \\pi^{i}\\right)$ )，再描述能 够产生这组样本的可能性 (Q函数) ;\n有了对于 $\\gamma$ 的估计之后， Q函数只和样本有关 (传统意义上的似然函数亦如此，完 全数据的似然函数还与 ${ }^{\\mathbf{1}}$ 有关)，而不再含有隐变量，从而使得最大化Q函数成为可能;\n最大化Q函数的过程实则就是使得能够产生这组样本的可能性最大，与最大化似然 函数的思路如出一辙。\n\n\n\n4.3 M-step\n有个Q函数，就可以对Q函数进行最大化，得到下一次迭代的模型参数了，即：\n\n\n\\dot{\\mu}^{+1}, \\Sigma^{i+1}, \\pi^{i+1}=\\arg \\max Q\\left(\\mu, \\Sigma, \\pi, \\mu^{i}, \\Sigma^{i}, \\pi^{i}\\right)对Q函数进行求导, 并另其导数为 0, 可得;\n\n\\mu_{k}^{i+1}=\\frac{\\sum_{t=1}^{T} \\frac{\\pi_{k}^{i} N\\left(y_{t} ; \\mu_{k}^{i}, \\Sigma_{k}^{i}\\right)}{\\sum_{k=1}^{K} \\pi_{k}^{i} N\\left(y_{t} ; \\mu_{k}^{k}, \\Sigma_{k}^{i}\\right)} y_{t}}{E\\left(\\gamma_{t, k} \\mid y_{t}, \\mu^{i}, \\Sigma^{i}, \\pi^{i}\\right)}, k=1,2 \\ldots K\n\\Sigma_{k}^{i+1}=\\frac{\\sum_{t=1}^{T} \\frac{\\pi_{k}^{i} N\\left(y_{t} ; \\mu_{k}^{i}, \\Sigma_{k}^{i}\\right)}{\\sum_{k=1}^{i} N\\left(y_{t}, \\mu_{k}^{i}, \\Sigma_{k}^{i}\\right)}\\left(y_{t}-\\mu_{k}^{i}\\right)^{2}}{E\\left(\\gamma_{t, k} \\mid y_{t}, \\mu^{i}, \\Sigma^{i}, \\pi^{i}\\right)}, k=1,2 \\ldots K\n\\pi_{k}^{i+1}=\\frac{E\\left(y_{t, k} \\mid y_{t}, \\mu^{i}, \\Sigma^{i}, \\pi^{i}\\right)}{T}, k=1,2 \\ldots K\n其中 $\\mu_{k}^{i+1}, \\Sigma_{k}^{i+1}, \\pi_{k}^{i+1}$ 分别表示第 $(\\mathrm{i}+1)$ 次迭代，第 $\\mathrm{k}$ 个类的均值，协方差矩阵和所占的权重。  \n\n4.4 一个例子梳理EM算法的整个过程\nEM算法的核心思想是: 通过迭代的过程来找到一组最优的参数 $(\\mu , \\Sigma , \\pi *)$, 使得这组参数表 示的模型最有可能产生现有的采样数据。每次迭代的过程就是参数矫正的过程。\n\n\n\n现假设初始化一组参数在这组参数$(\\mu^0,\\Sigma^0,\\pi^0)$下，2类二维高斯分布如图11绿色椭圆所示。然后利用现有的参数，E-step开始对样本数据进行划分（对$\\gamma$进行估计）。蓝色的样本大多都被划分给第1类模型，橘黄色的样本大多都被划分给第2类模型。但是第1类模型还有优化空间：第1类模型还不能使得蓝色样本出现的联合概率达到最大。第2类模型也是如此。M-step便优化了2类模型的参数，得到新的参数$({\\mu ^1},{\\Sigma ^1},{\\pi ^1})$，使得优化后2类高斯分布如图11红色椭圆所示。其中，第1类模型主要优化的是模型均值（即椭圆的中心），第二类模型主要优化的是模型协方差矩阵（即椭圆的长轴、短轴和长短轴的方向）。然后重复进行E-step和M-step，直到参数$(\\mu ,\\Sigma ,\\pi )$收敛。\n\n最后谈谈混合高斯模型的参数$\\pi$。\n\n混合高斯模型的参数$\\mu ,\\Sigma$比较好理解，用于描述各个高斯分布的形状，对于它们的调整也比较直观：使得本高斯分布能够更好地接纳被划分到这类分布的样本。而为什么要有参数π \\piπ？它描述的是各个高斯分布所占的比重，如果不加“歧视”的话（样本来源于各个高斯分布的可能性一致），则有$\\pi_k=\\frac{1}{K}$而如果对于某一类高斯分布（即为i）有侧重的话，则相应的$\\pi_i$较大，体现在图11中就是被分配给各个类的样本数占样本总数的比例。如果一轮优化后，某一类高斯分布又接纳了更多样本，则其$\\pi_i$变大，反之变小（所以图11从绿色椭圆调整为红色椭圆实际上两个类所对应的权重也被优化了）。\n而从本质上来看参数$\\pi$，则是为了混合高斯模型能有更好的曲面拟合能力。当参数$\\pi$退化为某一类高斯分布的权重远远大于其他类高斯分布的时候，混合高斯模型就退化成了单高斯模型！\n\n5 总结\n图12和图13梳理了高斯分布和混合高斯分布参数估计的逻辑流程。\n\n\n\n\n相对比于高斯分布的参数估计，混合高斯分布的参数估计更加复杂。主要原因在于隐变量的存在。而为什么混合高斯分布的参数估计需要多次迭代循环进行？是因为EM算法中对于γ 的估计利用的是初始化或者第i步迭代的参数$({\\mu ^i},{\\Sigma ^i},{\\pi ^i})$​，这对于样本的分类划分是有误差的。所以它只能通过多次迭代优化寻找更佳的参数来抵消这一误差。\n终于把这篇文章梳理完了。世界杯要结束了，伪球迷也想见证一下冠军诞生。至此，本文结束\n\n","categories":["CV"]},{"title":"Recovery of 3D structure","url":"/2021/08/15/cv/9.%203D%20structure/","content":"Recovery of 3D structure\n\nRecovery of 3D structure1. Measure three-dimensional information\nCamera model\n\nCamera calibration(标定)\n\nEpipolar geometry\n\n2. Things aren’t always as they appear…\n\nSingle-view ambiguity\n失去深度信息\n\n\n\n\n\nWhen certain assumptions hold, we can recover structure from a single view\n\n\n\nIn general, we need multi-view geometry\n\n\n3. Review: Pinhole camera model\n\nf = focal length 焦距\no = aperture光圈 = pinhole = center of the camera \n\n\n\n\\begin{gathered}\nQ=\\left[\\begin{array}{l}\nX \\\\\nY \\\\\nZ\n\\end{array}\\right] \\rightarrow Q^{\\prime}=\\left[\\begin{array}{l}\nX^{\\prime} \\\\\nY^{\\prime}\n\\end{array}\\right] \\\\\n\\mathfrak{R}^{3} \\rightarrow \\mathfrak{R}^{2}\n\\end{gathered}\n\\begin{aligned}\n&\\mathrm{X}^{\\prime}=f \\frac{X}{Z} \\\\\n&\\mathrm{Y}^{\\prime}=f \\frac{Y}{Z}\n\\end{aligned}\nQuestion: Is this a linear transformation?\nIt’s not, because it has X and Z in the equations.\n\n\n\n3.1 Homogeneous coordinates 欧式坐标与齐次坐标互转换\n\\begin{aligned}\n&E \\rightarrow H \\\\\n&\\quad(x, y) \\Rightarrow\\left[\\begin{array}{l}\nx \\\\\ny \\\\\n1\n\\end{array}\\right] \\quad(x, y, z) \\Rightarrow\\left[\\begin{array}{l}\nx \\\\\ny \\\\\nz \\\\\n1\n\\end{array}\\right]\n\\end{aligned}\n\\begin{aligned}\n&H \\rightarrow E \\\\\n&\\qquad\\left[\\begin{array}{c}\nx \\\\\ny \\\\\nw\n\\end{array}\\right] \\Rightarrow(x / w, y / w) \\quad\\left[\\begin{array}{c}\nx \\\\\ny \\\\\nz \\\\\nw\n\\end{array}\\right] \\Rightarrow(x / w, y / w, z / w)\n\\end{aligned}3.2 Projective transformation in Homogeneous coordinates\n投影变换\n先将其转换为其次坐标系，然后就可以用线性式子来表示变换关系\n\n\n\\begin{gathered}\nQ=\\left[\\begin{array}{l}\nX \\\\\nY \\\\\nZ\n\\end{array}\\right] \\rightarrow \\quad Q^{\\prime}=\\left[\\begin{array}{l}\nX^{\\prime} \\\\\nY^{\\prime}\n\\end{array}\\right] \\quad \\quad  \\\\\n\\mathrm{X}^{\\prime}=f \\frac{X}{Z}\\quad  \n\\mathrm{Y}^{\\prime}=f \\frac{Y}{Z} \\\\\n\\left(\\begin{array}{l}\nX \\\\\nY \\\\\nZ \\\\\n1\n\\end{array}\\right) \\mapsto\\left(\\begin{array}{c}\nf X \\\\\nf Y \\\\\nZ\n\\end{array}\\right)=\\left[\\begin{array}{lll}\nf & & 0 \\\\\n& f & 0 \\\\\n& & 1 & 0\n\\end{array}\\right]\\left(\\begin{array}{c}\nX \\\\\nY \\\\\nZ \\\\\n1\n\\end{array}\\right)=P Q\n\\end{gathered}3.3 Camera calibration(标定)\n由于摄像机的位置不固定，所以需要设立一个世界坐标系。然后所有变换在该坐标系进行\n\n\n\n\nNormalized (camera) coordinate system:camera center is at the origin原点, the principal axisis 主轴 the z-axis,\nCamera calibration: figuring out transformation from world coordinate system to image coordinate system\n这里我们可以看到已经有两个坐标系，分别为摄像机坐标系、变换后的坐标系。不同的是，一个是摄像机的坐标系，原点位于图片主点；变换后的坐标系，其坐标原点在图片的左下角上（右上角）\n\n3.3.1 From retina plane to images\nretina plane 视平面\n\n\n\nPrincipal point (p):point where principal axis intersects the image plane\n主轴交图像坐标系的点叫主点\n\n\nNormalized coordinate system: origin of the image is at the principal point\n规范后的坐标系原点在主点上\n\n\nImage coordinate system: origin is in the corner\n图像坐标系的原点是左下角\n\n\n\n3.3.2 Principal point offset\n我们进行投影时，会首先投影到规范化坐标系，之后再将该坐标系平移到图像角点\n\n先进行坐标平移，再进行投影变换\n\n\n\n\n\\begin{aligned}\n&(X, Y, Z) \\mapsto\\left(f X / Z+p_{x}， f Y / Z+p_{y}\\right)\\\\\n&\\left(\\begin{array}{l}\nX \\\\\nY \\\\\nZ \\\\\n1\n\\end{array}\\right) \\mapsto\\left(\\begin{array}{c}\nf X+Z p_{x} \\\\\nf Y+Z p_{y} \\\\\nZ\n\\end{array}\\right)=\\left[\\begin{array}{ccc}\nf & & p_{x} & 0 \\\\\n& f & p_{y} & 0 \\\\\n&  & 1 & 0\n\\end{array}\\right]\\left(\\begin{array}{l}\nX \\\\\nY \\\\\nZ \\\\\n1\n\\end{array}\\right)\n\\end{aligned}\n\n$\\mathrm{P}=\\mathrm{K}[\\mathrm{I} \\mid 0]$ 规范化矩阵\n\n3.3.3 Pixel coordinates\nPixel size: $\\frac{1}{m_{x}} \\times \\frac{1}{m_{y}}$\n$m_{x}$​​ pixels per meter in horizontal direction \n$m_{y}$​​ pixels per meter in vertical direction\n观察式子，我们会发现，其只是又做了依次scale，所以只需要左乘一个scale transformation matrix\n\n\n(X, Y, Z) \\mapsto m_{x} f X / Z+p_{x}, m_{y} f Y / Z+p_{y}\nK=\\underset{pixels/m}{\\left[\\begin{array}{ccc}\nm_{x} & & \\\\\n& m_{y} & \\\\\n& & 1\n\\end{array}\\right]}\\underset{m}{\\left[\\begin{array}{ccc}\nf & & p_{x} \\\\\n& f & p_{y} \\\\\n& & 1\n\\end{array}\\right]}=\\underset{pixels}{\\left[\\begin{array}{ccc}\n\\alpha_{x} & & \\beta_{x} \\\\\n& \\alpha_{y} & \\beta_{y} \\\\\n& & 1\n\\end{array}\\right]}\n有五个自由度\n\n3.3.4 Camera rotation and translation3.3.4.1 3D Translation\nx^{\\prime}=x+t x ; y^{\\prime}=y+t y ; z^{\\prime}=z+t z\n\\left[\\begin{array}{l}\nX^{\\prime} \\\\\nY^{\\prime} \\\\\nZ^{\\prime} \\\\\n1\n\\end{array}\\right]=\\left[\\begin{array}{llll}\n1 & 0 & 0 & t x \\\\\n0 & 1 & 0 & t y \\\\\n0 & 0 & 1 & t z \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{c}\nX \\\\\nY \\\\\nZ \\\\\n1\n\\end{array}\\right]3.3.4.2 3D Scaling\nX^{\\prime}=X \\times S_x ; Y^{\\prime}=Y \\times S_y ; Z^{\\prime}=Z \\times S_z\n\\left[\\begin{array}{c}\nX^{\\prime} \\\\\nY^{\\prime} \\\\\nZ^{\\prime} \\\\\n1\n\\end{array}\\right]=\\left[\\begin{array}{cccc}\nS_x & 0 & 0 & 0 \\\\\n0 & S_y & 0 & 0 \\\\\n0 & 0 & S_z & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\nX \\\\\nY \\\\\nZ \\\\\n1\n\\end{array}\\right]3.3.4.3 3D rotation transformation3D rotation is done around a rotation axis\n\nFundamental rotations – rotate about x, y, or z axes\nCounter-clockwise rotation逆时针旋转 is referred to as positive rotation (when you look down negative axis)\n\n\n\nRotation about Z – similar to 2D rotation\n\n\n\\begin{aligned}\n&x^{\\prime}=x \\cos (\\theta)-y \\sin (\\theta) \\\\\n&y^{\\prime}=x \\sin (\\theta)+y \\cos (\\theta) \\\\\n&z^{\\prime}=z\n\\end{aligned}\n\\left[\\begin{array}{lcll}\n\\cos (\\theta) & -\\sin (\\theta) & 0 & 0 \\\\\n\\sin (\\theta) & \\cos (\\theta) & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\nRotation about y (z -&gt; y, y -&gt; x, x-&gt;z)\n\n\n\\begin{aligned}\n&z^{\\prime}=z \\cos (\\theta)-x \\sin (\\theta) \\\\\n&x^{\\prime}=z \\sin (\\theta)+x \\cos (\\theta) \\\\\n&y^{\\prime}=y\n\\end{aligned}\n\\left[\\begin{array}{lccc}\n\\cos (\\theta) & 0 & \\sin (\\theta) & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n-\\sin (\\theta) & 0 & \\cos (\\theta) & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\nRotation about x (z -&gt; x, y -&gt; z, x-&gt;y)\n\n\n\\begin{aligned}\n&y^{\\prime}=y \\cos (\\theta)-z \\sin (\\theta) \\\\\n&z^{\\prime}=y \\sin (\\theta)+z \\cos (\\theta) \\\\\n&x^{\\prime}=x \\\\\n&\\left[\\begin{array}{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & \\cos (\\theta) & -\\sin (\\theta) & 0 \\\\\n0 & \\sin (\\theta) & \\cos (\\theta) & 0 \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\n\\end{aligned}3.3.5 Composing Transformation\n3.3.6 Camera rotation and translation\nYou can think of object transformations as moving (transforming) its local coordinate frame\n世界坐标系到camera坐标系\n\n\nAll the transformations are performed relative to the current coordinate frame origin and axes\n\n\n\nIn general, the camera coordinate frame will be related to the world coordinate frame by a rotation and a translation\nConversion from world to camera coordinate system (in non-homogeneous coordinates):\n\n\n\n\n\\tilde{X}_{\\text {cam }}=R(\\tilde{X}-\\tilde{C}) \\quad\\left(\\begin{array}{c}\n\\tilde{X}_{\\text {cam }} \\\\\n1\n\\end{array}\\right)=\\left[\\begin{array}{cc}\nR & -R \\widetilde{C} \\\\\n0 & 1\n\\end{array}\\right]\\left(\\begin{array}{c}\n\\tilde{X} \\\\\n1\n\\end{array}\\right)\n其实可以这么理解这个变换矩阵，对于偏置量会受旋转的影响：\n\n\n\\left[\\begin{array}{ll}\nR_{3\\times3} & O_{3\\times 1}\\\\\nO_{1\\times 3} & 1_{1\\times 1}\n\\end{array}\\right]_{4\\times 4}\n\\left[\\begin{array}{ll}\nI_{3\\times3} & -\\widetilde{C}_{3\\times 1}\\\\\nO_{1\\times 3} & 1_{1\\times 1}\n\\end{array}\\right]_{4\\times 4}=\n\\left[\\begin{array}{cc}\nR & -R \\widetilde{C} \\\\\n0 & 1\n\\end{array}\\right]_{4\\times 4}\n\\tilde{X}_{\\text {cam }}=R(\\widetilde{X}-\\widetilde{C}) \\quad X_{\\text {cam }}=\\underset{\\text{3D transformation \nmatrix (4 x 4)}}{\n\\left[\\begin{array}{cc}\nR & -R \\widetilde{C} \\\\ \n0 & 1\\end{array}\\right]} \nX\n2D transformation matrix (3 x 3) 从摄像机坐标系平移到另一个坐标系，并做了投影变换和尺度变换\n\n\nK=\\underset{pixels/m}{\\left[\\begin{array}{ccc}\nm_{x} & & \\\\\n& m_{y} & \\\\\n& & 1\n\\end{array}\\right]}\\underset{m}{\\left[\\begin{array}{ccc}\nf & & p_{x} \\\\\n& f & p_{y} \\\\\n& & 1\n\\end{array}\\right]}=\\underset{pixels}{\\left[\\begin{array}{ccc}\n\\alpha_{x} & & \\beta_{x} \\\\\n& \\alpha_{y} & \\beta_{y} \\\\\n& & 1\n\\end{array}\\right]}\n用于视角变换，从二维变换转为三维变换，相当于矩阵维度变换\n\n\n[\\mathrm{I} \\mid 0]\n我们可以这么理解一下式子，其先将世界坐标系转到了摄像机坐标系，再进行平移变换，最后进行投影变换和尺度变换\n\n\n\n\n$K$摄像机内部参数，$[R\\mid t]$外部参数\n\n\n3.4 Camera parameters\nIntrinsic parameters\nPrincipal point coordinates\n$p_x,p_y$\n\n\nFocal length\n$f$\n\n\nPixel magnification factors\n$m_x,m_y$\n\n\nSkew (non-rectangular pixels)\nRadial distortion 畸变\n越远离光圈越容易发生弯曲\n\n\n\n\n\n\n\\begin{gathered}\n\\mathbf{P}=\\mathbf{K}\\left[\\begin{array}{ll}\n\\mathbf{R}\\mid \\mathbf{t}\n\\end{array}\\right] \\\\\n\\mathrm{K}=\\left[\\begin{array}{cc}\nm_{x} & \\\\\n& m_{y} & \\\\\n& & 1\n\\end{array}\\right]\\left[\\begin{array}{cc}\nf  & &p_{x} \\\\\n&f & p_{y} \\\\\n& & 1\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\alpha_{x} & & \\beta_{x} \\\\\n&\\alpha_{y} & \\beta_{y} \\\\\n& & 1\n\\end{array}\\right]\n\\end{gathered}\n\nExtrinsic parameters\nRotation and translation relative to world coordinate system\nWhat is the projection of the camera center?\n\n\n\n\n\\mathbf{P}=\\mathbf{K}\\left[\\begin{array}{ll}\n\\mathbf{R} \\mid -\\mathbf{R} \\widetilde{\\mathbf{C}}\n\\end{array}\\right]\n$C$是摄像机中心在世界坐标系的坐标\n\n\n\\mathbf{P} \\mathbf{C}=\\mathbf{K}[\\mathbf{R}\\mid-\\mathbf{R} \\tilde{\\mathbf{C}}]\\left[\\begin{array}{c}\n\\widetilde{\\mathbf{C}} \\\\\n1\n\\end{array}\\right]=0\nThe camera center is the null space of the projection matrix!\n投影矩阵:$\\mathbf{P} \\mathbf{C}=\\mathbf{K}[\\mathbf{R}\\mid-\\mathbf{R} \\tilde{\\mathbf{C}}]$​\ncamera center:$\\left[\\begin{array}{c}\\widetilde{\\mathbf{C}} \\\\1\\end{array}\\right]$\n在数学中，一个算子 $A$ 的零空间是方程 $Av = 0$ 的所有解 $v$ 的集合。它也叫做 $A$ 的核空间。如果算子是在向量空间上的线性算子，零空间就是线性子空间。因此零空间是向量空间。\n\n\n\n4. Camera calibration\n参数不可知\n\n\n\\begin{gathered}\n\\lambda \\mathbf{x}=\\mathbf{K}[\\mathbf{R} \\mid \\mathbf{t}] \\mathbf{X} \\\\\n{\\left[\\begin{array}{c}\n\\lambda x \\\\\n\\lambda y \\\\\n\\lambda\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n* & * & * & * \\\\\n* & * & * & * \\\\\n* & * & * & *\n\\end{array}\\right]\\left[\\begin{array}{l}\nX \\\\\nY \\\\\nZ \\\\\n1\n\\end{array}\\right]}\n\\end{gathered}\nGiven $n$ points with known $3 D$ coordinates $X_{i}$ and known image projections $\\boldsymbol{x}_{i}$​​, estimate the camera parameters\n通过实验，可以同时测得3D坐标和图像投影坐标\n\n\n\n\n\n\n\\begin{array}{r}\n\\chi_{i}=P X_{i} \\quad P=\\left[\\begin{array}{l}\nP_{1} \\\\\nP_{2} \\\\\nP_{3}\n\\end{array}\\right] \\\\\n\\text { Pixel: } \\chi_{i}=\\left[\\begin{array}{l}\nx_{i} \\\\\ny_{i}\n\\end{array}\\right]=\\left[\\begin{array}{l}\n\\frac{P_{1} X_{i}}{P_{3} X_{i}} \\\\\n\\frac{P_{2} X_{i}}{P_{3} X_{i}}\n\\end{array}\\right]\n\\end{array}\n上面的式子是先做了变换，然后将齐次坐标转为了欧式坐标\nTwo linearly independent equations\nP has 11 degrees of freedom\nOne 2D/3D correspondence gives us two linearly  independent equations\n6 correspondences needed for a minimal solution\n\n4.1 Nonlinear method\n\\chi_{i}=P X_{i} \\quad \\chi_{i}=\\left[\\begin{array}{l}\nx_{i} \\\\\ny_{i}\n\\end{array}\\right]=\\left[\\begin{array}{l}\n\\frac{P_{1} X_{i}}{P_{3} X_{i}} \\\\\n\\frac{P_{2} X_{i}}{P_{3} X_{i}}\n\\end{array}\\right] \\quad\\left\\{\\begin{array}{c}\n-x_{i}\\left(P_{3} X_{1}\\right)+P_{1} X_{1}=0 \\\\\n-y_{i}\\left(P_{3} X_{1}\\right)+P_{2} X_{1}=0 \\\\\n-x_{i}\\left(P_{3} X_{n}\\right)+P_{1} X_{n}=0 \\\\\n-y_{i}\\left(P_{3} X_{n}\\right)+P_{2} X_{n}=0\n\\end{array}\\right.\n\\left[\\begin{array}{ccc}\n0^{T} & \\mathbf{X}_{1}^{T} & -y_{1} \\mathbf{X}_{1}^{T} \\\\\n\\mathbf{X}_{1}^{T} & 0^{T} & -x_{1} \\mathbf{X}_{1}^{T} \\\\\n\\cdots & \\cdots & \\cdots \\\\\n0^{T} & \\mathbf{X}_{n}^{T} & -y_{n} \\mathbf{X}_{n}^{T} \\\\\n\\mathbf{X}_{n}^{T} & 0^{T} & -x_{n} \\mathbf{X}_{n}^{T}\n\\end{array}\\right]\\left(\\begin{array}{l}\n\\mathbf{P}_{1}^{\\mathrm{T}} \\\\\n\\mathbf{P}_{2}^{T} \\\\\n\\mathbf{P}_{3}^{T}\n\\end{array}\\right)=0\n\\mathbf{A} \\mathbf{p}=\\mathbf{0}\nHomogeneous least squares:|find $\\mathbf{p}$ minimizing $|\\mathbf{A} \\mathbf{p}|^{2}$\nSolution given by eigenvector of $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}$​ with smallest eigenvalue\n奇异值分解，求$p$的值\n\n\n\n5. Epipolar geometry5.1 Recovering structure from a single view\n从单个图片，即使有知识，但是很难进行重建，因为图片具有歧义，缺少深度信息\n\n\n\nFrom calibration rig: location/pose of the rig, K\nKnowledge about scene: point correspondences, geometry of lines &amp; planes, etc…\n这些知识包括点的依赖性、线的平行特征，平面等\n\n\nIntrinsic ambiguity of the mapping from 3D to image (2D)\n具有内部歧义性，主要在投影的时候，丢失了深度信息\n\n\n\n\n\nTwo eyes help!\n\n\n5.2 A taste of multi-view geometry: Triangulation\n\nGiven projections of a 3D point in two or more images (with known camera matrices), find the coordinates of the point\n给定3D point的投影坐标，要求3D坐标\n\n\n\n\n\nWe want to intersect the two visual rays corresponding to $x_1$​and $x_2$​​​, but because of noise and numerical errors, they don’t meet exactly\n理论上是可以知道x的位置，即使不知道，有两张照片也可以找的到，但是实际上有噪音，所以很难找到交点\n\n\n$\\text { Find } \\mathrm{X} \\text { that minimizes } d^{2}\\left(\\mathbf{x}_{1}, \\mathbf{P}_{1} \\mathbf{X}\\right)+d^{2}\\left(\\mathbf{x}_{2}, \\mathbf{P}_{2} \\mathbf{X}\\right)$\n最小化投影距离与真实距离\n\n\n\n\n5.3 问题分类\n求相机内参\n\n\n\nMotivation: Given a set of known 3D points seen by a camera, compute the camera parameters\n\nCalibration!\n\n\n定位真实空间位置\n\n\n\n\nStructure: Given known cameras and projections of the same 3D point in two or more images, compute the 3D coordinates of that point\nTriangulation!\n给定同一点的一些投影坐标和相机参数等，用三角法求真实坐标\n\n\n要求一张图片的点对应另一张图片的另一个点\n\n\n\nCorrespondence: Given a point in one image, find the corresponding point in another one.\n知道摄像机，也知道图片，要求一张图片的点对应另一张图片的另一个点\n\n\n\n5.4 Epipolar geometry\n\nBaseline（基线） —— line connecting the two camera centers\n两个相机中心的连线\n\n\nEpipolar Plane（极平面）——plane containing baseline and $X$​\n这里有三个坐标系，两个摄像机坐标系，一个世界坐标系，也可以将世界坐标系和其中一个摄像机坐标系移到到重合\n\n\nEpipoles（极点） ——intersections of baseline with image planes\n基线和图片的交点$e$\n\n\nEpipolar Lines —— intersections of epipolar plane with image planes (always come in corresponding pairs)\n极平面和图像平面的交线$l,l’$\n\n\n\n\n\nIf we observe a point $x$ in one image, where can the corresponding point $x’$​​ be in the other image?\n\n\n\nPotential matches for $x$​​ have to lie on the corresponding epipolar line $ l’$​.\nPotential matches for $x$​ ‘ have to lie on the corresponding epipolar line $l$​​​.\n无论是已知哪一个点，要找匹配，都在相关的极线上，所以匹配的时候，只要遍历极线的点就行\n这个问题其实是一个三点共线问题：即要证明$O’$和$X$的连线与图片平面的交点一定在极线上\n\n5.5 Epipolar constraint example\n5.6 Epipolarconstraint: Calibrated case\n现验证匹配的投影点是否在交线上，即已知$x’$​坐标，验证其是否在直线上\n先将所有点的坐标转到世界坐标系里表达\n\n\n\nK_{\\text {camenical }}=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\n假设世界坐标系和其中一个摄影坐标系原点重合\nIntrinsic and extrinsic parameters of the cameras are known, world coordinate system is set to that of the first camera.\n返回到世界坐标系当中\n对于摄像机坐标系上的任意一点坐标$x’$，我们可以将其变换为世界坐标系表示\n\n\n\n\nx'=RX_w+t\nx^{\\prime T}\\cdot \\left[t \\times\\left(R x^{}+t\\right)\\right]=0\\quad \\Rightarrow \\quad x^{\\prime T}[t_x]Rx=0\n\\text { Recall: } \\quad \\mathbf{a} \\times \\mathbf{b}=\\left[\\begin{array}{ccc}\n0 & -a_{z} & a_{y} \\\\\na_{z} & 0 & -a_{x} \\\\\n-a_{y} & a_{x} & 0\n\\end{array}\\right]\\left[\\begin{array}{l}\nb_{x} \\\\\nb_{y} \\\\\nb_{z}\n\\end{array}\\right]=\\left[\\mathbf{a}_{\\times}\\right] \\mathbf{b}\n\\boldsymbol{x}^{\\prime T}\\left[\\boldsymbol{t}_{\\times}\\right] \\boldsymbol{R} \\boldsymbol{x}=0 \\quad \\Rightarrow \\quad \\boldsymbol{x}^{\\prime T} E \\boldsymbol{x}=0\n\\boldsymbol{x}^{\\boldsymbol{T}} E \\boldsymbol{x^\\prime}=0\nLecture10 更新解法\n\n\n\n\n\n由于$x’$是右边那个极平面的法向量，所以会垂直于极线，那么对于满足任意$x’$都垂直于极线的方程，显然就是极线的方程\n$\\boldsymbol{Ex}$​​​​ is the epipolar line associated with $\\boldsymbol{x}\\left(\\boldsymbol{l}^{\\prime}=\\boldsymbol{E} \\boldsymbol{x}\\right)$​​​​\nRecall: a line is given by $a x+b y+c=0$ or $\\mathbf{l}^{T} \\mathbf{x}=0$ where $\\mathbf{l}=\\left[\\begin{array}{l}a \\ b \\ c\\end{array}\\right], \\quad \\mathbf{x}=\\left[\\begin{array}{l}x \\ y \\ 1\\end{array}\\right]$\n\n\n\\boldsymbol{x'}^T \\boldsymbol{E} \\boldsymbol{x}=0\n\n$E \\boldsymbol{x}$​​ is the epipolar line associated with $\\boldsymbol{x}\\left(\\boldsymbol{l}^{\\prime}=\\boldsymbol{E} \\boldsymbol{x}\\right)$​​\n$\\boldsymbol{E}^{T} \\boldsymbol{x}^{\\prime}$ is the epipolar line associated with $\\boldsymbol{x}^{\\prime}\\left(\\boldsymbol{I}=\\boldsymbol{E}^{\\top} \\boldsymbol{x}^{\\prime}\\right)$\n$E \\boldsymbol{e}=0$ and $\\boldsymbol{E}^{\\top} \\boldsymbol{e}^{\\prime}=0$\n$E$​ is singular (rank two)\n因为$t_x$的rank是2\n\n\n$E$​ has five degrees of freedom\nThe calibration matrices $K$ and $K^{\\prime}$ of the two cameras are unknown\nWe can write the epipolar constraint in terms of unknown normalized coordinates:\n\n\n\\hat{\\boldsymbol{x}}^{\\prime T} \\boldsymbol{E} \\hat{\\boldsymbol{x}}=0 \\quad \\hat{\\boldsymbol{x}}=\\boldsymbol{K}^{-1} \\boldsymbol{x}, \\quad \\hat{\\boldsymbol{x}}^{\\prime}=\\boldsymbol{K}^{\\prime-1} \\boldsymbol{x}^{\\prime}\n\\begin{array}\\\\\nx=K[I,O]X\\\\\n\\hat{x}=K^{-1}x=[I,O]X\n\\end{array}\n这里的$[I,O]$相当于视角转换，由齐次坐标变为欧式坐标\n乘一个逆就可以变到另一个点的规范化坐标系\n\n\n\\begin{aligned}\n&\\hat{\\boldsymbol{x}}=\\boldsymbol{K}^{-1} \\boldsymbol{x} \\\\\n&\\hat{\\boldsymbol{x}}^{\\prime}=\\boldsymbol{K}^{\\prime-1} \\boldsymbol{x}^{\\prime}\n\\end{aligned}\n(K^{-1}x)^TE(K'^{-1}x')=0\nx^T(K^{-1})^TEK'^{-1}x'=0\n\\hat{\\boldsymbol{x}}^{\\prime T} E \\hat{\\boldsymbol{x}}=0 \\quad \\Rightarrow \\boldsymbol{x}^{\\prime T} \\boldsymbol{F} \\boldsymbol{x}=0 \\quad \\text { with } \\quad \\boldsymbol{F}=\\boldsymbol{K}^{\\prime-T} \\boldsymbol{E} \\boldsymbol{K}^{-1}\n$\\boldsymbol{F} \\boldsymbol{x}$ is the epipolar line associated with $\\boldsymbol{x}\\left(\\boldsymbol{l}^{\\prime}=\\boldsymbol{F} \\boldsymbol{x}\\right)$\n$\\boldsymbol{F}^{\\boldsymbol{T}} \\boldsymbol{x}^{\\boldsymbol{x}}$ is the epipolar line associated with $\\boldsymbol{x}^{\\prime}\\left(\\boldsymbol{l}=\\boldsymbol{F}^{\\boldsymbol{T}} \\boldsymbol{x}^{\\prime}\\right)$\n$\\boldsymbol{F} \\boldsymbol{e}=0$ and $\\boldsymbol{F}^{T} \\boldsymbol{e}^{\\prime}=0$\n$\\boldsymbol{F}$ is singular (rank two)\n$\\boldsymbol{F}$ has seven degrees of freedom\n\n5.7 Estimating the fundamental matrix5.7.1 The eight-point algorithm\n\\boldsymbol{x}=(u, v, 1)^{T}, \\quad \\boldsymbol{x}^{\\prime}=\\left(u^{\\prime}, v^{\\prime}, 1\\right)\n\\left[\\begin{array}{lll}\nu^{\\prime} & v^{\\prime} & 1\n\\end{array}\\right]\\left[\\begin{array}{lll}\nf_{11} & f_{12} & f_{13} \\\\\nf_{21} & f_{22} & f_{23} \\\\\nf_{31} & f_{32} & f_{33}\n\\end{array}\\right]\\left[\\begin{array}{l}\nu \\\\\nv \\\\\n1\n\\end{array}\\right]=0\n\\left[\\begin{array}{llllllll}\nu^{\\prime} u & u^{\\prime} v & u^{\\prime} & v^{\\prime} u & v^{\\prime} v & v^{\\prime} & u & v & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\nf_{11} \\\\\nf_{12} \\\\\nf_{13} \\\\\nf_{21} \\\\\nf_{22} \\\\\nf_{23} \\\\\n\\end{array}\\right]=0\n\nSolve homogeneous linear system using eight or more matches $\\rightarrow F$​\n这里会需要八个点，最终会变成两个矩阵相乘\nEnforce rank-2 constraint (take SVD of $F$​ and throw out the smallest singular value). Find F that minimizes $|\\mathrm{F}-\\hat{\\mathrm{F}}|=0$​ Subject to detf(F) $=0$​\n\n\n\n","categories":["CV"]},{"title":"Knowledge Graph Construction from Semi-Structured Data and Unstructured Data","url":"/2021/08/15/knowledge%20engineering/11.%20Knowledge%20Graph%20Construction%20from%20Semi-Structured%20Data/","content":"Knowledge Graph Construction from Semi-Structured Data and Unstructured Data\n1. Taxonomy Induction\nA taxonomy is a directed acyclic graph consisting of is-a relations between entities, including conceptual entities and individual entities.\n\nexample: a part of the taxonomy of Books\n\n\n\n\nHere, Taxonomy induction is to induce a taxonomy from the online encyclopedia.\n\nWikipedia has its own categorization system, but categories do not form a taxonomy with a fully-fledged subsumption hierarchy, so it is only a thematically organized thesaurus.\n\n\n\n\nTaxonomy induction from Wikipedia is to refine the Wikipedia category system by removing not-is-a relations. (WikiTaxonomy)\n\n1.1 The first step - Pre-Cleansing:\nRemove the categories used for Wikipedia administration, i.e., remove the categories whose labels contain any of the following strings: wikipedia, wikiprojects, lists, mediawiki, template, user, portal, categories, articles, and pages.\n\nWikipedia organizes many category pairs using patterns: Y X and X by Z (e.g., Miles Davis albums, Albums by artist)\n\n不是Is-A关系，remove\n\n\nThe relation between these categories is defined as is-refined-by, which is to better structure and simplify the categorization system and should be removed.\n\n1.2 The second step - Syntax-based Method：\nif a category pair share the same lexical head, then there exists an is-a relation between these two categories,\nlexical head 词汇中心词\ne.g., British Computer Scientists is-a Computer Scientists\n\n\nif the lexical head of one of the category occurs in non-head position in the other category, then a not-is-a relation is labeled between these categories. \ncategories的中心词出现在非中心词的位置，则不是IS-A关系\ne.g., Crime comics not-is-a Crime\n\n\n\n1.3 The third step - Connectivity-based Method ：\nFor each category $c$​, we find the article titled as the category name, e.g., article Microsoft for category Microsoft;\n首先对于每个category c，我们可以找到其article\n\n\nOn the found article, we collect all categories whose lexical heads are plural nouns $c a S e t=\\left\\{c a_{1}, c a_{2}, \\ldots, c a_{n}\\right\\} ;$​\n然后在我们找到的article页面下，收集每个category对应的中心词集合\n\n\nFor each $c$ ‘s super category $s c$ in the category system, we label the relation between $c$ and $s c$ as $i s-a$, if the head lemma of $s c$​ matches the head lemma of at least one category in caset.\n对于c的上级类sc，我们可以从其名称找到他的head lemmma（词根）\n比如Human names的中心词就是names\n然后进行匹配，如果某个sc和c是IS-A关系，则其lexical head 和head lemma是相同的\n\n\n\n\n\n1.4 The fourth step - Lexico-Syntactic based Method：\nlexico-syntactic patterns are leveraged to identify is-a and not-is-a relations between categories from large-scale corpora, e.g., all article in Wikipedia.\n\n\n1.5 The fifth step - Inference based Method：\npropagate the previously found relations based on the properties of transitivity of the is-a relation.\n传递性\n\n\n\n\n1.6 Exercise\nPlease extract a taxonomy from the following sentence (denote the answer as A is-a B): \nIBM, AMD, and Intel are High-tech companies using nanotechnology for several years.\nHigh-tech company is-a company\nIBM is-a High-tech company\nAMD is-a High-tech company\nIntel is-a High-tech company\n\n\n\n\n\n1.7 Summary: KG construction from Semi-Structured Data\nOnline encyclopedias are typical semi-structured data for knowledge graph construction.\nWe have introduced techniques on fact extraction, type inference, and taxonomy induction.\nAll introduced techniques have already been used to build real-word knowledge graphs, and shown good effectiveness and practicability.\nThere is no perfect technique on knowledge graph construction, so we need to study more.\n\n2. Knowledge Graph Construction from Unstructured Data (Text)2.1 Basic Tasks\n\n面向文本的实体链接或者发现新的实体\n关系抽取，已知两个实体看是否有关系/槽填充\n事件抽取\n\n2.2 Two Specific Tasks in Knowledge Engineering\nGeneral IS-A Relation Extraction \n(benefit to build taxonomies)\n\n\nTerminology/ Term Extraction \n术语抽取，利于领域构建知识图谱\n(benefit to domain-specific knowledge graph construction)\n\n\n\n2.3 General is-a Relation Extraction2.3.1 problem1\nThe general is-a relation is the semantic relationship between a more specific word (i.e., hyponym下位词) and the more general term (i.e., hypernym上位词).\n\nhyponym e.g., daisy and rose\nhypernym e.g., flower\n\n\nFeatures of is-a relations:\n\nTransitivity: $A$ is-a $B, B$ is-a $C \\rightarrow A$ is-a $C$e.g., dog is-a mammal, mammal is-a animal $\\rightarrow$ dog is-a anima\nAsymmetry: $A$​ is-a $B \\nrightarrow B$​ is-a $A$​e.g., dog is-a animal $\\nrightarrow$​ animal is-a dog\n\n\nTask Description:\n\nTriple generation: \nInput: a large scale corpus\n即输入是一大段语料\n\n\n\n\n\n\n\nOutput: triples denoting is-a relations\nBeijing is-a capital, Beijing is-a city, Tianjin is-a city, Hebei is-a province, Shanghai is-a city\n\n\n\n2.3.2 problem2\nTask Description: \nIS-A relation prediction: \nInput: a pair of candidate hyponym and hypernym, and the corresponding vector representations \n输入为下位词和上位词组成的pair\n\n\nOutput: true or false\ne.g., Pair(dog, animal) → true Pair(dog, cat) → false\n\n\n\n\n\n2.4 Methods Classification:\nPattern-based Methods (task: triple generation)\nDistributional Methods (task: is-a relation prediction) \nUnsupervised Distributional Methods \nSupervised Distributional Methods\n\n\n\n2.4.1 Pattern-based Methods：Hearst Patterns\n\nExercise\nPlease extract is-a relations from the following sentence with Hearst Patterns, and derive all is-a relations by the transitivity of the is-a relation.\n\nThere are further opportunities on exporting UK red meat to such countries as China, Japan, Korea, and Southeast Asian countries, including Singapore or Vietnam.\n\nChina is-a country\nJapan is-a country\nKorea is-a country\nSoutheast Asian country is-a country\nSingapore is-a Southeast Asian country\nVietnam is-a Southeast Asian country\nInference:\nSingapore is-a country\nVietnam is-a country\n\n\n\n\n\nWhy Introducing Distributional Methods?\nLimitations of Pattern-based Methods: \nThe coverage and generalization are uncertain: \n无法推广\nThe hyponym and hypernym must appear in a sentence at the same time.\n召回率低\n\n\n\n\nDistributional Methods aim to solve the problem of co-occurrence sparsity between the hyponym and hypernym.\n\n2.4.2 Unsupervised Distributional Methods\nDistributional Inclusion Hypothesis: \n\nIt assumes that a hyponym only appears in some of its hypernym‘s contexts, but a hypernym appears in all contexts of its hyponyms. \n下位词只出现在一些上位词的contexts里，而上位词出现在下位词所有contexts里\ne.g., the concept “fruit” has a broader spectrum of contexts than its hyponyms, such as ”apple“, ”banana“ and “pear”.\n\n\nA Classic Asymmetric Distributional Similarity Measure: WeedsPrec.\n\n一种无监督提取is-a关系的方法\nIt captures the features of $u$, which are included in the set of features for a broader term $v$.\n\n\n\\operatorname{WeedsPrec}(u \\rightarrow v)=\\frac{\\sum_{f \\in F_{u} \\cap F_{v}} W_{u}(f)}{\\sum_{f \\in F_{u}} W_{u}(f)}\nFor each term $u, v$​​​ is candidate hypernym;\n\n$u,v$都是候选上位词\n\n\n$f$ represents a contextual word with which $u$​ co-occurs;\n\n$f$代表u的context word\n\n\n$F_{u}$ is a set of $f$\n$F_u\\cap F_v$是$u,v$背景词的交集\n\n\n$W_{u}(\\mathrm{f})$ quantifies the statistical association between the $f$ and $u$, such as Point-Wise Mutual Information (i.e., $P M I(u, f))$.\n\n\nPoint-Wise Mutual Information: \n\ncompute the point-wise mutual information between a word w and a context word c.\n\n\n\n\nP M I(w, c)=\\log \\frac{p(w, c)}{p(w) p(c)}\n$N:$​​​ How many sentences does the corpus contain? \n\n$ f(w) \\leq N:$ How many sentences contain  $w$ ?\n\n$ f(w, c) \\leq f(w):$  How many sentences contain $w$ and $c$ ? \n\n$ f(\\mathrm{c}) \\leq N:$​​ How many sentences contain $c$​​​ ?\n$p(w)=f(w) / N    $\n$p(\\mathrm{c})=f(c) / N  $\n$p(w, c)=f(w, c) / N$\n\nWhen the correlation between two words $w$​ and $c$​ is strong, $P(w, c)$​ will be much larger than $P(w) \\mathrm{P}(c)$​, so $P M I(w, c)$​ is larger.\n\n\n2.5 Supervised Distributional Methods\nRepresent the term pair $(u, v)$ as a combination of $\\boldsymbol{u}$ and $\\boldsymbol{v}$ (vector representations)\nConcat : $u \\oplus v$\nDiff: $v-u$\nSum: $\\quad u+v$\nDot-product: $u \\cdot v$\n\n\ntrain a binary classifier over the representation $(\\boldsymbol{u}, \\boldsymbol{v})$​\n\n2.5.1 What’s wrong with simple calculations?\n\n这种方法可能由于数据集的原因导致并没有学会推理而是记住某个词就是上位词\n\n2.5.2 Project learning\n学习如何将下位词映射到上位词的空间，再进行分类\n\nProject learning learns a function to measure how possible there is an is-a relation between two words.\n\nKeyPoint: A projection tensor $T$ is used to project the hyponym vector into the hypernym vector.\n\n​    下位词映射到上位词\n\n\n\n\n\nInput: Given a query $\\mathbf{q}$​ and a candidate hypernym $\\mathbf{h}$​\nOutput: The possibility that pair(q, h) is an is-a relation\n\nStep1: look up word embeddings $\\mathbf{q}$​ and $\\mathbf{h}$​ through a embedding table\n\nStep2 : Randomly initialize the projection vector $\\boldsymbol{T}(K \\times d \\times d)$​\nStep3: Calculate the similarity vector s:\n\n\ns=q^{T} T_{i} h\nStep4: Map vector $s$ to score $\\mathrm{y}, \\mathbf{F}$ is generally a multilayer perceptron :\n\n\n\\mathrm{y}=\\mathrm{F}(s)3. Terminology/ Term Extraction\nTerminology extraction is associated with some other tasks, such as NER,keyword extraction, etc.\n\nDifferent from other tasks, terminology extraction is highly related to the domain.\n\nTerminology extraction is th key issue of ontology construction, text summarization, knowledge graphs, etc.\n\n\n\n3.1 Definition—Framework3.1.1 Input Text:\nEg: He did not try to navigate after the first bold flight, for the reaction had taken something out of his soul.\n\n3.1.2 Preprocessing\nTokenization: Tokenization describes splitting paragraphs into sentences, or sentences into individual words.\n\nEg: [‘He’, ‘did’, ‘not’, ‘try’, ‘to’, ‘navigate’, ‘after’, ‘the’, ‘first’, ‘bold’, ‘flight’, ‘,’, ‘for’, ‘the’, ‘reaction’, ‘had’, ‘taken’, ‘something’, ‘out’, ‘of’, ‘his’, ‘soul’, ‘.’]\n\n\nCleaning(Stopwords):**A majority of the words in a given text are connecting parts of a sentence rather than showing subjects, objects or intent. Word like  ‘the’ or ‘and’ can be removed by comparing text to a list of stopword.\n\nEg: [‘try’, ‘navigate’, ‘first’, ‘bold’, ‘flight’, ‘reaction’, ‘taken’, ‘something’, ‘soul’, ‘.’]\nPOS: Part of Speech (POS) often requires look at the proceeding and following words and combined with either a rule-based or stochastic method. \n词性标注\nEg: [(‘try’, ‘VB’), (‘to’, ‘TO’), (‘navigate’, ‘VB’), (‘first’, ‘JJ’), (‘bold’, ‘JJ’), (‘flight’, ‘NN’), (‘reaction’, ‘NN’), (‘taken’, ‘VBN’), (‘something’, ‘NN’), (‘soul’, ‘NN‘)]\n\n\nStemming:Stemming is a process where words are reduced to a root by removing inflection through dropping unnecessary characters, usually a suffix.\n通过去除后缀找词根\nEg: The stemmed form of leafs is: leaf\nEg:The stemmed form of leaves is: leav\n\n\nLemmazation:Lemmazation is an alternative approach from stemming to removing inflection.\n找词根\nEg: The lemmatized form of leafs is: leaf\nEg: The lemmatized form of leaves is: leaf\n\n\n\n3.1.3 Filtering\nFiltering:\nCommon Dictionary Filtering (Filter by common Chinese dictionary)\n去除常用词\nIf Candidate Terms appear in common Chinese dictionary:Delete the Candidate Terms\n\n\n\n\nIrregular Filtering (Filter irregular words)\n\nFor each Candidate Terms in Candidate list, Calculate the frequency of strings appearing in the corpus:$\\mathrm{A}=$ frequency of Candidate Terms striing$B=$ frequency of Candidate Terms string removing the first character$\\mathrm{C}=$ frequency of Candidate Terms string removing the last character Then: score $=$ $\\mathrm{A} /(\\mathrm{B}+\\mathrm{C}-\\mathrm{A})$If score $&lt;$​​​ Threshold: Delete the Candidate Terms\n\n\nExample:A = “right of transit passage”B = ”right of transit“                    score = 0.99, keep the Candidate Terms:C = “of transit passage”.             ”right of transit passage“\n\n\n3.2 Approaches\nLinguistic-based approaches\nStatistical-based approaches\nGraph-based approaches\n\n3.2.1 Linguistic-based approaches — Chunker\nNPs——Noun Phrases:\n\nNPs: Noun Phrases\nA type of phrase whose grammatical function is equivalent to a noun\n\n\nNoun phrases can name a person, place, thing or idea.\nExamples:\nI want a skateboard.\nThe yellow house is for sale.\n\n\nNoun phrases can generally serve as subject, object, attributive and other components in a sentence.\n\n\nTheory\n\nMore than 90% of the terms extracted in corpus are Noun Phrases\n\n\n\n\n\nToolkit\nNLTK RegexpParser: Convert regular expressions into syntax trees\nStep1: Define patterns of NPs\nStep2: Find Candidate Terms using Chunker\nStep3: Candidate Terms Filtering\n\n\n\nStep1: Define patterns of NPs\n一些模板\n\n\n\n{\\}\n{*+}\n{\\**+}\n{\\*+}\n\n\nStep2: Find Candidate Terms using Chunker\nDefine NP patterns using methods before\nNPChunker = nltk.RegexpParser(patterns)\n\n\nPOS tagging for each sentence\ntagged_words = [nltk.pos_tag(word) for word in train_dataset]\n\n\nUsing NPChunker to get tree and Candidate Terms\ntree = NPChunker.parse(tagged_words)\n\n\n\n\n3.2.2 Statistical-based approaches\nTheory\nCandidate terminology with higher frequency is more likely to be a terminology.\n\n\nStatistical Feature:\nTermhood: Measure the relevance between term and domain.\nTF-IDF\n\n\n\n\nUnithood: Measure the collocation and adhesion within term.\n\nMI(Mutual information)\n\n\nEvaluate the importance of a word to a document.\n\nAssuming word with higher TF value and higher IDF value is more relevant to domain.\nTF: Term frequency.\n\n\n\n\n\\mathrm{TF}=\\frac{\\text { Number of certain word in a document }}{\\text { Number of all words in a document }}\nIDF: Inverse document frequency\n\n\n\\text{IDF}=\\log \\left(\\frac{\\text { Number of all documents in corpu }}{\\text { Number of documents containing the certain word }+1}\\right)\nFormula\nTF-IDF value is directly proportional to the number of occurrences of a word in the document and inversely proportional to the number of occurrences of the word in the whole corpus.\n\n\n\n\nExercise——TF-IDF\nSuppose there are 100 words in a document, and the word “cow” appears three times. The word “cow” has appeared in 1,000 documents, and the total number of documents is 10,000,000. What is the TF-IDF score of the word “cow”?\n\n\n\\begin{array}{ll}\n\\text{TF}=\\frac{3}{100}\\\\\n\\text{IDF}=\\log \\frac{10000000}{1000+1}=3.9996\\\\\nTF\\times IDF=0.119987\n\\end{array}\n例：假定《亚洲的网络技术》一文长度为1000个词，“亚洲”、“网络”、“技术”各出现20次，则这三个词的“词频”（TF）都为0.02。 然后，搜索Google发现，包含“的”字的网页共有250亿张（假定这就是中文网页总数），包含“亚洲”的网页共有62.3亿张，包含“网络”的网页为0.484亿张，包含“技术”的网页为0.973亿张。计算“亚洲”、“网络”、“技术”的TF-IDF值.\n\n\n\\begin{array}{ll}\n\\text{IDF(亚洲)}=\\lg\\frac{250}{26.3}=0.603\\\\\n\\text{IDF(网络)}=\\lg\\frac{250}{0.484}=2.713\\\\\n\\text{IDF(技术)}=\\lg\\frac{250}{0.973}=2.410\\\\\n\\text{TF-IDF(亚洲)}=0.603\\times 0.02=0.01206\\\\\n\\text{TF-IDF(网络)}=2.713\\times 0.02=0.05426\\\\\n\\text{TF-IDF(技术)}=2.410\\times 0.02=0\\\\\n\\end{array}4. Statistical-based approaches—MI4.1 PMI\nA special case of MI. It is used to calculate the degree of association between words in NLP.\n用于计算两个词的联系程度\n\n\n\n\nP M I(x ; y)=\\log _{2} \\frac{p(x, y)}{p(x) p(y)}=\\log _{2} \\frac{p(x \\mid y)}{p(x)}=\\log _{2} \\frac{p(y \\mid x)}{p(y)}\nxy represents $2 \\sim \\mathrm{n}(\\mathrm{n} \\geq 2)$ words. For example, when two words are used, $\\mathrm{x}$ represents the former word and $\\mathrm{y}$ represents the latter word; In three words, $\\mathrm{x}$ represents the first (two) words and y represents the last two (one) words; And so on. $-$​ Usually used for double-word terminology.\n\nxy表示一个组合,xy是内部的词\n\n\nWhen the correlation between words $x y$ is strong: $P M I&gt;0$, and when it is weak: $P M I \\approx 0$\n\nA large PMI value means that the combination between words is tight, and the more likely it is to become a terminology.\n\n\n4.2 Exercise— PMI\nUse the following Co-occurrence Matrix to represent the frequency of simultaneous appearance of two words in text. Calculate the PMI between information and data.\n\n\n\n\\begin{array}{l}\nP(\\text{Information,data})=6/19\\\\\nP(\\text{Information})=11/19\\\\\nP(\\text{data})=7/19\\\\\nPMI(\\text{Information}|\\text{data})=\\log_2\\frac{6/19}{(11/19)\\times(7/19)}=\\log_2\\frac{114}{77}\n\\end{array}5. Graph-based methods5.1 PageRank And TextRank\nPageRank:\nCalculate the importance of webpages based on the link between them.\n如果这个网页被多次链接，那么这个网页更重要\n\n\n\n\nTextRank:\nRegard ‘word’ as ‘webpage’Calculate the importance of words based on the \nco-occurrence between them.\nTurn the directed graph in PageRank into an undirected graph.\n把单词当成网页，算单词的重要性\n\n\nFeature\nTextRank can extract terminologies from a single document without relying on other corpora.\n无需训练，可直接抽取术语\n\n\n\n\n\n5.2 PageRank5.2.1 PageRank’s Theory\n\nRegard the Internet as a directed graph, with webpages as nodes in the graph and links between webpages as edges in the graph.\nWhen a webpage is linked by many other webpages, it means that this webpage is more important, that is, the PR value (PageRank value) of this webpage will be higher.\nIf a webpage with a high PR value links to another webpage, the PR value of the linked webpage will increase accordingly.\n简单理解就是互联网是一张巨大的有向图，网页被链接越多那么重要程度越高，并且这种重要程度可以传递给邻居\n\n5.2.2 PageRank’s Formula\nDivide the PR value of a webpage equally according to the total number of its links, and take this value as the voting value of the webpage to its’ linked webpage. Therefore, for webpage $i$​, its PR value can be expressed as:\n$i$​​处的PR值等于邻居的PR值除以其自身的出度\n\n\n\n\nP R(\\mathrm{i})=\\sum_{j \\in B_{\\mathrm{i}}} \\frac{P R(\\mathrm{j})}{O u t(j)}\n$P R(\\mathrm{i}): \\mathrm{PR}($​ PageRank )score of webpage i.\n$B_{\\mathrm{i}}$ : Collection of webpages that linked to webpage i.\nOut(i): Out degree of webpage $\\mathrm{i}$​.\n\n5.2.3 Example\nAs shown in the right figure, suppose a set consisting of only four webpages: $A, B, C$​ and $\\mathrm{D}$​. If webpages $\\mathrm{B}, \\mathrm{C}$​ and $\\mathrm{D}$​ are all linked to webpage $\\mathrm{A}$​, and webpages $\\mathrm{B}, \\mathrm{C}$​ and $\\mathrm{D}$​ have no other links, then the PR value of webpage $A$​ will be the sum of the PR values of webpages $B, C$​ and $D$​ :\n\n\nP R(A)=P R(B)+P R(C)+P R(D)\n\nAs shown in the right figure, webpage $B$​ has links to webpage $A$​ and $C$​, webpage $D$​ has links to webpages $A, B$​ and $C$​. Suppose one webpage can only vote for another webpage once. So webpage B will vote for $1 / 2$​ of the linked webpage and webpage $\\mathrm{D}$​ will vote for $1 / 3$​ of the linked webpage. In this case, the PR value of webpage A is:\n\n\nP R(A)=\\frac{P R(B)}{2}+\\frac{P R(C)}{1}+\\frac{P R(D)}{3}\n5.2.4 Extended formula of PageRank\nThe above formula assumes that users only click the link of the current webpage to browse the next webpage, but the random browsing method is more in line with the real situation. Thus, the random browsing model is generated, and the $P R$​​ value of each web page in the random browsing model is calculated by the following formula:\nd 阻尼系数，即有一定可能随机跳转页面\n最终PageRank会收敛到一个稳态\n\n\n\n\nP R(i)=(1-\\mathrm{d})+d \\times \\sum_{j \\in B i} \\frac{P R(j)}{\\operatorname{Out}(j)}\n$P R$​ value is initially set as $1 / \\mathrm{N}$​.\n\nN: Num of all webpages.\n\nd: Damping coefficient, representing the probability of browsing webpages accordance with the link, default value is $0.85$​.\n1-d: The probability that the viewer randomly browses another webpage.\n\n5.3 Graph-based approaches—TextRank\nPageRank:Construct graph according to the link relationship between webpages.\nTextRank:Construct graph according to the co-occurrence relationship between words.\n\n5.3.1 TextRank’s Formula:\n其相当于构建了有向有权图,i点的重要程度由其邻居决定\n\n\nW S\\left(\\mathrm{~V}_{\\mathrm{i}}\\right)=(1-d)+d \\times \\sum_{\\mathrm{V}_{\\mathrm{i}} \\epsilon\\left(\\mathrm{V}_{\\mathrm{i}}\\right)} \\frac{w_{j i}}{\\sum_{\\mathrm{V}_{k} \\in O u t\\left(\\mathrm{v}_{j}\\right)} w_{j k}} W S\\left(\\mathrm{~V}_{j}\\right)\n$w_{j i}$ ​ :Weight of edge connecting node $\\mathrm{i}$​ and node $\\mathrm{j}$​.\n$W S\\left(\\mathrm{~V}_{\\mathrm{i}}\\right):$ Weight of word $\\mathrm{i}$, initially value is 1 .\n$\\operatorname{Out}\\left(\\mathrm{V}_{\\mathrm{i}}\\right):$ Out degree of word $\\mathrm{i}$.\n\n5.3.2 Steps Of TextRank\nInputText:\n淡黄的长裙, 蓬松的头发, 牵着我的手看最新展的油画\n\n\nPreprocessing:\n\n淡黄 长裙 蓬松 头发\n牵我 手 看最新 展出 油画\n\n\nConstruct graph $\\mathbf{G}(\\mathbf{V}, \\mathbf{E}): \\mathrm{V}$​ is composed of words generated in the above steps, and then use the co-occurrence relationship to construct an edge between any two nodes. The edge between two nodes is only when their corresponding words cooccur in a window of length $\\mathrm{K}$​. Given $\\mathrm{K}=2$​ :\n\n\n\n\nIteration: Iteratively calculate the weight of each node until convergence according to the formula.\n\n\nW S\\left(\\mathrm{~V}_{\\mathrm{i}}\\right)=(1-d)+d \\times \\sum_{\\mathrm{V}_{\\mathrm{i}} \\epsilon\\left(\\mathrm{V}_{\\mathrm{i}}\\right)} \\frac{w_{j i}}{\\sum_{\\mathrm{V}_{k} \\in \\text { out }\\left(\\mathrm{v}_{j}\\right)} w_{j k}} W S\\left(\\mathrm{~V}_{j}\\right)","categories":["KnowledgeEngineering"]},{"title":"Knowledge Graph Construction from Semi-Structured Data","url":"/2021/08/15/knowledge%20engineering/10.%20Knowledge%20Graph%20Construction%20from%20Semi-Structured%20Data/","content":"Knowledge Graph Construction from Semi-Structured Data\n1. What is semi-structured data?\nSemi-structured data is a form of structured data that does not obey the tabular structure of data models associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers to separate semantic elements and enforce hierarchies of records and fields within the data.\n\nA typical example: online encyclopedias\n\n百科数据\n\n\n\n\n2. Knowledge Graphs Built from Online Encyclopedias\n\nBabelNet世界最大多语言词典\n\n2.1 Modules of Knowledge Graph Construction from Online Encyclopedias\nFact Extraction\nType Inference（推断类别）\nTaxonomy Induction  （分类归纳）\n\n2.2 Fact Extraction\n\nFacts are factual knowledge represented as triples, each of which is in the form of . \nsubject: an entity (an individual entity or a conceptual entity (i.e., concept))\npredicate: property/relation \nobject: an entity or a literal value\n\n2.2.1 Fact Extraction from Infoboxes\n\n\nEditing Format：为渲染前编辑的形式（用于抽取）\nRendered Output：渲染之后的输出\n\n2.2.2 Method\nGeneric Infobox Extraction: do not map synonymous properties,\n不进行融合，即不做同义词归一化\ne.g., birthdate与dateOfBirth\n\n\nMapping-based Infobox Extraction: map synonymous properties\n写了大量的规则， 做同义词匹配\nRules: http://mappings.dbpedia.org/\n\n\n\n\n\nDBpedia Ontology: 2795 properties, 685 concepts with subClassOf relations.https://wiki.dbpedia.org/services-resources/ontology\n做完归一化后可以得到本体\n\n\n\n2.2.2.1 Fact Extraction from DRpedia\n\n2.2.2.2 Fact Extraction from YAGO\n\nYAGO also defines infobox heuristics(启发法) to map infobox properties to YAGO relations.\n\nDifference between YAGO and DBpedia: YAGO uses LEILA to parse literals of different types, such as dates and quantities, and normalizes units of measurement to ISO units.\n\n利用LEILA进行literal解析，并归一到ISO单位\n\n\n\n\n2.2.3 Fact Extraction from Categories\n\n正则表达式抽取\n\n\n\nsubject: the article entity\nQiang Yang\n\n\npredicate: the target relation\nbirthOnDate\n\n\nobject: the string captured by the brackets of the regular expression\n1964 births\n\n\n提取完之后需要做property\nNote: Domain and Range checking is necessary.\n\n\n\n\n\n如图我们可以看到，直接把pedia的主题作为subject，然后从categories里面进行正则化匹配，从而得到predicate 和object\n\n\n\n匹配完后需要进行domain和range的checking\n\n2.3.4 Exercise\nIn the Wikipedia article page of “Mo Yan”, the categories as follows. Please extract facts from them in turtle using YAGO category maps without domain and range checking, and clarify the subsequent validation steps on the extracted fact.\nYAGO category maps\n\n\n\n\n@prefix expr: &lt;http://www.example.org/resource/&gt;@prefix expp: &lt;http://www.example.org/property/&gt;expr: MoYan expp: bornOnDate &quot;1955&quot;;\t\t\texpp: hasWonPrice expr: Han Chinese Nobel,\t\t\t\t\t\t\t  expr: Nobel,\t\t\t\t\t\t\t  expr: Mao Dun Literature Prize.\n\n需要check property的 range和domain\n\n2.3 Type Inference\nOne of the most valuable kinds of knowledge is type information, which refers to the axioms stating that an instance is of a certain type.\n\nBarack Obama rdf:type President of the United StatesItaly rdf:type Country in Europe\n2.3.1 Type Inference: Applications\n\n\nType是连接schema和Instance层的重要关系\n\n\n\n\nType Inference in Knowledge Graph Construction: 一种是从0开始的构建任务\n另外一种是有部分已经有type，其他的是做一种不全工作\n\n\n\n2.3.2 Type Inference from Infoboxes2.3.2.1 Names of infobox templates\nNames of infobox templates are extracted as entity types (DBpedia).\nraw type : Mapping to DBpedia Ontology $\\rightarrow$​ ontology classes\n直接从Info Box 找，进行映射\n\n\n\n\n2.3.2.2 concept-instance pairs\nProperty-value pairs in infoboxes are mapped to concept-instance pairs (Zhishi.me).\nEach article in the online encyclopedia is taken as an instance, so all articles form an instance set $I=\\{i_1, i_2,…, i_n\\}$​​;\nEach category (i.e., article category) in the online encyclopedia is treated as aconcept (i.e., class), so all categories compose a concept/class set C={c1, c2,…, cm};\nIn the infobox of each article, a set of property-value pairs can be extracted as$\\{,… , \\}$​​​.\n可以这么理解在维基百科里我们可以获得每篇文章的实体，以及每个类别的实体，然后我们像针对某一篇文章进行map，我们就可以从那篇文章的InfoBox里找到所有的 property-value pairs，然后我们就可以检查property是否在classes里，value是不是在instance set里，如果是那么就映射成了v type p\n\n\n\n\ninstances\n\n\n\nconcepts\n\n\n\nproperty-value pairs:{&lt;name, Schindler’s List&gt;, ,…}\n\n\n\nProperty-value pairs in infoboxes are mapped to concept-instance pairs (Zhishi.me).\nFor each property-value pair $$, if $p_k$ belongs to the concept set $C$ and $v_k$ belongs to the instance set $I$, then we infer that $p_k$ is the type of $v_k$.\n\n\n\n\n2.3.3 Type Inference from Categories\nEach article is usually assigned to several categories in online encyclopedias, so these categories can be taken as important candidate types for entities/ instances.\n\n在pedia上，我们可以发现有分配categories信息，而这样的信息可以作为候选type\n\n\nYAGO is the first work to infer entity types from the categories in Wikipedia by using heuristics（启发法）.\n\n\n\n\n\nYAGO classifies all categories into conceptual ones, which are actually entity types, and non-conceptual ones. \n但是Categories 有正确也有错误，所以要进行筛选\nYAGO parses each category name like Naturalized citizens of Germany into a pre-modifier (Naturalized), a head (citizens) and a post-modifier (Germany); \n大致意思就是把每个catygories解析成pre-modifier+head+post-modifier形式\n\n\nIf the head is not an acronym缩写 (e.g., NYC) and is a plural复数 word, then the given category is a candidate conceptual category; \nFor each of the candidate conceptual categories, if it contains a non-conceptual word (manually summarized by YAGO), then it may be an administrative category (e.g., Articles with unsourced statements), \n如果候选词里面包含了一些YAGO总结的非概念性词，那么应该舍弃掉\n\n\nor a relational category (e.g., 1987 births), or a thematic category (Physics), and it will be filtered out.\n关系性的词以及理论词\n\n\n\n\n\n\n\n如图我们可以发现大部分复数中心词都是正确的category\n\n2.3.3.1 Problems:\nThe heuristics cannot be applied in some languages (e.g. Chinese and Japanese), in which nouns have no explicit singular or plural forms. \n对于没有复数的语言不利；\n\n\nThe heuristics cannot catch the semantic associations between instances and classes (i.e., categories), which may lead to mistakes in the process of type inference.\n没有考虑语义关系，可能造成映射错误，因为并不是所有复数的都是category，如图所示\n\n\n\n\n2.3.3.2 MulType\nTo solve the above problems, MulType mines a language-independent feature: attribute (i.e., property) to perform language-independent type inference.\n\nThe assumption of the method: \n\nGiven the attributes (i.e., properties) “actors, release date, director” of an instance, we intuitively infer its type as “Movie”. \nactor 等具有代表性，可以推断\n\n\nGiven the attributes “name, foreign name” of an instance, we cannot infer its type by intuition.\nname foreign name 没有区分度，无法推断\n\n\n代表属性假设\nMulType uses attributes to build semantic associations between instances and classes (i.e., categories), and presents an attribute-driven type inference assumption: In Wikipedia, if an instance contains representative attributes of one of its classes (i.e., categories), there may exist a rdf:type relation from the instance to the category with a high probability.\n如果一个实体包含一个类的代表属性，那么他有很高的概率是这个类\n\n\n\n\n\n2.3.3.3 process\nBased on the above assumption, MulType extracts type information in two steps: attribute extraction and type information generation.\n\n\n\nAttribute extraction:\nInstance attribute extraction: Extract instance attributes directly from infoboxes (see Figure (a)). \nClass attribute extraction: Extract class attributes from infobox templates (see Figure(b)), which are templates that provide standardized information (i.e., attributes) across related articles.\nNot enough! There are only thousands of infobox templates.\n\n\n\n\n\n2.3.3.4 Class attribute extraction:\nDefinition 1: Infobox Template based Extraction Rule (IT-ER). Given an infobox template it and a class $c$, the local names of it and $c$ are respectively denoted as $n(i t)$ and $n(c) .$ All the attributes of it can be propagated传播 to $c$ if\n$n($​ it $)$​ and $n(c)$​ are the same (ignoring case and the difference in singular and plural forms for some languages),\nor “Category:n(it)” can redirect to $c$​,\nor $n($​ it $)$​ can redirect to $n(c)$​ or $n(c)$​ can redirect to $n($​ it $)$​.\n\n\n可以这么理解，为了从模板进行抽取关系，我们需要进行匹配，匹配其实就是看模板的local name和类别的local name，如果“一样”，则可以讲模板的所有属性传递到某一类别\n\nExamples: \n1) Infobox template“Template:Infobox islands”and class“Category:Islands”have the same local name“islands”;\n\nsame local name \n\n2) The singular forms of“n(Category:Countries)” (i.e.,“Country”) and“n(Template:Infobox country)” (i.e.,“country”) are the same when ignoring case;\n\n忽略大小写\n\n3) If we submit the query “Category:n(Template:Infobox university)”to Wikipedia, it can be redirected to“Category:Universities and colleges”;\n\n对于两个category如果有从定向的关系，那么这两者的属性是一样的\n\n\n\n“n(Category: States of The United States)”can redirect to“n(Template:Infobox U.S. state)”.\n模板和category有重定向关系，具体来看，我们可以从WIKI的主题下面的小字看到这种关系\n\n\n\n\nTop-Down\n\nDefinition 2: Top-Down Hierarchy-based Extraction Rule (TDH-ER). If a class c has no attribute extracted from infobox templates, and it has super-classes with attributes, then all the attributesof its super-classes should be inherited by c.\n\n对于那些无法从模板的到属性的类，我们可以从他的父类进行继承\n\n\nThis is inspired by the inheritance in object-oriented programming that one class can inherit all the attributes of its super-class. \n\nExample: class Manager inherits the attribute name and gender from class Person.\n\n\nSets(one set corresponds to one language) of isA relations between classes are collected from open knowledge bases, including DBpedia, Yago, BabelNet, WikiTaxonomy and Zhishi.schema, in order to refine the class hierarchies of different languages in Wikipedia.\n我们可以通过一些知识百科搜集的isA关系来建立这种层次结构\n\n\n\nBottom-Up\n\nDefinition 3: Bottom-Up Hierarchy-based Extraction Rule(BUH-ER). Ilf a class c has no attribute extracted from infobox templates, and it has hyponyms（下义词） (include instances and sub-classes) with attributes, then the attributes can be propagated to c when they are shared by more than half of these hyponyms.\n\n对于一些上位词，它可以从下位词拿到属性，但必须有半数下位词拥有这个属性\n\n\nThis is based on the idea that a class is traditionally a placeholder for a set of instances sharing similar attributes (Dowty, Wall, &amp; Peters, 1981).\n\nDowty, D. R., Wall, R., &amp; Peters, S. (1981). Introduction to Montague semantics. Springer Netherlands.\n\n\n\n\n\n\n有解决70%的页面没有InfoBox，其次InfoBox的并不完整\n\n2.3.3.5 acquire the most similar instances\n这里是想要通过近义词来完成type任务，因为并不是所有实例都有infobox，那么就无法通过实例来提取attribute\nType Information Generation \n\n\nAcquisition of Most Similar Instances: \n得到更多相似的实体\n\n\nInfoboxes are not always contained by the corresponding article pages of instances. \n信息框并不总是包含在实例的相应文章页面中。\n\n\nThe information in existing infoboxes may be not complete.\n缺了的属性可以从相似词里拿\n\n\n\n\ntry to leverage the attributes of most similar instances to complement the given instance’s attributes\n\nExample: The city Zhenjiang misses the attribute mayor, but its similar instance Changzhou has the attribute mayor, which can be borrowed by Zhenjiang.\n\n\nTo acquire the most similar instances that have attributes, different similarity metrics can be used to measure the similarity degree between instances. \n\nGiven two instances $i_1$​​ and $i_2$​​, the Context Similarity Metric is computed as\n\n\n\\operatorname{CSM}\\left(i_{1}, i_{2}\\right)=\\frac{v\\left(i_{1}\\right) \\times v\\left(i_{2}\\right)}{\\left|v\\left(i_{1}\\right)\\right| \\times\\left|v\\left(i_{2}\\right)\\right|}\nwhere $v(i)$ is the vector representation of instance $i$​. Such vector representations can be trained by pre-trained models on the whole Wikipedia.\n\n大致意思是把词word embedding成vector，再计算相似度\n\n\nGiven two instances $i_1$​​ and $i_2$​​, the Existing Classes Similarity Metric is computed as\n\n\n\n\\operatorname{ECSM}\\left(i_{1}, i_{2}\\right)=\\frac{\\mid E C \\operatorname{set}\\left(\\left(i_{1}\\right) \\cap E C \\operatorname{set}\\left(i_{2}\\right) \\mid\\right.}{\\left|E \\operatorname{Cset}\\left(i_{1}\\right) \\cup E C \\operatorname{set}\\left(i_{2}\\right)\\right|}\n类似于Jaccard相似度\nwhere $v(i)$ is the set of classes (i.e., categories) in the Wikipedia article pages for each instance.\n就是每个article里categories的集合\n\n\n\n\nExample of Existing Classes Similarity Metric : suppose two instances $i_1= \\text{IntelliJ IDEA}, i_2= \\text{Apache XMLBeans}$​\n\n$\\text{ECSM}(i_1,i_2)=1/8=0.125$\n\nTo combine the Context Similarity Metric and Existing Classes Similarity Metric, an Integrated Instance Similar Score is defined by maximizing the product of these two metrics:\n\n\n\\operatorname{IISS}\\left(i_{1}, i_{2}\\right)=\\left(\\operatorname{CSM}\\left(i_{1}, i_{2}\\right)+1\\right) \\times\\left(\\operatorname{ECSM}\\left(i_{1}, i_{2}\\right)+1\\right)\nWe add 1 to each value of both the metrics before the multiplication to avoid a 0 Integrated Instance Similar Score. If there does not exist an instance satisfying $\\operatorname{IISS}\\left(i_{1}, i_{2}\\right)&gt;1$, the instance $i_{1}$​ will not get any similar instance.\n如果有很多&gt;1，则可以做一个排序，然后取top5\n\n\n\n2.3.4 Type Information Generation\nType Inference with Random Graph Walk: \nAfter extracting attributes of instances and classes, each instance, its attributes and the classes in its corresponding Wikipedia article page naturally form a graph, in which the given instance and its classes are linked by shared attributes. \nTo compute the probability of the class being the type of the given instance leveraging the graph structure, a random graph walk model is used to infer the types of each given instance.\n\n\n\n\n\nthe initial built graph of instance $i$​​\n\n2.3.4.1 Type Inference with Random Graph Walk:\nAfter adding most similar instances of $i$​ that have attributes into the built graph\n把最相似的实体加入图中\n\n\n\n\n\nthe attribute-driven type inference assumption: \nIn Wikipedia, if an instance contains representative attributes of one of its classes (i.e., categories), there may exist a rdf:type relation from the instance to the category with a high probability. \n如果一个实体拥有一个类的代表属性，那有一个较高概率属于这个类别\n\n\nTo model the above assumption in the random graph walk model, it is supposed that if some class can be reached by more of its representative attributes on the built graph of each instance, then the probability of the class being the type of the given instance is higher.\n拥有越多一个类的代表特征，概率越大\n\n\n使用random graph walk来看哪些可以尽可能走到category\n\n2.3.4.2\nTransition Probability Starting from the Given Instance. Given an instance $i$​, the transition probability $P_{L A}$​ from $i$​ to one of its attributes $a_{j}^{i}$​ in the built graph is\n\n\n就是一个加权\n\n\nP_{I A}\\left(i, a_{j}^{i}\\right)=\\alpha \\cdot \\frac{W_{e i g h t}\\left(a_{j}^{i}\\right)}{\\sum_{j=1}^{n} W e i g h t\\left(a_{j}^{i}\\right)}\nwhere $\\alpha \\in[0,1]$​​ is a constant, Weight $\\left(a_{j}^{i}\\right)$​​ is the reciprocal value of the frequency that $\\left(a_{j}^{i}\\right)$​​​​ occurs in the attribute sets of all the classes in Wikipedia.\nThe fewer classes an attribute is shared by, the more representative this attribute is.\nThe lower frequency of an attribute, the higher the weight.\n\n2.3.4.3\nTransition Probability Starting from the Given Instance. Given an instance $i$, the transition probability $P_{I S}$ from $i$ to one of its most similar instance $s_{r}^{i}$ in the built graph is defined as\n\n\nP_{I S}\\left(i, s_{r}^{i}\\right)=(1-\\alpha) \\cdot \\frac{\\operatorname{IISS}\\left(i, s_{r}^{i}\\right)}{\\operatorname{IISS}(i, *)}\n实体到相似词的概率\n$\\text { where } \\alpha \\in[0,1] \\text { is a constant, } I I S S\\left(i, s_{}^{i}\\right) \\text { is the Integrated Instance Similar Score between }$​ $i$​ and $s_{r}^{i}, \\operatorname{IISS}(i, )$​ is the sum of Integrated Instance Similar Score between $i$​​​​ and each of its most similar instances.\n\n2.3.4.4\nTransition Probability Starting from an Attribute. Given an instance $i$, the transition probability $P_{A C}$ from the attribute $a_{j}^{i}$ to a class $c_{k}^{i}$​ in the built graph is defined as\n\n\nP_{A C}\\left(a_{j}^{i}, c_{k}^{i}\\right)=\\frac{1}{\\left|N_{C}\\right|}\n一个属性被越少的类拥有，则代表性越高\n\na1 10000 个类有 -&gt; 1/10000\na2 100个类有 $\\rightarrow$ 1/100\n我们的目的是是为了从i出发能走到正确的type，我们希望走到最具代表性的属性，通过具有代表性的属性走到最终的type，那么i就有越大可能性属于某个type\n\n\nWhen walking a step from an instance to an attribute, the walk tends to choose the most representative attribute so that it has better chance to walk to the correct classes, i.e., types.\n\n实体游走时倾向于走最具代表性特征的路径，从而找到正确的class\n\n\nwhere $N_{C}$​ is the number of the classes that have edges connecting $a_{j}^{i}$​ in the built graph.\nFor example, $P_{A C}\\left(a_{1}, c_{2}\\right)=P_{A C}\\left(a_{3}, c_{2}\\right)=0.5$​\n\n\n\n用$a_j$​到$c_i$​​的变数的倒数来表示边的概率\n\n2.3.4.5 processa. Starting from an instance $i$, the walk may get to an attribute $a_{j}^{i}$ either by the directed edge from $i$ to $a_{j}^{i}$ with the transition probability $P_{I A}\\left(i, a_{i}^{i}\\right)$ or by two steps through one of $i$ ‘s most similar instances $s_{r}^{i}$ with the transition probabilities. $P_{L S}\\left(i, s_{n}^{i}\\right)$ and $P_{L A}\\left(s_{x}^{i}, a_{i}^{i}\\right) .$​​\nb. Starting from an attribute $a_{j}^{i}$, walk a random step to one of the classes $c_{k}^{i}$ with the transition probability $P_{A C}\\left(a_{j}^{i}, c_{k}^{i}\\right)$.c. Based on the random walk process, the probability starting from the instance $i$ to a class $c_{k}^{i}$ is computed as\n\n\\begin{aligned}\nP_{r g w}\\left(i, c_{k}^{i}\\right)=& \\sum_{j=1}^{n^{+}} P_{I A}\\left(i, a_{j}^{i}\\right) \\cdot P_{A C}\\left(a_{j}^{i}, c_{k}^{i}\\right)+\\\\\n& \\sum_{r=1}^{t} P_{I S}\\left(i, s_{r}^{i}\\right) \\cdot \\sum_{j=1}^{n^{+}} P_{I A}\\left(s_{r}^{i}, a_{j}^{i}\\right) \\cdot P_{A C}\\left(a_{j}^{i}, c_{k}^{i}\\right)\n\\end{aligned}d. After normalizing $P_{r g w}\\left(i, c_{k}^{i}\\right)$, if it is greater than a threshold, then $c_{l}^{i}$ is inferred as $i$ ‘s type.\n3. Type Inference from Text\nTwo steps on type inference from text: \n\nType extraction, which extracts the types of each entity from the first sentence of its corresponding article. \n为维基百科第一句话进行抽取\nThe first sentence in each article gives a textual definition of each entity, and it usually provides important type information.\n\n\nType disambiguation, which links extracted types to correct Wikipedia entities. \nSince the types extracted in the first step are just plain strings, the second step aims to assign real senses to these types.\n把提取的信息映射到class的实体，这样就达到了消歧\n\n\n\n\nType inference with a lightweight Syntactic Pattern: Type Extraction \n\n为维基百科第一句话进行抽取\nTo extract entity types from the first sentences in Wikipedia articles, LHD directly applies a simple syntactic pattern as follows\n\n\n\n\n\nwhere A is a sequence of any tokens, and B denotes candidate types.\nType inference with a lightweight Syntactic Pattern: Type Extraction \nOnly using the pattern to extract entity types is effective but the precision cannot be guaranteed. Thus, several heuristics are proposed to improve the extraction quality.\n简单但是有噪音，消除噪音\n\n\n\n\n\nSearch Linker. This linker takes the extracted type as the input, and leverages the Wikipedia Search API to return the disambiguated Wikipedia article.\n用这个API去检查\n\n\n\nExercise1)Given the Wikipedia article page of “IntelliJ IDEA”, including the first sentence, categories, and the infobox. Please write correct types from them (directly use plain strings).\n\n\n第一句话：integrated development environment(IDE)\ncategories：Free integrated development environments，Integrated development environments, Java development tools，Products\nInfobox：Software\n\n4. Taxonomy Induction（分类归纳）4.1 directed acyclic graph\nA taxonomy is a directed acyclic graph consisting of is-a relations between entities, including conceptual entities and individual entities.\n\nexample: a part of the taxonomy of Google products\n\n\n\n4.2 not form a taxonomy\nHere, Taxonomy induction is to induce a taxonomy from the online encyclopedia.\n\n\n\n\nWikipedia has its own categorization system, but categories do not form a taxonomy with a fully-fledged subsumption hierarchy, so it is only a thematically organized thesaurus.\n这里意思是讲pedia的层级关系可能和我们想要的IsA关系不一样，所以需要重新生成\n\n\n\n\n\n蓝色框不是正确的下位词\n\n4.3 Solution\nTaxonomy induction from Wikipedia is to refine the Wikipedia category system by removing not-is-a relations. (WikiTaxonomy)\n\nThe first step - Pre-Cleansing: \n\n预处理，总结pedia那些用于管理的词条，应该删去\nRemove the categories used for Wikipedia administration, i.e., remove the categories whose labels contain any of the following strings: wikipedia, wikiprojects, lists, mediawiki, template, user, portal, categories, articles, and pages. \nWikipedia organizes many category pairs using patterns: Y X and X by Z (e.g., Miles Davis albums, Albums by artist)\npattens 去组织的这些pair只是为了管理，并不是IsA\n\nThe relation between these categories is defined as is-refined-by, which is to better structure and simplify the categorization system and should be removed.\n\n\n\nThe second step - Syntax-based Method： \n\n基于语法的方法\nif a category pair share the same lexical head, then there exists an is-a relation between these two categories, \n中心词一样，就有IsA\ne.g., British Computer Scientists is-a Computer Scientists \n\n\nif the lexical head of one of the category occurs in non-head position in the other category, then a not-is-a relation is labeled between these categories. \n如果其中一个类别的词头出现在另一个类别的非词头位置，那么这些类别之间的关系就被标记为not-is-a关系。\ne.g., Crime comics not-is-a Crime\n\n\n\n\nThe third step - Connectivity-based Method ：\n\n\n\nFor each category $c$, we find the article titled as the category name, e.g., article Microsoft for category Microsoft;\nOn the found article, we collect all categories whose lexical heads are plural nouns caSet $=\\left\\{c a_{1}, c a_{2}, \\ldots, c a_{n}\\right\\}$\nFor each $c$ ‘s super category $s c$ in the category system, we label the relation between $c$ and $s c$ as $i s-a$, if the head lemma of $s c$ matches the head lemma of at least one category in caSet.\n\n\n\nThe fourth step - Lexico-Syntactic based Method： \nlexico-syntactic patterns are leveraged to identify is-a and not-is-a relations between categories from large-scale corpora, e.g., all article in Wikipedia.\n\n\n\n\n\nThe fifth step - Inference based Method： \npropagate the previously found relations based on the properties of transitivity of the is-a relation.\n\n\n\n\n4.4 Exercise\nPlease extract a taxonomy from the following sentence (denote the answer as A is-a B):\nIBM, AMD, and Intel are High-tech companies using nanotechnology for several years.\nHigh-tech is-a company\nIBM is-a High-tech company\nAMD is-a High-tech company\nIntel is-a High-tech company\n\n\n\n","categories":["KnowledgeEngineering"]},{"title":"Knowledge Graph Querying","url":"/2021/08/15/knowledge%20engineering/13.%20Knowledge%20Graph%20Querying/","content":"Knowledge Graph Querying\n1. RDF Query Language: SPARQL\nW3C Stack\nSPARQL is an RDF query language, that is, a semantic query language for databases, able to retrieve and manipulate data stored in RDF format.\n\n\n\nSPARQL stands for\n\n(originally) Simple Protocol and RDF Query Language.\n(now) SPARQL Protocol and RDF Query Language.\n\n\nHow to get information from RDF graphs by SPARQL?\n\nPattern matching\nPattern: describe subgraphs of the queried RDF graph\nMatching: match the pattern to the subgraphs to contribute an answer\nBuilding blocks: graph patterns (i.e. RDF graphs with variables)\n\n\n\n\n\n\n\n\n子图与知识图谱做匹配，把知识图谱中所有位于中心的词拿过来\n\n1.1 SPARQL Example\n1.2 Components of SPARQL Queries\n\nPrologue\n\nPrefix definitions for compact URIs;\nAttention (difference w.r.t. Turtle): No period (“.”) character as separator.\n\n\nQuery Form\n\nSELECT, DESCRIBE, CONSTRUCT or ASK\n\n\nDataset specification\n\nFrom\nSpecify the RDF dataset to be querying\n\n\nQuery pattern\n\nWHERE clause specifies the graph pattern to be matched\n指明什么样的语句需要被匹配\n\n\nSolution modifiers\n\nOrder: put the solutions in order; \nProjection: choose certain variables; \n指定返回特定变脸结果\n\n\nDistinct: ensure solutions in the sequence are unique; \n确保返回结果是唯一\n\n\nReduced: permit elimination of some non-unique solutions; \n防止删除重复\n\n\nOffset: control where the solutions start from in the overall sequence of solutions; \n从第n个返回\n\n\nLimit: restrict the number of solutions.\n限制返回数量\n\n\n\n\n\n2. SPARQL\nSPARQL Syntax (RDF term syntax)\nSyntax for IRI\nSyntax for literals\nSyntax for variables\nSyntax for blank nodes\nGraph Patterns for Query Pattern\nTriple Pattern\nDifferent Graph Patterns\n\n\n\n2.1 SPARQL Syntax: RDF Term Syntax\nSyntax for IRI\nIRIs are a generalization of URIs and are fully compatible with URIs and URLs.\nThe following fragments are some of the different ways to write the same IRI:\n\n\n\n\n\n一旦定义BASE，那么所有的都是这一个；PREFIX则不同\n\nThe general syntax for literals\n\nA string (enclosed in either double quotes, “…”, or single quotes, ‘…’);\nWith either an optional language tag (introduced by @) or an optional datatype IRI or prefixed name (introduced by ^^).\n\n\n\n\n\n字符串用单引号或双引号皆可\n\nSyntax for query variables\n\nQuery variables in SPARQL queries have the global scope\nUse of a given variable name anywhere in a query identifies the same variable.\n\n\nVariables are prefixed by either “?” or “\\$”;\nThe “?” or “\\$​” is not part of the variable name.\n\n\n\n\n\n\n\n查找所有谓语是name的三元组，返回其并于\n\nSyntax for blank nodes\n\n空节点就是表示变量\nBlank nodes in graph patterns act as non-distinguished variables, not as references to specific blank nodes in the data being queried.\nBlank nodes are indicated by either the label form, such as “_:abc”, or the abbreviated form “[]”.\nA blank node that is used in only one place in the query syntax can be indicated with [].\n\n\nThe same blank node label cannot be used in two different basic graph patterns in the same query.\n\n\n\n\n\nTriple Patterns\nare basic units of graph patterns;\nare written as a whitespace-separated list of a subject, predicate and object;\nThere are abbreviated ways of writing some common triple patterns;\nThe following examples express the same query:\n\n\n\n\nTriple Patterns: Predicate-Object Lists\n\nTriple patterns with a common subject can be written so that the subject is only written once and is used for more than one triple pattern by employing the “;” notation.\n共用主语\n\n\nExercise\nPlease rewrite the following triple pattern by using the same subject only once.\n\n?a int:number 123789 .?a int:pair 34567.?a int:id 666777 .\n?a int:number 123789；   int:pair 34567；   int:id 666777 .\n\nTriple Patterns: Object Lists\nIf triple patterns share both subject and predicate, the objects may be separated by “,”.\n\n\n\n\nExercise\nPlease rewrite the following triple pattern by using the same subject and predicate only once.\n\n?a string:name ‘Bob’.?a string:name ’Bobby’.?a string:name ‘Boob’.?a string:name ‘Bob_’.\n?a string:name ‘Bob’,’Bobby’,‘Boob’,‘Bob_’.\n2.2 Graph Patterns\nSPARQL is based around graph pattern matching.\nDifferent types of graph patterns for the query pattern (WHERE clause):\nBasic Graph Patterns, where a set of triple patterns must match;\nGroup Graph Pattern, where a set of graph patterns must all match;\nOptional Graph patterns, where additional patterns may extend the solution;\nAlternative (Union) Graph Pattern, where two or more possible patterns are tried;\n\n\n\n2.2.1 Basic graph patternsTriple patterns are similar to RDF triples, but any component can be aquery variable.\n?x foaf:name ?name .\n\nMatching a triple pattern to a graph: bindings between variables andRDF terms.\n\nMatching of basic graph patterns:\n\nA Pattern Solution of Graph Pattern GP on graph G is any substitution S such that S(GP) is a subgraph of G. \nSimple queries \nMultiple matches \nMatching RDF literals \nBlank node labels in query results\n\n\n\n\nSimple queries\n\n\n\n\n注意返回要有列名\n\nExerciseGiven the dataset and the SPARQL query as follows, please write the query results.\n# Graph: http://example/addresses@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; .&lt;http://example/president25&gt; foaf:givenName &quot;Bill&quot; .&lt;http://example/president25&gt; foaf:familyName &quot;McKinley&quot; .&lt;http://example/president27&gt; foaf:givenName &quot;Bill&quot; .&lt;http://example/president27&gt; foaf:familyName &quot;Taft&quot; .&lt;http://example/president42&gt; foaf:givenName &quot;Bill&quot; .&lt;http://example/president42&gt; foaf:familyName &quot;Clinton&quot; .\nPREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt;SELECT ?name ?familyWHERE &#123; ?x foaf:givenName ?name .?x foaf:familyName ?family .&#125;\n\n\n\n\nname\nfamily\n\n\n\n\n“Bill”\n“McKinley”\n\n\n“Bill”\n“Taft”\n\n\n“Bill”\n“Clinton”\n\n\n\n\n\nMultiple Matches\n\n\nExercise\nGiven the dataset and the SPARQL query as follows, please write the query results.\n\n# Graph: http://example/addresses@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; .&lt;http://example/president25&gt; foaf:givenName &quot;Bill&quot; .&lt;http://example/president25&gt; foaf:familyName &quot;McKinley&quot; .&lt;http://example/president27&gt; foaf:givenName “Bob&quot; .&lt;http://example/president27&gt; foaf:id 159486.&lt;http://example/president42&gt; foaf:givenName “Marry&quot; .&lt;http://example/president42&gt; foaf:familyName &quot;Clinton&quot; .\nPREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt;SELECT ?name ?familyWHERE &#123; ?x foaf:givenName ?name .?x foaf:familyName ?family .&#125;\n\n\n\n\nname\nfamily\n\n\n\n\n“Bill”\n“McKinley”\n\n\n“Marry”\n“Clinton”\n\n\n\n\n\nMatching RDF literals\n\n@prefix dt: &lt;http://example.org/datatype#&gt; .@prefix ns: &lt;http://example.org/ns#&gt; .@prefix : &lt;http://example.org/ns#&gt; .@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .:x ns:p &quot;cat&quot;@en .:y ns:p &quot;42&quot;^^xsd:integer .:z ns:p &quot;abc&quot;^^dt:specialDatatype .\n\n第一种是找不到结果的，因为不加@是不一样的意义\n\n\n\nBlank node labels in query results\n\nThere need not be any relation between a label in the form of a blank node in the result set and a blank node in the data graph with the same label.\n\n\nBlank node labels in query results\n\nThere need not be any relation between a label in the form of a blank node in the result set and a blank node in the data graph with the same label.\n\n\n\n\n2.2.2 Group Graph Patterns\nIn a SPARQL query string, a group graph pattern is delimited with braces: {}.\n\n\n\nBesides triple patterns, group graph pattern can contain constraints\nSyntax: Keyword FILTER followed by a filter expression\n\n\n\n\n\n\n\n\nname\nfamily\n\n\n\n\n“Bill”\n“McKinley”\n\n\n\n\n2.2.3 Optional Graph Patterns\nIf the optional part does not match, it creates no bindings but does not eliminate the solution.\nOptional patterns may result in unbound variables\n\n\n\nExercise\nGiven the dataset and the SPARQL query as follows, please write the query results.\n\n\n\n\n\nAnswer\n\n\n\n\n\nname\nfamily\n\n\n\n\n“Bill”\n“McKinley”\n\n\n“Bob”\n\n\n\n“Marry”\n\n\n\n\n\nConstraints in Optional Pattern Matching\n\n\n\nMultiple Optional Graph Patterns\n\n\n2.2.4 Alternative(Union) Graph Patterns\nCombine graph patterns so that one of several alternative graph patterns may match.\n\n\nExercise\nPlease write the SPARQL query on “list all volcanos located in Italy or Norway” given the following data.\n\ndepedia:Mount_Etna\trdf:type\tumbel-sc:Volcano;\t\t\t\t\trdfs:label \t&quot;Etna&quot;;\t\t\t\t\tp:location\tdbpedia:Italy.depedia:Mount_Baker\trdf:type\tumbel-sc:Volcano;\t\t\t\t\tp:location\tdbpedia:United_States.depedia:Beerenberg\trdf:type\tumbel-sc:Volcano;\t\t\t\t\trdfs:label\t&quot;Beerenberg&quot;@en;\t\t\t\t\tp:location\tdbpedia:Norway.\n\nAnswer\n\nSELECT ?volcano rdf:type umbel-sc:Volcano.WHERE &#123;    &#123;?Mount p:location dbpedia:Italy.&#125;    UNION &#123;?Mount p:location dbpedia:Norway.&#125;&#125;\n2.3 Dataset specification\nSPARQL queries are executed over an RDF dataset:\n\nOne default graph and\nZero or more named graphs (identified by an IRI).\n\n\nEvaluation of patterns w.r.t. the active graph (initially the default graph),i.e., the graph used for matching graph patterns;\n\nGRAPH clause is used for making a named graph the active graph.\n\n\n\n\n加了Graph 子句就会在那么graph查询，否则就会在defalt查询\n\n\n\n2.4 Query Forms\nSELECT \nResult: sequence of solutions (i.e., sets of variable bindings); \nSelected variables separated by space (not by comma!);\nAsterisk character (“*”) selects all variables in the pattern.\n\n\n\n\nExercise\nGiven the dataset and the SPARQL query as follows, please write the query results.\nDataset\n\n# Default graph (stored at http://example.org/dft.ttl)@prefix dc: &lt;http://purl.org/dc/elements/1.1/&gt; .&lt;http://example.org/bob&gt; dc:publisher &quot;Bob Hacker&quot; .&lt;http://example.org/alice&gt; dc:publisher &quot;Alice Hacker&quot; .# Named graph: http://example.org/bob@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; ._:a foaf:name &quot;Bob&quot; ._:a foaf:mbox &lt;mailto:bob@oldcorp.example.org&gt; .# Named graph: http://example.org/alice@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; ._:a foaf:name &quot;Alice&quot; ._:a foaf:mbox &lt;mailto:alice@work.example.org&gt; .\n\nSPARQL query\nFROMNAMD可以省略，因为标明了GRAPH ?g\n\n\n\nPREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt;PREFIX dc: &lt;http://purl.org/dc/elements/1.1/&gt;SELECT ?who ?g ?mboxFROM &lt;http://example.org/dft.ttl&gt;FROM NAMED &lt;http://example.org/alice&gt;FROM NAMED &lt;http://example.org/bob&gt;WHERE&#123;?g dc:publisher ?who .GRAPH ?g &#123; ?x foaf:mbox ?mbox &#125;&#125;\n\nAnswer:| who   | g     | mbox || ——— | ————— | ————— || “Bob Hacker” | \\http://example.org/bob | \\&#x62;&#111;&#x62;&#64;&#111;&#108;&#x64;&#x63;&#x6f;&#x72;&#x70;&#46;&#101;&#x78;&#97;&#x6d;&#112;&#108;&#x65;&#x2e;&#x6f;&#114;&#x67;&#92;|| “Alice Hacker” |  \\http://example.org/alice |  |\n\n2.5 DESCRIBE\nResult: an RDF graph (i.e., all RDF triples) that describes the resources found;\nThe DESCRIBE clause can take IRIs to identify the resources.\nThe resources to be described can also be taken from the bindings to a query variable in a result set.\n\n\n\n2.6 CONSTRUCT\nResult: an RDF graph constructed from a template;\n\nTemplate: a graph pattern with the variables from the query pattern.\n\n将模板的变量换掉，其余不变\n\n\n\ndbpedia:Mount_Etna rdfs:label “Etna”;\n​                                       rdf:type myTypes:VolcanosOutsideTheUS.\n\ndbpedia:Beerenberg rdfs:label “Beerenberg”@en;\n​                                       rdf:type myTypes:VolcanosOutsideTheUS.\n\n\n2.7 ASK\nCheck whether there is at least one result;\nResult: true or false.\n\n\n2.8 DELETE/INSERT\nINSERT\nInsert the new RDF triples into the existing RDF graph.\n\n\n\n\n\nDELETE\nDelete some triples in the RDF graph.\n\n\n\n\n\nDELETE/INSERT\nRemove or add triples from/to the Graph Store based on bindings for a query pattern specified in a where clause.\n\n\nWITH 表示数据集\n\n先做查询，在再查询里做插入删除\n\n\n\nExercise\nGiven the dataset and the SPARQL query as follows, please write the query results.\n\nQ1@prefix org: &lt;http://example.com/ns#&gt; ._:a org:employeeName &quot;Alice&quot; ._:a org:employeeId 12345 ._:b org:employeeName &quot;Bob&quot; ._:b org:employeeId 67890 .\nPREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt;PREFIX org: &lt;http://example.com/ns#&gt;DELETE &#123;?person ?p ?o .&#125;where &#123;?person org:employeeId ?id .FILTER (?id &gt; 50000)?person ?p ?o .&#125;\n\n删除后：\n\n_:a org:employeeName &quot;Alice&quot; ._:a org:employeeId 12345 .\n2. Q2@prefix org: &lt;http://example.com/ns#&gt; .# Graph: http://person_:a org:employeeName &quot;Alice&quot; ._:a org:employeeId 12345 ._:b org:employeeName &quot;Bob&quot; ._:b org:employeeId 67890 .# Graph: http://person2_:c org:employeeId 13579\nPREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt;PREFIX org: &lt;http://example.com/ns#&gt;INSERT &#123;GRAPH &lt;http://person2&gt; &#123;?person ?p ?o .&#125;&#125;where &#123;Graph: &lt;http://person&gt;&#123;?person org:employeeId ?id .FILTER (?id &lt; 50000)?person ?p ?o .&#125;&#125;\n@prefix org: &lt;http://example.com/ns#&gt; .# Graph: http://person_:a org:employeeName &quot;Alice&quot; ._:a org:employeeId 12345 ._:b org:employeeName &quot;Bob&quot; ._:b org:employeeId 67890 .# Graph: http://person2_:c org:employeeId 13579 ._:a org:employeeName &quot;Alice&quot; ._:a org:employeeId 12345 .\n2.9 CLEAR\nRemove all the triples in the specified graph(s) in the Graph Store.\n\n\n2.10 MOVE\nMove all data from an input graph into a destination graph.\n\n\n2.11 Solution Modifiers\nOnly for SELECT queries;\n\nModify the result set as a whole (not single solutions);\n\nKeywords: DISTINCT, ORDER BY, LIMIT, OFFSET.\n\nDISTINCT\nRemove duplicates from the result set.\n\n\n\n\nORDER BY\nOrder the results.\n\n\n\nASC for ascending (default) and DESC (e.g., DESC(?name)) for descending.\n\n\nLIMIT\nlimits the number of result:\n只返回5个\n\n\n\n\nOFFSET\nposition/index of the first reported results:\n\n\n\nOrder of the result should be predictable (combine with ORDER BY)\n\nBINDINGS\n\n\nVALUE\nadd data to the query directly.\n增加限制\n\n\n\n\nAGGREGATES\nallows for the grouping of solutions and\n\nthe computation of values over the groups.\n\n\n\n\nGROUP BY groups the solutions; (i.e., students who attend the same lecture)\n\nCOUNT is an aggregate function that counts the solutions within a group; (i.e., number of students in the lecture) \n\nHAVING filters aggregated values\nQuestion: Please use natural language to explain this SPARQL query!\n查选课人数超过5人的课\n\n\n\nNEGATION:\n\n\nQuestion: Please use natural language to explain the above SPARQL queries!\n\n","categories":["KnowledgeEngineering"]},{"title":"Knowledge Graph Alignment","url":"/2021/08/15/knowledge%20engineering/12.%20Knowledge%20Graph%20Alignment/","content":"Knowledge Graph Alignment\n1. Why do we need Knowledge Graph Alignment?\nKnowledge graph construction needs to fuse the data from multiple sources!\n\n\n\nIdentifying the same entity with different descriptions from multiple sources is the key of knowledge graph alignment!\n\n\n\n我们希望把不同源的同一信息进行融合\n\nThe Same Entity in Multiple Sources\n\nKnowledge graph alignment identifies relationships between classes (equivalence, subClassOf, and etc.), properties (equivalence, subPropertyOf, and etc.), and instances (sameAs).\n\n\n\nKnowledge Graph Alignment consists of: \nontology matching (i.e., schema matching), \ninstance matching.\n\n\n\n2. Ontology Matching (本体匹配)：\nIt is the process of finding correspondences (i.e., relationships) between classes (or properties) of different ontologies.\n在不同的本体寻找匹配的类别和属性\n作用在class和property上\n\n\n\n\n2.1 Benefits of Ontology Matching\nCreating global ontologies from local ontologies\n整合全局本体\n\n\nReuse information between ontologies\n全局本体是公认的知识，则不用再重新建\n\n\nDealing with heterogeneity\n处理异构性\n\n\nQueries across multiple distributed resources\n利于在多个分布式条件下做查询\n\n\n\n2.2 Ontology Matching Process\nDefinition (Matching process) The matching process can be seen as a function $f$​​​ which, from a pair of ontologies to match $o$​​​ and $o’$​​​, an input alignment $A$​​​, a set of parameters $p$​​​ and a set of resources $r$​​​, returns an alignment $A$​​​ ‘ between these ontologies:\n\n\nA^{\\prime}=f\\left(o, o^{\\prime}, A, p, r\\right)\n\n\nparameters: weight, threshold… resources: common \nsense knowledge, domain-specific thesauri…\n\n2.3 Ontology Matching Techniques\nElement-level matching techniques\nAnalysing entities or instances in isolation \nIgnoring their relations with other entities or their instances\n不会考虑实体外部的关系，只考虑词之间的内部关系\n\n\nStructure-level techniques\nAnalysing how entities or their instances appear together in a structure (e.g. by representing ontologies as a graph)\n会考虑实体周围的关系，例如$IsA$\n\n\n\n2.4 Element-level Matching Techniques: String-based2.4.1 Prefix\ntakes as input two strings and checks whether the first string starts with the second onenet $=$ network; but also hot $=$​ hotel \n利用前缀进行匹配，如果出现前缀完全等于另一个词，则形成匹配\n\n2.4.2 Suffix\ntakes as input two strings and checks whether the first string ends with the second one\nID = PID; but also word $=$​ sword\n后缀匹配\n\n2.4.3 Edit distance - Levenshtein distance is used here\ntakes as input two strings and calculates the number of edition operations, (e.g., insertions, deletions, substitutions) of characters required to transform one string into another\nnormalized by length of the maximum string\n\n\n\n这里editor修正为$2/7$\n\n2.4.4 N-gram\ntakes as input two strings and calculates the number of common n-grams (i.e., sequences of $n$ characters) between them, normalized by $\\max ($ length $($ string 1$)$, length $($ string 2$))$\nExample:\ntrigrams(nikon) $=\\{$ nik, iko, kon $\\}$\ntrigrams(nike) $=\\{$ nik, ike $\\}$\n$\\operatorname{sim}($ nikon, nike $)=1 / 3$​​\n\n\n\n2.4.5 Exercise\nCompute the trigram based similarity between two strings University and Universe\n\ntrigrams(University)={Uni,niv,ive,ver,ers,rsi,sit,ity}\n\ntrigrams(Universe)={Uni,niv,ive,ver,ers,rse}\n\n$\\text{sim(University,Universe)}=5/8$\n\n\n2.5 Element-level Matching Techniques2.5.1 Language-basedTokenization\n\nparses names into tokens by recognizing punctuation, cases\nHands-Free_Kits $\\rightarrow\\langle$ hands, free, kits $\\rangle$Lemmatization\nanalyses morphologically tokens in order to find all their possible basic formsKits $\\rightarrow$ Kit\n\nElimination\n\ndiscards “empty” tokens that are articles, prepositions, conjunctions, etc.\na, the, by, type of, their, from\n\n2.5.2 Resource-basedWordNet\n\nA $\\sqsubseteq B$ if $A$ is a hyponym of $B$​\nBrand $\\sqsubseteq$ Name$A=B$​ if they are synonyms\n\n\nQuantity $=$​ Amount\n$A \\perp B$ if they are antonyms反义词 or the siblings in the part of hierarchy\nMicroprocessors $\\perp \\mathrm{PC}$​ Board\n\n\n\n2.5.3 Constraint-based\nDatatype comparison\n\ninteger &lt; real\n\n可能是sub的关系\n\n\ndate $\\in[1 / 4 / 200530 / 6 / 2005]&lt;$ date $[$ year $=2005]$$\\{a, c, g, t\\}[1-10]&lt;\\{a, c, g, u, t\\}+$\n\n\n\nMultiplicity comparison$\\left[\\begin{array}{ll}1 &amp; 1\\end{array}\\right]&lt;\\left[\\begin{array}{ll}0 &amp; 10\\end{array}\\right]$​ \nMultiplicity:[minCardinality maxCardinality] of a property\n完全不相交，可能就是不想关\n\n\nCan be turned into a distance by estimating the ratio of domain coverage of each datatype.\n\n如果整数\n\n\n2.6 Structure-level Matching Techniques\nGraph-based Techniques: \nconsider the input ontologies as labeled graphs; \nif two nodes from two ontologies are similar, their neighbors may also be somehow similar (similar subclasses, superclasses, and properties).\n\n\nTaxonomy-based Techniques: \nare also graph algorithms which consider only is-a relations between classes; \nis-a links connect terms that are already similar, therefore their neighbors may be somehow similar.\n\n\nModel-based Techniques \nhandle the input ontologies based on its semantic interpretation (e.g, model-theoretic semantics); \nif two classes (or properties) are the same, then they share the same interpretation.\n\n\nInstance-based Techniques: \ncompare sets of instances of classes to decide if these classes match or not (i.e., exist equivalence or subClassOf relations).\n用实例集合去比较，如果完全相同则就是equivalence\n\n\n\n2.6.1 Exercise\nCompute the Jaccard similarity between the first-order neighbor classes of the classes Car and Automobile from different ontologies.\n\n\n\n\\begin{array}{l}\nA\\cup B=\\{\\text{Vehicle},\\text{Jeep},\\text{Truck},\\text{Van}\\}\\\\\nA\\cap B=\\{\\text{Vehicle},\\text{Truck}\\}\\\\\n\\text{Jaccard}(\\text{Car},\\text{Automobile})=\\frac{|A\\cap B|}{|A\\cup B|}=\\frac{1}{2}\n\\end{array}2.7 Matcher Composition\nSequential composition of matchers\n\n\n\nProblem: error accumulation and low coverage\n第一个正确率0.95，那么一直乘会越来越小1\n\n\n\n2.8 Parallel composition of matchers\n\n可以得到结果后进行投票或者只是得到特征再送进机器学习模型\ne.g.A single similarity measure composed by the similarity obtained from their names, the similarity of their superclasses, the similarity of their instances and that of their properties\n\n\n\n3. A Real-World Case: Book Ontology Matching\nGiven two ontologies $O_{1}$​ and $O_{2}$​, generate candidate matched classes by pairing any two classes from the two ontologies. A pair of candidate matched classes is denoted as $\\left(C_{1 \\mathrm{k}}, C_{2 \\mathrm{P}}\\right)$​, and note that $\\left(C_{2 \\mathrm{p}}, C_{1 \\mathrm{k}}\\right)$​​​​ is a different pair since we need to measure the asymmetric similarities between classes.\n寻找非对称相似度\n\n\n\n3.1 String Similarity\nC L \\operatorname{sim}\\left(C_{1 k}, C_{2 p}\\right)=\\frac{\\operatorname{LCS}\\left(I\\left(C_{1 k}\\right), I\\left(C_{2 p}\\right)\\right)}{\\left|I\\left(C_{1 k}\\right)\\right|}\nwhere LCS means the length of the longest common substring, $l(\\cdot)$​ returns the label of the class, and $|\\cdot|$​ returns the length of the input label.\n\n\n3.2 Neighbor Class Set Similarity\n\\operatorname{NCSsim}\\left(C_{1 k}, C_{2 p}\\right)=\\frac{\\left|N C S\\left(C_{1 k}\\right) \\cap N C S\\left(C_{2 p}\\right)\\right|}{\\left|N C S\\left(C_{1 k}\\right)\\right|}\nwhere $\\left|\\mathrm{NCS}\\left(C_{1 \\mathrm{k}}\\right) \\cap \\mathrm{NCS}\\left(C_{2 \\mathrm{p}}\\right)\\right|$ is the size of the intersection of $\\mathrm{NCS}\\left(C_{1 \\mathrm{k}}\\right)$ and $\\operatorname{NCS}\\left(C_{2 \\mathrm{p}}\\right), N C S(\\cdot)$ returns the set of first-order neighbor classes in the given ontology.\n\n\n3.3 Neighbor Class Set Similarity\nWe submit the label l(C) of a snippets as the textual context;\n先把本体扔到搜索引擎得到他的长文本\n\n\nIn the top-k returned snippets of Web pages, the words co-occurred with l(C) in the same sentence are extracted;\n把搜索中的topk个abstract搜集，然后抽取共现词\n\n\nAfter removing the stopwords and the words with low frequency (e.g., less than 3), TF-IDF is adopted for weighting each word $u$​ :\n对于所有共现词可以计算tf-idf\n即合并所有的abstract可以计算tf，以及计算document的freq\n\n\n\n\nw_{u}=t f_{u} \\cdot i d f_{u}\nTextual Context Similarity.\nThe textual context vector representation of a class $C$ is denoted as: $\\mathrm{TC}(\\mathrm{C})=&lt;\\mathrm{w}_{1}(\\mathrm{C}), \\mathrm{w}_{2}(\\mathrm{C}), \\ldots, \\mathrm{w}_{\\mathrm{n}}(\\mathrm{C})&gt;$, and $n$ is the number of all words.\nThe textual context similarity between classes $C_{1 \\mathrm{k}}$ and $C_{2 \\mathrm{p}}$​ is computed as:\n内积除以模\n\n\n\n\n\n\nT C \\operatorname{sim}\\left(C_{1 k}, C_{2 p}\\right)=\\frac{\\sum_{v=1}^{n} T C\\left(C_{1 k}\\right)_{v} \\cdot T C\\left(C_{2 p}\\right)_{v}}{\\sum_{v=1}^{n} T C\\left(C_{1 k}\\right)_{v}{ }^{2}}3.4 Instance Set Similarity\n\\operatorname{ISSim}\\left(C_{1 k}, C_{2 p}\\right)=\\frac{\\left.\\mid \\operatorname{IS(C}_{1 k}\\right) \\cap \\operatorname{IS}\\left(C_{2 p}\\right) \\mid}{\\left.\\mid \\operatorname{IS(C}_{1 k}\\right) \\mid}\nwhere $I S(\\cdot)$​ returns the instance set of the class, and we can identify the same book instances using the ISBN number in the book domain.\n\n\n3.5 Exercise3.5.1\nGiven two ontologies as follows, compute the String Similarity, Neighbor Class Set Similarity, Instance Set Similarity (introduced in the real-world case: book ontology matching) on the class pairs (book, Textbook) and (Textbook, book), respectively.\n\n\n\nStringSimilarity\nlen(book)=4,len(Textbook)=8\nsubstring(book, Textbook)=book\nlen(substring)=4\nstrIngSimilarity(book, Textbook)=4/4=1\nstrIngSimilarity(Textbook, book)=4/8=1/2\n\n\nNeighbor Class Set Similarity\nNeighbor(book)={literature,volume}\nNeighbor(Textbook)={volume,Engineering,Science}\nNeighbor Class Set Similarity(book, Textbook)=1/2\nNeighbor Class Set Similarity(Textbook, book)=1/3\n\n\nInstance Set Similarity\nInstance(book)={Red Sorghum,Introduction to Algorithm}\nInstance(Textbook)={Red Sorghum}\nInstance Set Similarity(book, Textbook)=1/2\nInstance Set Similarity(Textbook, book)=1\nGiven two ontologies $O_{1}$​ and $O_{2}$​, generate candidate matched classes by pairing any two classes from the two ontologies. A pair of candidate matched classes is denoted as $\\left(C_{1 \\mathrm{k}}, C_{2 \\mathrm{P}}\\right)$​, and note that $\\left(C_{2 \\mathrm{p}}, C_{1 \\mathrm{k}}\\right)$​ is a different pair since we need to measure the asymmetric similarities between classes.\n\n\n\n3.5.2\nQuestion: now we have String Similarity, Neighbor Class Set Similarity, Textual Context Similarity, and Instance Set Similarity, please tell which one belongs to the element-level matching techniques? Which one is a structure-level matching technique?\nString Similarity,Textual Context Similarity element-level matching techniques\nNeighbor Class Set Similarity, Instance Set Similarity structure-level matching technique\n\n\n\n3.6 Aggregate these similarities:self-training\nAggregate these similarities by a semi-supervised learning strategy: self-training, for binary classification on subClassOf relations: \n\nIn each iteration, self-training accepts the labeled data as training data and learns a classifier. \nThen the classifier is applied to the unlabeled data and adds class pairs of high confidence to the labeled data to train a new classifier for the next iteration. \nThe whole process will terminate if the difference between the predicted labels of the class pairs given by classifiers in the two consecutive iterations is smaller than a threshold or the maximal number of iterations is achieved.\n用少量数据训练得弱分类器，从而预测无标签数据，得到高置信度得数据之后再继续训练，终止条件：到达指定epoch或者标签数据不再改变\n\n\nThe binary classifier can use SVM, Random Forest, Neural Networks, and etc.\n\n传统方法SVM，Random Forest的效果最好\n\n\nIn each iteration, rules are applied to filter out misclassified relations.\n\n每次迭代过程用设定的规则过滤掉错误分类\n\n\nRULE 1:\n\nGiven two book classes $C_{1 \\mathrm{k}}$​ and $C_{2 \\mathrm{p}}$​, if the label string $l\\left(C_{1 \\mathrm{k}}\\right)$​ is the suffix of the label string $l\\left(C_{2 \\mathrm{p}}\\right)$​, and $l\\left(C_{2 \\mathrm{p}}\\right)$​ does not contain “与”, “和”, and “\\&amp;”, then $C_{2 \\mathrm{p}}$​ is the subclass of $C_{1 \\mathrm{k}}$​​​.\nExample: 企业管理 subClassOf 管理\n如果机器学习误分类了即给了很低的置信度，那么我们可以使用这条规则进行修正\n\n\nRULE 2;\n\nGiven two book classes $C_{1 \\mathrm{k}}$ and $C_{2 \\mathrm{p}}$, if the label string $l\\left(C_{2 \\mathrm{p}}\\right)$ contains “与” or “和” or “\\&amp;”, then using these symbols as separators to segment the label string $l\\left(C_{2 \\mathrm{p}}\\right)$. If one of the segmented strings and $l\\left(C_{1 \\mathrm{k}}\\right)$ are the same, then $C_{1 \\mathrm{k}}$ is the subclass of $C_{2 \\mathrm{p}}$​​​.\n先分割再看包含关系\nExample: 计算机 subClassOf 计算机与互联网\n\n\n\nWith generated subClassOf relations, how to get equivalent classes?\n\nA=B -: A subclassOf B and B subclassOf A\n\n3.7 OAEI\n\n\nKnowledge Graph Alignment consists of: \n\nontology matching (i.e., schema matching), \ninstance matching.\n\n\nInstance Matching (实例匹配)：It is the process of finding different instances of the same real-world objects.\n\n\n\n\n\n平均四个实体有4个url\n\n4. Instance Matching with Knowledge Graph Embedding\nEmbedding maps discrete variables to continuous vector representations;\nEmbedding learning techniques has achieved great progress in CV, NLP, Speech Recognition, and etc.;\nKnowledge Graph Embedding aims to map entities and relations to continuous vector representations.\n\n\n\nConventional approaches are challenged by the symbolic, linguistic and schematic heterogeneity of independently-created KGs\nEmbedding-based approaches measure entity similarities based on entity embeddings\nThree key modules\nKG embedding\nAlignment inference\nHow they interact\n\n\n\n\n\n\n\nKnowledge graph embedding: TransE\n\n\n\n训练多个Transe，来做三元组对齐\n\nCorpora: (partially-aligned) multilingual KGs\n\nEnabling: inferable embeddings of multilingual semantics\n\nCan be applied to:\n\nKnowledge alignment\nCross-lingual Q&amp;A\nMultilingual chat-bots\n\n\n\n\n4.1 MTransE\n\nKnowledge model和Transe一样\nAlignment model为了缩小需要匹配的不同语言的三元组\n\n4.2 Different alignment techniques\n\n5. Instance Matching with Rules\nThe Same Entity in Multiple Sources\n\n\n\n\nseeds是预先得到的一些标签数据\n\nAutomatically discovering and refining dataset-specific matching rules in iterations\n\nDeriving these rules by finding the most discriminative datacharacteristics for a given data source pair.\n\n\n\n5.1 Seeds - Lightweight Entity Matching\nPunctuation Cleaning:\nSpace Shuttle Endeavour $\\approx$​ Space Shuttle “Endeavour”\n去掉标点后，如果字符串完全相同，则可以认为这两个实体是完全匹配的\n\n\n\n\n\nRedirects Information:\n\n重定向：百度百科会自动将网页重定向到另一个页面，该页面的标题是重定向信息\nA redirects to B means A and B are synonyms\n\n\nMining Properties Equivalences\n\n当我们已知两个实体等价，而这两个实体都有多个属性值对，这样我们就可以做属性匹配\n\n\n\n\n\nFor each pair of existing matched instances, their property-value pairs are merged.\n在一对实例可以找到很多这样的属性对\n\n\n\n\n\nMatching rule (frequent set mining):\n\n算法：Association Rule Mining 把频繁出现的属性值对放在一起，作为实例等价规则，如以下的例子\nbaidu:x and hudong:x are matched, iff.\nvalueOf(baidu:标签) = valueOf(hudong:中文学名)\nand\nvalueOf(baidu:拉丁学名) = valueOf(hudong:二名法)\nand\nvalueOf(baidu:纲) = valueOf(hudong:纲)\n\n\nThe Wrapper Algorithm\n\n一开始用轻量级算法，从而挖掘出规则\n然后再通过规则进行实例匹配，不断迭代\n\n\n\n\n\nThe wrapper is an implementation of Expectation-Maximization iterations.\n这里使用EM算法，可以这么理解，这里的匹配就是我们要的参数（未知），而规则就是我们要预测的分布\nThe E-step\n估计missing data\nThe E-step estimates the missing data (matches) using the observed data and the current estimate for the parameters (matching rules).\n\n\nThe M-step\nThe M-step computes parameters maximizing the likelihood function as the data estimated in E-step are used in lieu of the actual missing data.\nM: matches\n$\\Theta$: parameters\n\n\nL(\\theta ; M)=\\operatorname{Pr}(M \\mid \\theta)\n给定当前规则的情况下，实例匹配出现的概率\n\nThe Likelihood Function\n\nL(\\theta ; M) \\approx \\frac{\\mid \\text { ConnectedComponent }(M) \\mid}{|\\operatorname{Edge}(M)|}\nAssuming that no equivalent instances exist in a single data source, we can infer that an instance is equivalent to at most one another from the other data source.\nIncorrect matches in $M$ may result in a node connecting to more than one other node, which is contrary to the assumption.\n\n\n\n如果L高，那么规则的置信度就高，那么我们就可以把这个规则加入到规则集\n\nPrecisions\n\n\n\n\nSampling a certain number of output matches.\n\nThe X-axis indicates the proportions of selected seeds in complete reference matches.\n\n\nQuestionWhat are the advantages and disadvantages of the embedding-based method and rule-based method for instance matching?\n\n如果数据质量不高，embedding对噪声的容忍性高\n如果数据质量高，那么用规则的方法就会带来更高的准确率\n\n","categories":["KnowledgeEngineering"]},{"title":"review1","url":"/2021/08/15/knowledge%20engineering/8.%20Review/","content":"review1\n1. XML Namespaces1.1 Naming conflicts exist in different XML documentsXML document_1&lt;table&gt;&lt;tr&gt;\t&lt;td&gt;Apples&lt;/td&gt;    &lt;td&gt;Bananas&lt;/td&gt;&lt;/ tr&gt;&lt;/table&gt;\nXML document_2&lt;table&gt;\t&lt;name&gt;African Coffee Table&lt;/name&gt;    &lt;width&gt;8G&lt; /width&gt;&lt;length&gt;120&lt;/length&gt;&lt; /table&gt;\n1.2 Solution : Adding prefixes as namespacesXML document_1&lt;h:table&gt;\t&lt;h:tr&gt;\t\t&lt;h:td&gt;Apples&lt;/h:td&gt;        &lt;h:td&gt;Bananas&lt;/h:td&gt;    &lt;/h:tr&gt;&lt;/h:table&gt;\nXML document_2&lt; f:table&gt;\t&lt;f:name &gt;African Coffee lable&lt;/f:name&gt;    &lt;f:width&gt;80&lt;/f:width&gt;\t&lt;f:length&gt;120&lt;/f:length&gt;&lt;/f:table&gt;\n\n1.3 XML Namespace Syntax:\n\nURI 英文字符   IRI任何编码\n\n2. RDF\nthe data model of Semantic Technologies and of the Semantic Web;\n\nstructures metadata about Web sites, pages, etc.:\n\nPage author, creator, publisher, editor,…\n\nData about them: email,phone, job,…\n\n\n\n\n\ncan be used for machine-readable data exchange;\n\nis introduced in W3C Recommendation of 1999 (Version 1.O);- uses XML as main syntax for serialization.\n\n\n\n\nARDF knowledge base: a directed labeled graph, i.e., Knowledge Graph\n\n\n2.1 RDF Triple (Statement)\n\nSubjects: Resource or blank node\nPredicates: Resource\nObject: Resource, literal or blank node\n\nLiterals are:\n\ndata values;\nencoded as strings;\ninterpreted by datatypes;\ntreated the same as strings without datatypes, called plain literal;\nA plain literal may have a language tag;\nDatatypes are not defined by RDF, but usually from XML Schema.\n\n\n\nTyped Literals:\n“Beantown3’^^xsd:string“\nThe Bay State”^^xsd:stringPlain literal and literals with language tags:\n“France”‘France”@en\n “France”‘@fr“法国”@zh“\n“Frankreich”@de\nEqualities for Literals:“0013^^xsd:integer =“13”^^xsd:integer“123.0^^xsd:decimal= “00123”1^^xsd:integer (based on data type hierarchy)\nExercise\nDoes the datatype“德国”equals to“德国”@ zh ?\n\n不一样\n\nBlank node: unnamed resource or complex node\n\nThe set of blank nodes, the set of all lRls (named resources)and the set of all literals are pairwise disjoint;\nRepresentation of blank nodes is syntax-dependent:underline+colon+ID (Turtle syntax):_:xyz,_:bn;\nThe scope of the lD of a blank node is only the document towhich it belongs.\n\n2.2 RDF: N-ary RelationsExample:\nprefix ex: &lt;http://lexample.org/&gt; .ex: Chutneyex:hasIngredient &quot;1lb green mango&quot;,\t\t\t\t\t\t&quot;1tsp. Cayenne pepper&quot; .\n\n@prefix ex :&lt;http://lexample.orgl&gt; .@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .ex:Chxtney ex:hasIngredient ex:ingredient1 . ex:ingredient1 rdf:value ex:greenMango;\t\t\t   ex:amount  &quot;11b&quot; .\n\n&lt;rdf:Description rdf:about=&quot;http://example.org/Chutney&quot;&gt;\t&lt;ex:hasIngredignt rdf:nodeID=&quot;id1&quot;/&gt;&lt;/rdf:Description&gt;&lt;rdf:Description rdf:nodeID=&quot;id1&quot;&gt;\t&lt;ex:ingredient rdf:resource=&quot;http://example.org/greenMango&quot;/&gt;\t&lt;ex:amount&gt;1lb&lt;/ex:amount&gt;&lt;/rdf:Description&gt;\n&lt;rdf:Description rdf:about=&quot;http://example.org/Chutney&quot;&gt;\t&lt;ex:hasIngredient rdf:parseType=&quot;Resource&quot;&gt;\t\t&lt;ex:ingredient rdf:resource=&quot;http://example.org/greenMango&quot;/&gt;        &lt;ex:amount&gt;1lb&lt;/ex:amount&gt;\t&lt;/ex:hasIngredient&gt;&lt;/rdf:Description&gt;\n\nsw:John sw:is_a sw:professors ;\t\tsw:has_name &quot;John Doe”;\t\tsw:has_id “987654321&quot;.sw:John sw:is_a sw:professors .sw:John sw:has_name “John Doe”.sw:John sw:has_id 987654321.\n\n\n\n\n3. RDFS\nprovides a data-modeling vocabulary for RDF data;\nis an extension of the basic RDF vocabulary;\ntries to provide consistent explanations for RDF data;- allows for specifying schema knowledge;\nMothers are female\nOnly persons write books\nis a part of the W3C Recommendation.\n\n\nWhy do we need RDFS when there already exists XML Schema?\nXML Schema only defines syntax without semantics;\nXML Schema cannot reference “things”outside the document.\n\n\n\n3.1 RDFS: Class and InstanceGiven a triple:\nex:SemanticWeb rdf:type ex:Textbook .\nwhich characterizes“Foundations of Semantic Web Technologies”as aninstance of the class “Textbook”.\n\nAresource can be the member of more than one class\n\nex:SemanticWeb rdf:type ex:Book .\n\nlnstance and class names cannot be distinguished syntactically with lRls.. \n\nRDFS helps explicitly state that a resource denotes a class:\nex:book rdf:type rdfs:Class .\n\nrdfs:Class is the“class of all classes” .\n\n\n3.2 Class Hierarchy (Taxonomy)\n3.3 Property and Property Hierarchy\nAllow to state that a certain property can only be between things of a certain class:\ne.g. when a is married to b, then both a and b are Persons\n\n\n\nex:isMarriedTo rdfs:domain ex:Person.ex:isMarriedTo rdfs:range ex:Person\n\nDatatypes can also be used for adding property restrictions:\n\nex:hasAge rdfs:range xsd:nonNegativeInteger\n3.4 RDFS: Reification\nHow to graphically represent a sentence by means of the blank node?\n\n\n\n3.5 Example: Reasoning with RDFS\n4. Constituents of a DL Knowledge Base\n4.1 Description Logic: ALC\nALC: the simplest DL\n\n\n\ncomplex concepts are defined as follows: \n\n\n⊥ and T are concepts； \n\nFor concepts C and D, ¬C, C⊓D, and C⊔D are concepts; \n\nFor a role r and a concept C, ∃r.C and ∀r.C are concepts \n\n\n\nExample: Student ⊓ ∀attendsCourse.MasterCourse\nIt describes the concept comprising : all students that attend only master courses.\n\n\n\n4.2 Example: Are the following valid DL concepts\n𝐴∀𝑅.𝐵 \n(𝐴⨅𝐵)⨆𝐶\n(∃𝑅.𝐴)⨅𝐵\n¬𝐴⨆𝐶¬\n\n\n×√√×\n\n4.2.1 TBox:\nFor concepts C, D, a general concept inclusion (GCI) axiom has the form: C ⊑ D \nC≡D is an abbreviation for C ⊑ D and D ⊑ C. \na TBox (terminological Box) consists of a set of GCIs.\n\n\n\n\n4.2.2 ABox:\nan ALC ABox assertion can be of one of the following forms: \nC(a), called concept assertion \nr(a, b), called role assertion - an ABox consists of a set of ABox assertions\n\n\n\n(1) Any student must attend at least one lecture.        Student ⊑ ∃Attend.Lecture\nStudents are those who attend at least one lecture?\n\n不对的，因为这个are表示是完全相等的关系，但是后面那个concept的范围更大\n\n(2) John is a brother of Mary.        Brother(John,Mary)(3) Parents are exactly those who are mothers or fathers.        Parent ≡ Mother ⊔ Father\n\n\n\n\n\nA set of axioms (knowledge base) is satisfiable (or consistent) if it has a model.\nIt is unsatisfiable (inconsistent) if it does not have a model.\nInconsistency is often caused by modeling errors.\nUnicorn ⊑ Fictitious \nUnicorn ⊑ Animal \nAnimal ⊑¬Fictitious\n\n\n\n5. Deductive Reasoning: Backward Reasoning\nBackward reasoning (Backward chaining) is an inference method described colloquially as working backward from the goal.\nIt is often used in entailment checking or query answering in KG.\nIt uses the rules to rewrite the query in several ways and the initial query is entailed if a rewritten query maps to the initial facts.\n\nExample1:Rule: If you are human, then you are mortal. \nHuman(x)→Mortal(x)Question: Is Socrates a mortal? \nSolution: Check whether Mortal(Scorates) or Human(Scorates) is true.\n\n\n5.1 Other Tasks in Logical Reasoning: Inconsistency CheckingIncoherent ontology: ontology with at least one unsatisfiable concept. Example: PhDStudent ⊑ Student, PhDStudent ⊑ Employee, Student ⊑EmployeeInconsistent ontology: ontology without a model. Example: PhDStudent ⊑ Student, PhDStudent ⊑ Employee, Student ⊑Employee, PhDStudent(John)\n\n6. TransE: Take Relation as Translation\nFor a fact (head, relation, tail), take the relation as a translation operator from the head to the tail .\n\n\n\nTransE \nFor each triple , h is translated to t by r.\n\n\n\n\n\n\nQuestion\n\n\n7. RDB2RDFDirect Mapping\n\n\n7.1 RDB2RDF: R2RML\nR2RML is a language for specifying mappings from relational to RDF data.\nA mapping takes as input a logical table, i.e.,\na database table\na database view (a virtual table collecting data from relational tables), or\nan SQL query (called an “R2RML view” because it is like an SQL view but does not modify the database)\n\n\n\nA triples map has three parts: - the input logical table \na subject map \nseveral predicate-object maps (combining predicate and object maps).\n\n\n\n\n\n\n\n\n\n\n8. Exercise\nPlease represent the following sentences graphically by means of blank nodes.Jonathon says cherries taste sweet.1月14日，国家药监局宣布酚酞片停止生产.\n\n\n\nWhat can be inferred from the following triples using RDFS semantics?\nex:Undergraduate_student rdfs:subClassOf ex:Student .ex:Postgraduate_student rdfs:subClassOf ex:Student .ex:Professor rdfs:subClassOf ex: Academic_staff .ex:Academic_staff rdfs:subClassOf ex:University_staff .ex:Teach rdfs:domain ex:Academic_staff .ex:Teach rdfs:range ex:Student .ex:Supervise rdfs:domain ex:Professor .ex:Supervise rdfs:range ex:Postgraduate_student .ex:John ex:Supervise ex:Mary .\nex:Professor rdfs:subClassOf ex: University_staff .ex:John rdf:type ex:Professor .ex:Mary rdf:type ex:Postgraudate_student .ex:John rdf:type ex:Academic_staff .ex:John rdf:type ex:University_staff .ex:Mary rdf:type ex:Student .\n\nExpress the following sentences in Description Logics.\n\n\nLi is a student of SEU.Any person who is enrolled in a university is a college student.\n\n\nAre the following valid OWL axioms? If not, why? (class: A,B,C,D, role: R)\n\n\nA⨅(B⨆ ∃R)\n\n(∀R.A⨆B)⨅A\n\n(A⨅B)¬C⨆D\n\n\n\n5) Please write the corresponding RDF triples (in Turtle) of the following sentence: “Grandpas are the men who are grandparents”. Specific classes are defined in the namespacehttp://semanticweb.org/. Prefixes are given as follows:@prefix sw: http://semanticweb.org/ .@prefix rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# .@prefix owl: http://www.w3.org/2002/07/owl# .\n_:x  rdf:type  owl:class  ;     owl:intersectionOf  (sw:Man  sw:Grandparent)  ;     owl:equivalentClass  sw:Grandpa .\n\nGive a model of the following ontology\n\n\nProfessor⊑FacultyMember,\n\nProfessor(Guilin Qi),\n\nhasAffiliation(Guilin Qi, SEU),\n\nStudentOf(Zhang, Guilin Qi)\n\n\nA model: \n\n∆={a,b,c}\n\nI(Guilin Qi)=a, I(SEU)=b, I(Zhang)=c\n\nI( Professor)={a}, I(FacultyMember)={a},\n\nI(hasAffiliation)={(a,b)},\n\nI(StudentOf)={(c,a)},\n\n\n\nGiven the following KG, use query rewriting on the query Person(x) (this query is used for checking whether x is a person) .\n\n\n\nGiven a relational database and RDF triples, please write corresponding R2RML triples maps which can map the given RDB to RDF.\n\n\n&lt;http://data.example.com/student/001&gt; rdf:type ex:Student ;                                                                ex:id &quot;001&quot; ;                                                                ex:name &quot;Wang&quot; ;                                                                ex:major &lt;http://data.example.com/major/211&gt; .&lt;http://data.example.com/student/002&gt; rdf:type ex:Student ;                                                                ex:id &quot;002&quot; ;                                                                ex:name &quot;Wu&quot; .&lt;http://data.example.com/major/211&gt; rdf:type ex:Major ;                                                             ex:name &quot;AI&quot; .\n","categories":["KnowledgeEngineering"]},{"title":"Knowledge Graph Construction","url":"/2021/08/15/knowledge%20engineering/7.%20Knowledge%20Graph%20Construction/","content":"Knowledge Graph Construction\nKnowledge Graph Construction1. Previous Exercises1.Every teacher must teach someone\n\nCorrect Answer: Teacher⊑∃𝑇each.Human\n\n\n\nEvery finger is a bodypart and is a part-of hand.\n\n\nFinger ⊑ BodyPart ⨅ ∃Part_of.Hand\n\n\n\nZhang is a teacher of SEU\n\n\nTeacher(Zhang, SEU)\n\n\n2.Give a model of the following ontology:\n\nPhDstudent ⊔ Undergraduatgestudent ⊑ Student,\n\nPhDstudent(John),\n\nUndergraduatgestudent(Jack),\n\nSister(Lisa,Jack),\n\nEmployee(Lisa)\n\nCorrect Answer:\n\nA model: Δ={ jo,l,ja}\nI(John)= jo, I(Lisa)=1, I(Jack)= ja\nI( PhDstudent)={ jo},\nI(Employee)={I},\nI( Undergraduatgestudent)={ja},\nI(Student)={ ja.jo}\nI(Sister)={(l,ja)}\n\n3.Write the inferred axioms using description logics after conducting classification in forward reasoning on the following axioms.:\n\nEndocarditis ⊑ Heart_Disease \n\nMiocardial_Infarction ⊑ Heart_Disease \n\nHeart_Disease ⊑ Disease \n\nEnterococcal_Endocarditis ⊑ Endocarditis\nCorrect Answer:\n\nEndocarditis ⊑ Disease \n\nMiocardial_Infarction ⊑ Disease \nEnterococcal_Endocarditis ⊑ Heart_Disease \nEnterococcal_Endocarditis ⊑ Disease\n\n2. Knowledge Graph Construction\nKnowledge Graph Construction: Extracting knowledge from heterogeneous data sources to form a knowledge graph.\n\n\n\n包装器，自动把半结构化数据爬取出来\n\n2.1 Knowledge Graph Construction from Structured Data\nBasics of Relational Database\nRDB2RDF: Direct Mapping &amp; R2RML\nTriple Extraction from Relational Web Tables\n\n3. Basics of Relational Databases3.1 Structuring data\nWe all structure the information we work with:\n\nSo we can find what we need, when we need it\nTo facilitate（促进） evaluation, comparison, and analysis\n\n\nThe structure you select influences\n\nThe kinds of information you collect\nHow it is possible to interrogate(查询) your data\nThe extent to which you can take advantage of your computer’s data-handling abilities\nHow easy it is to share data with others\n\n\nOptions for structuring &amp; analyzing data\n\n\n\n\nA table of bibliographic(著书目录的) data (not a table in relational database )\ntable会对数据重复存储\n\n\n\n\n\n3.2 An alternative approach\n先做一个作者表，通过映射关系，将一些信息分开表示\n\n\n\n\n再分表，把出版信息分开\n\n\n\n限定好Type的种类，使得可以与另一张表有效定位\n\n\n\nTo solve the above problems, we can design a relational database to store data\n\n3.3 relational database\nDatabase terms:\nA database is a collection of data\nData is organized into one or more tables\nEach row is a record\nEach column is a field\n\n\n\n\n\nDeciding on Fields\n\nPrinciples of designing database terms:\nThink of all the facts that will be collected 考虑所有情况\nplenty of fields 考虑所有column\nconsult widely 共识\nsmall facts, “atomic” 力度要比较细，希望信息尽量清晰，比如学生姓名而不是学生信息\ndifficult to add later 再加一个字段特别困难\n\n\n\n\nSet data types\n\n\n\n3.4 Example：\nAn example of designing a relational database:\n\nStudy of 18th century book trade \nWhat things are we interested in?\nPublications\nPublishers\nPeople\n\n\n\n\nAnd what information might we want to know about each of these things?\n\nNames\nDates\nPlaces\n\n\nDesign three tables at first:\n\n\n\n\nJoins between tables: Primary Key 唯一标识\n\nEach table needs a primary key 意味着id，要显式定义\nChoose (at least) one field that only contains unique values Commonly an auto-incrementing whole (integer) number\n\n\nJoins between tables: relate two tables by primary keys and foreign keys\n\n一个publisher可能对应多个publication，就是一对多，可以设置谁是谁的外键\n\n\n\n多个人可能参与写同一本书，一个person可能写很多本书，所以person和publication是多对多的。无法对多对多的关系建立计算机可识别的映射关系，所以需要新建一张表，这张表记录了所有Author与Publication的记录\n\n那么就可以隐式表达Person和Publication的关系\n\n\n\n3.5 Database design: workflow\nChoose fields\nAre they atomic?\nAre there plenty?\n\n\nGive each field a data type\nAre they consistent?\n\n\nArrange the fields into tables\nDo all the fields in the same table describe the same item?\n\n\nSet primary key fields\nA different primary key for each table?\nis this a field with no duplicate values?\n\n\nDraw relationships between tables\n\nWhich field relates each pair of tables?\nMark 1-to-many, many-to-many,1-to-1 relationships\n\n\nReview, reflect, challenge\nTalk through the design with someone else\n\n\n\n3.6 Once you’ve created your database\nAsk questions by constructing queries\nFind the records that meet certain criteria\nSearch, sort, count, and filter data\nPerform basic mathematical and statistical operations Export data for other types of analysis \n\n\nExport data for other types of analysis\n\nQuery example1: \n\nselect id, cityname, country, population, longtitude, latitude from City\n\nQuery Results\n\n\n\nQuery example2: \n\nselect id, cityname, country, population, longtitude, latitude from City where cityname=‘Tirane’\n\nQuery Results\n\n\nResults may resemble another table or spreadsheet\nBut the contents are customized(定制) to your requirements\n\n3.7 When to use a relational database\nYour data can be organized in tabular form\n\ne.g., information about things that share common properties (organized in one column field) \n\n\nYou are interested in multiple types of entity . \n\nAnd the relationships between them\nEntities may be concrete(具体的) or more abstract \n\n\nYou want to identify instances of things that meet certain criteria (query) \n\nYou want to be able to present one dataset in multiple different waysQuery results can be exported and used elsewhere\n\n\n3.8 Benefits of relational databases\nMore accurate representation of complex data\n\nAnd helps avoid duplication of information \n\n\nPermits flexible querying\n\nWider range of questions possible than with a spreadsheet (multiple tables)\nUseful if you are unsure which questions you will want to ask \n\n\nSuitable for collaborative use\n\nMultiple people can access and use the same database\nCan encourage (or enforce) consistency in data entry \n\n\nTechnology has been around for several decades\n\nWidely supported and well understood\n\n\n\n4. RDB2RDF: Direct Mapping4.1 What is RDB2RDF?\n4.2 Two W3C RDB2RDF Standards:\nDirect Mapping\nR2RML\n\nTools: \n\nFree: D2R, Virtuoso, Morph, r2rml4net, db2triples, ultrawrap, Quest; \nCommercial: Virtuoso, ultrawrap, Oracle SW.\n\n4.3 w3C RDB2RDF Standards\nStandards to map relational data to RDF\nA Direct Mapping of Relational Data to RDF\nDefault automatic mapping of relational data to RDF\n\n\nR2RML: RDB to RDF Mapping Language\nCustomizable language to map relational data to RDF\n\n\n\n\n\n4.4 Create URIs following some simple rules:\nMap\ntable to class （对应turtle语言type）\ncolumn to property (属性-&gt;谓词)\nrow to resource （一条记录）\ncell to literal value （turtle中的实值）\nin addition cell to URI \nif there is a foreign key constraint\n\n\n\n\nWe need IRIs for identifying\nthe resource class corresponding to a table\nthe resources represented by the table rows \nthe properties of the resources corresponding to table cells\nthe references due to foreign keys\n\n\nBase IRI \n\nfor the whole graph/dataset, \ne.g. @base http://foo.example/DB/ .\n\n\nTable name $\\rightarrow$​​ Class name, \ne.g. People $\\rightarrow$​​​ \\ 表示类别\n\n\nRow with PK $\\rightarrow$​​ Resource with PK, \ne.g,  这个表示一个instance，就是一个实例，\\\n\n\nTable row $\\rightarrow$​ Property, \ne.g.,    （\\）\n\n\nTable cells: what if NULL? 直接省略\nForeign key reference $\\rightarrow$​​​​ additional property, e.g.,  (\\)\n\n\nProvide a base IRI http://foo.example/DB/ !\n\n\n@base &lt;http://foo.example/DB/&gt; . @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;. &lt;People/ID=7&gt; rdf:type &lt;People&gt; . #某条记录的类别&lt;People/ID=7&gt; &lt;People#ID&gt; &quot;7&quot; .  #具体属性&lt;People/ID=7&gt; &lt;People#fname&gt; &quot;Bob&quot; . &lt;People/ID=7&gt; &lt;People#addr&gt; &quot;18&quot; . &lt;People/ID=7&gt; &lt;People#ref-addr&gt; &lt;Addresses/ID=18&gt; . #利用外键关联映射两张表的记录&lt;People/ID=8&gt; rdf:type &lt;People&gt; . &lt;People/ID=8&gt; &lt;People#ID&gt; &quot;8&quot; . &lt;People/ID=8&gt; &lt;People#fname&gt; &quot;Sue&quot; . &lt;Addresses/ID=18&gt; rdf:type &lt;Addresses&gt; . &lt;Addresses/ID=18&gt; &lt;Addresses#ID&gt; &quot;18&quot; . &lt;Addresses/ID=18&gt; &lt;Addresses#city&gt; &quot;Cambridge&quot; . &lt;Addresses/ID=18&gt; &lt;Addresses#state&gt; &quot;MA&quot; .\n4.5 ExercisePlease use direct mapping to map the following two relational tables to RDF triples with the base IRI http://foo.example/DB/ and prefix rdf：http://www.w3.org/1999/02/22-rdf-syntax-ns# .\n\n@base &lt;http://foo.example/DB/&gt; .@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;.&lt;Student/ID=001&gt; rdf:type &lt;Student&gt; .&lt;Student/ID=001&gt; &lt;Student#ID&gt; &quot;001&quot; .&lt;Student/ID=001&gt; &lt;Student#sname&gt; &quot;Zhang&quot; .&lt;Student/ID=001&gt; &lt;Student#major&gt; &quot;101&quot; .&lt;Student/ID=001&gt; &lt;Student#ref-major&gt; &lt;Major/ID=101&gt; .&lt;Major/ID=101&gt; rdf:type &lt;Major&gt; .&lt;Major/ID=101&gt; &lt;Major#ID&gt; &quot;101&quot; .&lt;Major/ID=101&gt; &lt;Major#mname&gt; &quot;CS&quot; .&lt;Major/ID=101&gt; &lt;Major#address&gt; &quot;CS_Building&quot; .\n5. RDB2RDF: R2RML\n\nR2RML is a language for specifying mappings from relational to RDF data.\n\n5.1 DV\n可以理解为物理表的一个虚表，没有实际物理内存\n\nA mapping takes as input a logical table, i.e.,\n\na database table\na database view (a virtual table collecting data from relational tables), or an SQL query (called an “R2RML view” because it is like an SQL view but does not modify the database)\n\n\n\n\nExample: database view\n\n5.2 A triples map5.2.1 Def\nA logical table is mapped to a set of triples by a rule called triples map.\n\n5.2.2 A triples map has three parts:\nthe input logical table \na subject map \nseveral predicate-object maps (combining predicate and object maps).\n\n5.3 Example:\nExample:@prefix rr: &lt;http:l//www.w3.org/ns/r2rml#&gt; .&lt;TriplesMap1&gt;a rr:TriplesMap;       #&lt;TriplesMap1&gt;前没有‘#’时要加这一句rr:logicalTable [rr:tableName &quot;Person&quot;];rr:subjectMap[\trr:template &quot;http://www.ex.com/Person/ID=&#123;ID&#125;&quot;;                 #ID=&#123;ID&#125;  直接&#123;ID&#125;都是可以的  看你自己怎么定义template\trr:class &lt;http://www.ex.com/Person&gt;;#表示从url拿class];rr:predicateObjectMap [\trr:predicate &lt;http:7/www.ex.com/Person#NAME&gt;; #表示从url拿predicate    rr:objectMap [rr:column &quot;NAME&quot;]; #表示从db中取].\n解析：\n#What is being mappedrr:logicalTable [rr:tableName &quot;Person&quot;]; #定义rr的指向\n#相当于&lt;Subject URI&gt; rdf:type &lt;Class URI&gt;rr:subjectMap[\trr:template &quot;http://www.ex.com/Person/ID=&#123;ID&#125;&quot;; #predicate URI                #Customized Subject URI\trr:class &lt;http://www.ex.com/Person&gt;;                 #Customized Class];\nrr:predicateObjectMap [\trr:predicate &lt;http:7/www.ex.com/Person#NAME&gt;; #Predicate URI    rr:objectMap [rr:column &quot;NAME&quot;]; #Object Literal].\n5.4 R2RML Examples5.4.1\nDB\n\n\n\nSet of RDF triples\n\n&lt;http://data.example.com/employee/7369&gt; rdf:type ex:Employee.&lt;http://data.example.com/employee/7369&gt; ex:name &quot;SMITH&quot;.&lt;http://data.example.com/employee/7369&gt; ex:department &lt;http://data.example.com/department/10&gt;.&lt;http://data.example.com/department/10&gt; rdf:type ex:Department.&lt;http://data.example.com/department/10&gt; ex:name &quot;APPSERVER&quot;.&lt;http://data.example.com/department/10&gt; ex:location &quot;NEW YORK&quot;.&lt;http://data.example.com/department/10&gt; ex:staff 1.\n\nR2RML\n\n@prefix rr: &lt;http://www.w3.org/ns/r2rml#&gt;.@prefix ex: &lt;http://example.com/ns#&gt;.&lt;#TriplesMap1&gt;rr:logicalTable [ rr:tableName &quot;EMP&quot; ];rr:subjectMap [\trr:template &quot;http://data.example.com/employee/&#123;EMPNO&#125;&quot;;\trr:class ex:Employee;];rr:predicateObjectMap [\trr:predicate ex:name;\trr:objectMap [ rr:column &quot;ENAME&quot; ];].\n\n5.4.2 View Definition\n&lt;#DeptTableView&gt; rr:sqlQuery &quot;&quot;&quot;SELECT DEPTNO,\tDNAME,\tLOC,\t(SELECT COUNT(*) FROM EMP WHERE EMP.DEPTNO=DEPT.DEPTNO) AS STAFF #查询两张表中同一个字段相同的个数FROM DEPT;&quot;&quot;&quot;.\n5.4.3 Mapping to a View Definition&lt;#TriplesMap2&gt; rr:logicalTable &lt;#DeptTableView&gt;;rr:subjectMap [     rr:template &quot;http://data.example.com/department/&#123;DEPTNO&#125;&quot;; \trr:class ex:Department; ]; rr:predicateObjectMap [ \trr:predicate ex:name; #对于rdf的Property\trr:objectMap [ rr:column &quot;DNAME&quot; ]; ]; rr:predicateObjectMap [ \trr:predicate ex:location; \trr:objectMap [ rr:column &quot;LOC&quot; ]; ]; rr:predicateObjectMap [ \trr:predicate ex:staff; \trr:objectMap [ rr:column &quot;STAFF&quot; ]; ].\n\n5.4.4 Linking Two Logical Tables\n@prefix rr: &lt;http://www.w3.org/ns/r2rml#&gt;.@prefix ex: &lt;http://example.com/ns#&gt;.&lt;#TriplesMap1&gt;rr:predicateObjectMap [\trr:predicate ex:department; #与rdf对应\trr:objectMap [#定义宾语映射，在第二张表找\t\trr:parentTriplesMap &lt;#TriplesMap2&gt;; #去map2去找subject来当宾语\t\trr:joinCondition [#表示这些属性是相同的\t\t\trr:child &quot;DEPTNO&quot;;#triple2就是child\t\t\trr:parent &quot;DEPTNO&quot;;#triple1就是parent\t\t];\t];].\n\nAdditional predicate object map for &lt;#TriplesMap1&gt;\nObject map retrieves subject from parent triples map by joining along a foreign key relationship\nIt joins\nthe current row of the logical table\nwith the row of the logical table of &lt;#TriplesMap1&gt; that satisfies the join condition 就是说Map1的每一行映射都满足这个条件\n\n\nNote:\nchild = referencing map\nparent = referenced map\n\n\n\n5.5 ExercisePlease write the R2RML triples map to map the following relational database to RDF triples with prefix rr: http://www.w3.org/ns/r2rml# and prefix ex: http://example.com/ns#(for classes and properties).\n\nRDF Triples\n&lt;http://data.example.com/student/001&gt; ex:name &quot;Zhang&quot;. &lt;http://data.example.com/student/002&gt; ex:name &quot;Wang&quot;.\n@prefix rr:&lt;http://www.w3.org/ns/r2rml#&gt; .@prefix rr:&lt;http://example.com/ns#&gt; .&lt;#TriplesMap1&gt;\trr:logicalTable [rr:tabelName &quot;RDB&quot;];\trr:subjectMap[\t\trr:template &quot;http://data.example.com/student/&#123;ID&#125;&quot;;\t\trr:class ex:Student;\t];\trr:predicateObjectMap[\t\trr:predicate ex:name; \t\trr:objectMap [rr:column &quot;Name&quot;];\t].\n6. Summary: RDB2RDF\nRDB2RDF is to map the content of Relational Databases to RDF.\nTwo W3C RDB2RDF standards: Direct Mapping and R2RML\nThe direct mapping defines a simple and intuitive transformation from RDB to RDF.\nR2RML is a language for expressing customized mappings (using external ontology vocabularies) from RDB to RDF.\n\n","categories":["KnowledgeEngineering"]},{"title":"Triple Extraction from Relational Web Tables","url":"/2021/08/15/knowledge%20engineering/9.Triple%20Extraction%20from%20Relational%20Web%20Tables/","content":"Triple Extraction from Relational Web Tables\n1. Tables are Everywhere on the Web\n1.1 Statistics on Web Tables\nWeb Tables : In 2008, the WebTables systems (developed by Google) extracts 14.1 billion HTML tables and finds 154 million are high-quality tables (1.1%).\nWeb Tables: The Web Data Commons project extracts 233 million Web tables from HTML pages in 2015.\nWikipedia Tables: In 2019, the Wikipedia snapshot contains more than 3 million tables from more than 520 thousand Wikipedia articles.\n\n1.2 Web Table Types\n1.3 Relational TableDef\nArelational table describes a set of entities in the core column(s)(相当于主键) along with their attributes in the remaining columns.\n\n\n1.4 Steps of Triple Extraction\nEntity Linking: Mention to Entity\n\n使得可以找到真正的信息\n\n\nColumn Typing: Column to Class (i.e., Type)\n\n给column分类\n\n\nRelation Extraction: Semantic Association between Columns to Relation\n\n推出列之间的关系\n\n\n\n\n2. Entity Linking2.1 What is Entity Linking (EL) in Web Tables?\nMapping the string mentions in table cells to their referent entities in a given knowledge base (KB).\n\n\n2.2 Detailed Steps of Entity Linking (Assumption: strings in a cell are a mention):\nCandidate Generation:\nIdentifying candidate referent entities of each string mention in table cells when given a knowledge base.\n\n\nEntity Disambiguation:\nChoosing only one entity from the candidate set as the referent entity of each string mention.\n\n\n\n2.3 Entity Linking: Candidate Generation2.3.1 Dictionary-based method\ncollecting all (string, entity) pairs from anchor texts, redirect pages, and disambiguation pages in Wikipedia; \nranking based on co-occurrence frequency.（成对出现概率）\n\n可以理解为通过已知的实体链接来学习出表格里的链接\n\n\n例如：\nJordan——-basketball player\n​            ——-professor\n​            ——-actor\n\n出现的频率并不一样，可以做一个排序\n\n\n2.3.2 String similarity based methodDef：\ncomputing string similarities between mentions and entities\n\nLevenshtein distance\nEdit distance: Levenshtein distance\nThe Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other.\n\n\n\n\n\nJaccard similarity\nJaccard similarity J(A, B) measures the similarity between finite sample set A and B.\n\n\nJ(A, B)=\\frac{|A\\cap B|}{|A\\cup B|}\n\nExercise\nCompute the Levenshtein distance between “kitten” and “sitting”.\n1+1+1=3\n\n\nCompute the Jaccard similarity between “University of California, Davis” and “University of California, Berkeley”in word level.\n\n\nA\\cup B=\\{University, of, California, Davis, Berkeley\\}\nA\\cap B=\\{University, of, California\\}\nJ(A, B)=\\frac{|A\\cup B|}{|A\\cap B|}=\\frac{3}{5}\nCompute the Jaccard similarity between “English”and “England”based on character-level trigrams.\n\n\nA=\\{Eng, ngl, gli, lis, ish\\}\nB=\\{Eng, ngl, gla, lan, and\\}\nJ(A, B)=\\frac{|A\\cap B|}{|A\\cup B|}=\\frac{2}{8}=\\frac{1}{4}2.3.3 Synonym-based method\ncombine with the dictionary-based method or string similarity based method\nuse a knowledge graph containing rich synonym information, e.g., BabelNet, WordNet\n\nWith the above methods, Top-K candidate referent entities for each mention can be obtained.2.4 Entity Disambiguation2.4.1 DefLocal Disambiguation \n\nOnly use the contextual information of the given mention and target entity for disambiguation without considering the effects of other referent entities in the same table.\n\nGlobal (Collective) Disambiguation \n\nLeverage the semantic associations between candidate referent entities to jointly disambiguate all entities for the mentions in a table.\n不同mention里的实体可以帮助链接\n\n\n\n2.4.2 Local Disambiguation: A Generative Model\nThe classic generative model is originally proposed to entity linking in text, but it can be applied to Web tables.\n假设Mention如下述生成过程：\n\n\n\nThe generative process for text: three steps\nChoose the referent entity e from the knowledge base with P(e), e.g., “Michael Jeffrey Jordan”;\nChoose a name s with 𝑃(𝑠│𝑒), e.g., “Jordan”;\nChoose a context c with 𝑃(𝑐│𝑒), e.g., “joins Bulls in 1984”.\n\n\n\n\n\nBased on the above generative story, the probability of a name mention m (its context is c and its name is s) referring to a specific entity e can be expressed as (assume that s and c are independent given e)\ns|e 与 c|e 条件独立\n\n\n\n\n𝑃(𝑚,𝑒)=𝑃(𝑠,𝑐,𝑒)=𝑃(𝑒)𝑃(𝑠│𝑒)𝑃(𝑐│𝑒)\nwhere P(e) corresponds to the popularity knowledge, \n\nP(s|e) corresponds to the name knowledge , and \nP(c|e) corresponds to the context knowledge.\n\n\nGiven a name mention m, to perform entity disambiguation, we need to find the entity e which maximizes the probability P(e|m), i.e.,\n\n\n\n𝑒=argmax_𝑒\\frac{P(m,e)}{P(m)}=argmax_e𝑃(𝑒)𝑃(𝑠|𝑒)𝑃(𝑐|𝑒)\nTraining data: \na set of annotated name mentions $𝑀=\\{𝑚_1,𝑚_2,…,𝑚_𝑛\\}$​​​, each m is a triple $𝑚=\\{𝑠,𝑒,𝑐\\}$​​; \nexample for text\n\n\n\n\n\ntable: Annotated Web tables from Web Data Commons project.\n只需将context换一下\n\n\n\nEntity popular model:\nP(e) reflects the popularity knowledge of the entity e; \nif e1 is more popular than e2, then $P(e_1)&gt;P(e_2)$;\n\n\n\nA more popular entity $e_1$ usually appears more times than a less popular entity $e_2$​ in a large text corpus（这里指的是知识库）, i.e., the frequency of occurrence $count(e_1)&gt;count(e_2)$​, e.g., in Wikipedia,\n这里应该是看mentions到知识库里映射实体数来决定\n\n\n\n\n\nThus, we model the popularity knowledge as：\n\n\nP(e)=\\frac{count(e)+1}{N+|M|}\nwhere $count(e)$ is the count of the name mentions whose referent entities are e in the training data,  （对应所有mention中在知识库映射的实体是e的个数）\nN is the number of all entities \nand |M| is the total name mention size.（用于平滑，使得所有实体至少有一个很小的base prob）\n\nThe estimation is further smoothed using the simple add-one smoothingmethod for the zero probability problem.\n\n\nEntity name model:\nP(s|e) encodes the name knowledge of entities;\n\nfor a specific entity e, its more frequently used name should be assigned a higher P(s|e), and a zero P(s|e) value should be assigned to those never used names.\n\n\n\n\nafter collecting all (entity, name) pairs from the name mention data set, then the maximum likelihood estimation is used to estimate the entity name model as\n不同于实体训练，这时候训练集是一个pair，之前的实体训练只要从知识库里找到所有实体就行\n\n\n\n\nP(s \\mid e)=\\frac{\\operatorname{count}(e, s)}{\\sum_{s} \\operatorname{count}(e, s)}\nwhere the count(e,s) is the count of the name mention s whose referent entity is e.\n\nProblem: it cannot deal with unseen names.\n\n\n\nunseen names include:\n\n\naliases 别词: “New York” $\\rightarrow$​ “Big Apple” ;\nacronyms 简写: “Michael Jeffrey Jordan” $\\rightarrow$ “MJ” ;\nmisspellings: “Barack Obama” $\\rightarrow$ “Barack Opama” .\n\n\nto solve this problem, apply the statistical translation model (IBM Model 1) to capture all possible name variations of the given word by translation operations as1) It is retained (translated into itself);2) It is translated into its acronym;3) It is omitted（省略）(translated into the word NULL);4) It is translated into another word (misspelling or alias).\n\n\n\nP(s|e) can be modeled as\n\n\n\nwhere $\\varepsilon$ is a normalization factor, $f$ is the full name of entity e, $l_{s}$ and $l_{f}$ are respectively the lengths of $s$ and $f, s_{i}$ is the $i$-th word of $s$, and $f_{j}$ is the $j$-th word of $f$​.\n\n这么理解就是$f_j$是full name 第j个word，full name 就是原来训练中出现的word，而$s_i$​是变换后出现的第i个word\n\n\nunseen names include:\n\n\n\naliases: “New York” $\\rightarrow$ “Big Apple” ;\nacronyms: “Michael Jeffrey Jordan” $\\rightarrow$ “MJ” ;\nmisspellings: “Barack Obama” $\\rightarrow$ “Barack Opama” .\n\n\nEntity context model (use text to explain):$P(c \\mid e)$​​ encodes the context knowledge of entities;\n\nif the entity $e$​​ frequently appears in the context $c$​​, then $P(c \\mid e)$​​​ should be higher; \n如果一个实体与知识库里出现的上下文频繁一致，则概率大\n\n\ne.g.,\n$C_1$: _wins NBA MVP.\n$C_2$: _is a researcher in machine learning.\n\n\nP(C_1 \\mid \\text{Michael Jeffrey Jordan} )>P(C_2 \\mid \\text{Michael Jeffrey Jordan} )Entity context model (use text to explain):\n\nthe context of each name mention $m$​ is the words surrounding $m$​;\nthe context knowledge of an entity $e$​ is encoded in an unigram language model $M_{e} .$​\n这么理解，我们把对于某个实体的context拆开成word级别，根据一元语法，就可以计算每个word的概率，从而形成集合\nM_{e}=\\left\\{P_{e}(t)\\right\\}where $P_{e}(t)$​ is the probability of the term $t$​ appearing in the context of $e$​. The term can be a word, a named entity, or a Wikipedia entity.\n\n\n\nEntity context model (use text to explain):\n\ngiven a context $c$​​ containing $\\mathrm{n}$​​ terms $t_{1}, t_{2} \\ldots t_{n}$​​, the entity context model estimates the probability $P(c \\mid e)$​​ as\nP(c \\mid e)=P\\left(t_{1} t_{2} \\ldots t_{n} \\mid M_{e}\\right)=P_{e}\\left(t_{1}\\right) P_{e}\\left(t_{2}\\right) \\ldots P_{e}\\left(t_{n}\\right)\naccording to the annotated name mention data set $M, P_{e}(t)$​ can be estimated by the maximum likelihood estimation:\nP_{e_{-} M L}(t)=\\frac{\\operatorname{Count}_{e}(t)}{\\sum_{t} \\operatorname{Count}_{e}(t)}\nwhere $Count _{e}(t)$ is the frequency of occurrences of a term $t$ in the contexts of the name mentions whose referent entity is $e$​​​.\n从$e$的所有context收集所有的term，这样就可以统计每一个term的频率\n\n\n\nEntity context model (use text to explain):\n\ndue to the sparse data problem, $P_{e}(t)$ is smoothed by Jelinek-Mercer smoothing method as\nP_{e}(t)=\\lambda P_{e_{M L}}(t)+(1-\\lambda) P_{g}(t)\nwhere $P_{g}(t)$​ is a general language model estimated using the whole Wikipedia data.\n\n2.4.3 More Features in Entity DisambiguationWeb Table Features:\nFeatures found in the table (T) or outside the table (C) \n表格外同样可以找到很多资源\n\n\nSingle table features (TS) refer to a value in a single cell while multiple features combine values coming from more than one cell (TM)\n\n\n\n2.4.4 Global Disambiguation: A Graph Model\nWhy global disambiguation works? \nsuppose we have three mentions which are needed to perform entity linking\n\n\n\n\n\n假设有三个mention需要做实体链接，借助他们的候选实体的语义关联，可以消除local ambiguation\n\nBuilding an Entity Disambiguation Graph for each given table. Each graph consists of:\n\nMention Nodes, Entity Nodes \nMention-Entity Edges: undirected edges between mentions and its candidate referent entities, \nEntity-Entity Edges: undirected edges between entities.\n\n\n\n\nEntity Disambiguation—Computing EL Impact Factors.\nTwo EL impact factors are as follows:\nimportance of each mention; \nsemantic relatedness between different nodes.\n测量不同节点的语义相关度\n\n\n\n\nEach node or edge in a constructed Entity Disambiguation Graph is assigned with a probability .\nFor mention-entity edges, their probabilities refer to the semantic relatedness between mentions and entities, called Mention-Entity Semantic Relatedness.\nFor entity-entity edges, their probabilities are seen as the semantic relatedness between entities, called Entity-Entity Semantic Relatedness\n\n\n\nMention-Entity Semantic Relatedness\nTwo features are used to measure Mention-Entity Semantic Relatedness $S R_{m e}(m, e)$​ between the given mention $m$​ and entity $e$​.\n\n\nString Similarity Feature strSim(m,e): depending on the Levenshtein distance between the string labels of $m$​​ and $e$​​.\n\n就算mention和实体的相似度\n\n\nMention-Entity Context Similarity Feature $contSim _{\\text {me }}(m, e)$​​ :\n\n计算mention和entity的背景相似度\n\nThe context of mention $m$​​ is denoted as menContext (m) :a) Collecting other mentions in the row or column where $m$​​​​ locates.\n\n同行与列词的集合\n\nb) Segmenting all the collected mentions into a set of words.\n\n\n  The context of entity $e$ is denoted as entContext(e):  a) Collecting all the RDF triples which $e$ exists in.  b) Segmenting all the objects (e is the subject) and subjects (e is the object) into a set of words.\n\n从知识库里找所有包含e的三元组，从而组成相应的背景集合\n$contSim_{m e}(m, e)$​​​​ is computed by the Jaccard similarity between menContext (m) and entContext ( e ).\n\n使用Jaccard similarity计算相似度\n\n\n\nS R_{m e}(m, e)=0.99 \\times\\left(\\alpha_{1} \\cdot \\operatorname{strSim}(m, e)+\\beta_{1} \\cdot \\operatorname{contSim}_{m e}(m, e)\\right)+0.01Entity Disambiguation-Computing EL Impact Factors.Two features are used to measure Entity-Entity Semantic Relatedness $S R_{e e}\\left(e_{1}, e_{2}\\right)$ between entities $e_{1}$ and $e_{2}$\n1) Triple Relation Feature $\\operatorname{I_{SRDF}}\\left(e_{1}, e_{2}\\right)$​ : verifying whether $e_{1}$​ and $e_{2}$​​ are in the same triple.2) Entity-Entity Context Similarity Feature contSim $_{e e}\\left(e_{1}, e_{2}\\right):$​    cont Sim $_{\\text {ee }}\\left(e_{1}, e_{2}\\right)$​ is computed by the $\\operatorname{Jaccard}$​ similarity between entContext $\\left(e_{1}\\right)$​ and entContext $\\left(e_{2}\\right)$​​.\n\nS R_{e e}\\left(e_{1}, e_{2}\\right)=0.99 \\times\\left(\\alpha_{2} \\cdot I_{S R D F}\\left(e_{1}, e_{2}\\right)+\\beta_{2} \\cdot \\operatorname{contSim}_{e e}\\left(e_{1}, e_{2}\\right)\\right)+0.01Entity Disambiguation-Iterative Probability Propagation.\nIterative probability propagation is used for combining different $\\boldsymbol{E} \\boldsymbol{L}$ impact factors for the $E L$ decisions.\nGiven an Entity Disambiguation Graph $G$ with $n$ nodes, we denote $G$ as an $n \\times n$ adjacency matrix $A, A_{i j}$ is the normalized semantic relatedness between node $i$ and node $j$, and $A_{i j}=A_{j i}$​.\n\nWe define an $n \\times 1$ vector $r$ to record the probabilities assigned to all the nodes, and iteratively compute $r$ with the following formula until convergence.\n\n\n\n2.4.5 A Test on Zhishi.me—Consisting of Three Chinese Linked KBs.\nApplying our proposed approach to more than 70,000Web tables with each single KB in Zhishi.me.\nFor each mention, and if any two identified entities from different KBs do not have the samesAs relation, then there exists a conflict here.2) 对于每条mention，如果有两个相同的实体来自不同的知识库，但是他们没有相同的关系，那么就会产生冲突\nAccording to the statistics, there exist conflicts in the EL results of38.94% mentions.\n单一知识库的实体映射问题，这其实可以理解就是，你一开始在做实体链接的时候，如果只用到一个知识库，那么由于这个知识库\n\n\n\nReasons of ConflictsReason 1: The samesAs relations are incomplete between KBs.\nSolution: \n\nCompleting samesAs relations with a supervised learning model using synonym feature, string similarity feature and entity-entity context similarity feature.\n\nReason 2: For some KBs, some potential correct referent entities do not rank the highest.\n\n没有排序对\n\nSolution: \n\nGrouping the entities representing the same individual into differentsets using the existing and newly learned samesAs relations.\n\n把相同的实体组成一个集合\n\n\nComputing the average ranking, the highest ranking and the number of the entities in each set, and then applying three heuristic rules to solving conflicts.\n\n计算每个集合的平均rangking，再排序\n\n\n\nDetail RulesRule 1: If both the average ranking and the highest ranking of the entities in a set rank the highest, and the number of the entities in this set is not less than half of the number of KBs, then we choose this set as the final EL results for the given mention.\nRule 2: If there exist two or more sets that the average ranking and the highest ranking of the sets’ corresponding entities are the same and rank the highest, also the number of the entities in each of these sets is not less than half of the number of KBs, then we choose one set at random as the final EL results for the given mention.\nRule 3: If the number of the entities in each set is less than half of the number of KBs, the original EL results of the given mention remain unchanged.\n\n总结就是只有当相同的实体出现到一定数量，才使用这个特殊规则\n\n\n\n对于一个mention我们去各个KB找相同的候选实体\n\n\n3.  Column Typing: Column to Class (i.e., Type)\n\n\n利用同一列某一个mention的实体类型作为该mention的实体类型\n或者利用Table本身head的信息比如city\n\n4. Relation ExtractionDefinition\nRelation extraction refers to the task of associating a pair of columns in atable with the relation that holds between their contents and / or extractingrelationship information from tabular data and representing them in astructured format (e.g., as subject-predicate-object triples).\n\nFor binary relationships, the relationship between columns A and B islabeled with R if a substantial number of pairs of values from A and Boccur in the relations database.\n\n\n\n\n利用知识库中本来存在的三元组，来推测出未知的三元组\n\n","categories":["KnowledgeEngineering"]},{"title":"QA","url":"/2021/08/15/nlp%20learning/Chapter11_QA/","content":"QA\n\n1.Why is QA back centre stage of AI?\nThe way of Human machine information interaction has changed\n\nThe rapid development of mobile and wearable devices requires effective and accurate information service in the form of natural language\n\n\n1. QA系统常见的问题类型：\nFactoid questions 基于事实型的问题\nWho wrote “the Universal Declaration of Human Rights”?\nHow many calories are there in two slices of apple pie?\nWhat is the average age of the onset of autism?\n\n\nComplex(narrative) questions: 复杂（描述性）问题\nIn children with an acute febrie illness, what is the effcacy of acetaminophen in reducing\nWhat do scholars think about Jefferson’s position ondealing with pirates?\n\n\n\n2. QA常见方法：\nIR-based approaches\nTREC; IBM Watson; Google\n\n\nKnowledge-based and Hybrid approaches\nIBM Watson; Apple Siri; Wolfram Alpha; TrueKnowledge Evi\n\n\nCommunity-based question answering\n知乎、Quora\n众包\n\n\n\n3. IR-based Factoid QA\n\nQuestion Processing  问题处理 \nDetect question type, answer type, focus, relations\n检测问题类型、答案类型、核心词、关系\n姚明、身高——&gt;依赖\n\n\nFormulate queries to send to a search engine\n形成queries输入搜索引擎\n\n\nPassage Retrieval 文本查找 \nRetrieve ranked documents 检索排名文档\n查询姚明、升高\nTF-IDF\n\n\n分解成合适的段落、并且重新排序 Break into suitable passages and rerank\nNER、词性标注\n搜索所有数字，对所有数字进行排序\n\n\n\n\n答案处理 Answer Processing\n提取候选答案 Extract candidate answers\n排序候选项 Rank candidates\nusing evidence from the text and external sources \n\n\n\n\n\n3.1 Question Procesing\nAnswer Type Detection\nDecide the named entity type (person, place) of the answer\n大致就是判断答案的命名实体是什么\n\n\nQuery Formulation\nChoose query keywords for the IR system\n利用IR系统选择问答中的关键词\n\n\nQuestion Type classification\n\nIs this a definition question, a math question, a list question?\n答案类型识别，即看这个答案整体内容属于什么类型\n\n\nFocus Detection\n\nFind the question words that are replaced by the answer\n找出被答案替换的疑问词\n\n\nRelation Extraction\n\nFind relations between entities in the question\n查找问题中实体之间的关系\n\n\nExample\n\nQuestion: Please return the two states you could be reentering if you’re crossing Florida’s northern border 如果你穿越佛罗里达州、北部边界，请返回你可能重新进入的两个州\n\nAnswer Type: US state\nQuery: two states, border, Florida, north\nFocus: the two states\nRelations: borders(Florida, ?x, north)\n\n\n\n3.2 Answer Type Detection: Named Entities\nWho founded Virgin Airlines?\n\nPERSON\n\n\nWhat Canadian city has the largest population?\n\nCITY.\n\n\n\n3.2.1 Answer Type Taxonomy\n6 coarse classes\nABBEVIATION, ENTITY, DESCRIPTION, HUMAN, LOCATION, NUMERIC\n缩写、实体、描述、人员、位置、数字\n\n\n50 finer classes\nLOCATION: city, country, mountain…\nHUMAN: group, individual, title, description\nENTITY: animal, body, color, currency…\n\n\n\n\n\n3.2.2 Methods\nHand written rules\nMachine Learning\nHybrids\n\nHand written rules\nRegular expression based rules can get some cases: 基于正则表达式的规则可以获得某些情况：\nWho { is|was|are|were } PERSON\nPERSON (YEAR YEAR)\n\n\nOther rules use the question headword 其他规则使用疑问词\n(the headword of the first noun phrase after the wh-word) （wh单词后第一个名词短语的中心词）\nWhich city in China has the largest number of foreign financial companies?\nWhat is the state flower of California?\n\n\n\nMachine Learning\nDefine a taxonomy of question types\nAnnotate training data for each question type\nTrain classifiers for each question class using a rich set of features.\n\nfeatures include those hand written rules!\n\n\nFeatures for Answer Type Detection\n\nQuestion words and phrases 疑问词和、短语\nPart of speech tags 词性标记\nParse features (headwords) 分析功能（标题词）\nNamed Entities 命名实体\nSemantically related words 语义相关词\n\n\n\n3.3 Query Formulation\nSelect all non stop words in quotations 选择报价单中的所有非停用词\nSelect all NNP words in recognized named entities 选择已识别命名实体中的所有NNP字\nSelect all complex nominals with their adjectival modifiers 选择所有带形容词修饰符的复数名词\nSelect all other complex nominals 选择所有其他复杂名词\nSelect all nouns with their adjectival modifiers 选择所有名词及其形容词修饰语\nSelect all other nouns 选择所有其他名词\nSelect all verbs 选择所有动词\nSelect all adverbs 选择所有副词\nSelect the QFW word (skipped in all previous steps) 选择QFW字（在前面的所有步骤中跳过）\nSelect all other words 选择所有其他单词\n\n3.4 Choosing keywords from the query\n3.5 Passage Retrieval\nStep 1: IR engine retrieves documents using query terms IR引擎使用查询系统术语检索文档\n\nStep 2: Segment the documents into shorter units 将文档分割为较短的单元\n\nsomething like paragraphs\n\n\nStep 3: Passage ranking 文章排名\n\nUse answer type to help rerank passages 使用答案类型帮助重新阅读文章\n\n\n\n3.5.1 Features for Passage Ranking\nNumber of Named Entities of the right type in passage  段落中正确类型的命名实体数\nNumber of query words in passage 段落中的查询字的数量\nNumber of question N grams also in passage 段落中N词的数量\nProximity of query keywords to each other in passage 查询关键字在段落中相互的相似度\nLongest sequence of question words 最长的疑问词序列\nRank of the document containing passage 包含段落的文档的文章\n\n3.6 Answer Extraction\nRun an answer type named entity tagger on the passages 在段落中检测答案实体类型的词\nEach answer type requires a named entity tagger that detects it 每个答案类型都要有一个标记用于检测\nIf answer type is CITY, tagger has to tag CITY\nCan be full NER, simple regular expressions, or hybrid\n\n\n\n\nReturn the string with the right type: \nWho is the prime minister of India (PERSON) Manmohan Singh , Prime Minister of India, had told left leaders that the deal would not be renegotiated\nHow tall is Mt. Everest? (LENGTH) The official height of Mount Everest is29035 feet\n\n\n\n3.7 Ranking Candidate Answers\nBut what if there are multiple candidate answers!\n\nQ: Who was Queen Victoria’s second son?\n\nAnswer Type: Person\nPassage:\nThe Marie biscuit is named after Marie Alexandrovna , the daughter of Czar Alexander II of Russia and wife of Alfred, the second son of Queen Victoria and Prince Albert\n\n\n\nUse machine learning:Features for ranking candidate answers\nAnswer type match : Candidate contains a phrase with the correct answer type.\nPattern match : Regular expression pattern matches the candidate.\nQuestion keywords: # of question keywords in the\nKeyword distance: Distance in words between the candidate and query keywords\nNovelty factor: A word in the candidate is not in the \n\nApposition features: The candidate is an appositive to question terms\n\nPunctuation location: The candidate is immediately followed by a comma, period, \nquotation marks, semicolon, or exclamation mark.\nSequences of question terms: The length of the longest sequence of question terms that occurs in the candidate answer.\n\nCandidate Answer scoring in IBM Watson\nEach candidate answer gets scores from &gt;50 components\n(from unstructured text, semi structured text, triple stores)\nlogical form (parse) match between question and candidate\n问题和候选人之间的逻辑形式（解析）匹配\n\n\npassage source reliability\n文章源的可靠性\n\n\ngeospatial location\nCalifornia is southwest of Montana”\n\n\ntemporal relationships\ntaxonomic classification\n\n\n\n3.8 Common Evaluation Metrics\nAccuracy (does answer match gold labeled answer?)\nMean Reciprocal Rank 平均倒数排名\nFor each query return a ranked list of M candidate answers. 对于每个查询，返回M个候选答案的排序列表。\nQuery score is 1/Rank of the first correct answer 查询分数为查到第一个正确答案排名的倒数\nIf first answer is correct: 1\nelse if second answer is correct: ½\nelse if third answer is correct: ⅓, etc.\nScore is 0 if none of the M answers are correct\n\n\nTake the mean over all N queries\n\n\n\n\nMRR=\\frac{\\sum_{i=1}^N\\frac{1}{rank_i}}{N}\nRelevance 相关度 The level in which the answer addresses users information needs\nCorrectness 正确度 The level in which the answer is factually correct\nConciseness 精炼度 答案不包含不相关信息\nCompleteness 完备度 答案应该完整\nSimplicity 简单度 答案易于解释\nJustification 合理度 Sufficient context should be provided to support the data consumer in the determination of the query correctness\n\n\nRight The answer is correct and complete\nInexact The answer is incomplete or incorrect\nUnsupported The answer does not have an appropriate evidence/justification\nWrong： The answer is not appropriate for the question\n\n4. Knowledge-based QA\n构建查询的语义表示 Build a semantic representation of the query \n\nTimes, dates, locations, entities, numeric quantities\n\n\n从该语义映射到查询结构化数据或资源 Map from this semantics to query structured data or resources\n\nGeospatial databases\nOntologies (Wikipedia infoboxes , dbPedia , WordNet , Yago\nRestaurant review sources and reservation services\nScientific databases\n\n\n\n4.1 Two challenges\n\n\n词法鸿沟 Lexical Gap Example\n构建出来的词不一定和知识库的实体相同 The constructed words are not necessarily the same as the entities of the knowledge base\n\n\n语义鸿沟\n构建出来的图不一定和知识库匹配  The constructed graph does not necessarily match the knowledge base\n\n\n\n4.1.1 Lexical Gap Example\nWhich Greek cities have more than 1 million inhabitants?\n\nSELECT DISTINCT ?uriWHERE {\t?uri rdf:type dbo:City\t?uri dbo:country res:Greece\t?uri dbo:populationTotal ?p\tFILTER (?p &gt; 1000000)}\n\nThere are expressions with a fixed, dataset independent meaning.\n有些表达式具有固定的、与数据集无关的含义。 most, one\n\n\nWho produced the most films?\n\nSELECT DISTINCT ?uriWHERE {?x rdf:type dbo:Film?x dbo:producer uri}ORDER BY DESC(COUNT(?x))OFFSET 0 LIMIT 1\n\nChallenges (Semantic gap):\n\nThe semantic gap [ between natural language and knowledge graphs\n\n\n\n4.1.2 Semantic Gap Example\nDifferent datasets usually follow different schemas, thus provide different ways of answering an information need\n\n\n\nThe meaning of expressions like the verbs to be to have and prepositions of with etc strongly depends on the linguistic context\n\n\n4.2 Pattern/Template based KN QA\nMotivation\nIn order to understand a user question, we need to understand\n\n\n\n\n4.3 Methods\nAn approach that combines both an analysis of the semantic structure and a mapping of words to URIs 一种结合语义结构分析和单词到URI映射的方法\n\nTemplate generation 模板生成\nParse question to produce a SPARQL template that directly mirrors the structure of the question, including filters and aggregation operations解析问题以生成直接反映问题结构的SPARQL模板，包括过滤和聚合操作\n\n\nTemplate instantiation 模板实例化\nInstantiate SPARQL template by matching natural language expressions with ontology concepts using statistical entity identification and predicate detection 通过使用统计实体识别和谓词检测将自然语言表达式与本体概念匹配来实例化SPARQL模板\n\n\n\n\nQuestion Who produced the most films?\n\n\n\n\nStep 1: Template generation Linguistic processing\n首先，获取自然语言问题的 POS tags 信息\n其次，基于 POS tags, 语法规则表示问句\n然后利用 domain dependent 词汇和 domain independent 词汇辅助分析问题\n最后，将语义表示转化为一个 SPARQL 模板\n\n\ndomain independent : who, the most\ndomain dependent : produced/VBD, films/NNS\n\n\n\nStep2: Template matching and instantiation NER\n有了 SPARQL 模板以后 需要进行实例化与具体的自然语言问句相匹配 。 即将自然语言问句与知识库中的本体概念相映射的过程\n对于 resources 和 classes, 实体识别常用方法\n用 WordNet 定义知识库中标签的同义词\n计算字符串相似度 ( Levenshtein 和子串相似度\n\n\n对于 property labels, 将还需要与存储在 BOA 模式库中的自然语言表示进行比较\n最高排位的实体将作为填充查询槽位的候选答案\n\n\n\n\n\nWho produced the most films??c CLASS [films]&lt;http://dbpedia.org/ontology/Film&gt;&lt;http://dbpedia.org/ontology/FilmFestival&gt;...?p PROPERTY [produced]&lt;http://dbpedia.org/ontology/producer&gt;&lt;http://dbpedia.org/property/producer&gt;&lt;http://dbpedia.org/ontology/wineProduced&gt;\n\nStep 3:Ranking\n每个 entity 根据 string similarity 和 prominence 获得一个打分\n一个 query 模板的分值根据填充 slots 的多个 entities 的平均打分\n另外 需要检查 type 类型\n对于所有的三元组 ?x rdf type &lt; 对于查询三元组 ?x p e 和 e p ?x 需要检查 p 的 domain/range 是否与 &lt; 一致\n\n\n对于全部的查询集合 仅返回打分最高的\n\n\n\nSELECT DISTINCT ?x WHERE {?x &lt;http://dbpedia.org/ontology/producer&gt; ?y .?y rdf:type &lt;http://dbpedia.org/ontology/Film&gt; .}ORDER BY DESC(COUNT(?y)) LIMIT 1Score: 0.76SELECT DISTINCT ?x WHERE {?x &lt;http://dbpedia.org/ontology/producer&gt; ?y .?y rdf:type &lt;http://dbpedia.org/ontology/FilmFestival&gt;.}ORDER BY DESC(COUNT(?y)) LIMIT 1Score: 0.60\n4.4 Parsing based KB QA\n大概和上面不同的是先将查询语句解析，即语义解析，最后构建查询；不同于上面模板用到了关系抽取、句法分析、语义组合 TODO\n\nPhrase mapping\n\nQuery Struecture (Logical Form) Computing\nQuery Evaluation\nAnswers Ranking\n\n\n\n\n\n\nSemantic Parsing on Freebase from Question Answer Pairs. EMNLP 2013\n\n\n\n5. Hybrid approaches (IBM Watson)\nBuild a shallow semantic representation of the query\n\n构建查询的浅层语义表示\n\n\nGenerate answer candidates using IR methods\n\n使用IR方法生成候选答案\nAugmented with ontologies and semi structured data\n增加了本体和半结构化数据\n\n\nScore each candidate using richer knowledge sources\n\n使用更丰富的知识来源为每个候选人打分\nGeospatial databases 地理空间数据库\nTemporal reasoning 时间推理\nTaxonomical classification 层次分类\n\n\n\n6. End to End(deep learning) based KB QA\nOnly for Single Relation and Simple Question\n\nStep1: Candidates generation\n\nFind main entity by Entity Linking 按实体链接查找主实体\nAll entities around the main entity in KG are candidates KG中主实体周围的所有实体都是候选实体\n\n\nStep2: Ranking\n\n\n\n\n\n\n\n7. Dealing with unexpected things…\n\nCaused by Processing\nPoor Ranking\nHarsh Query Constraints\nMisunderstanding of Query\n\n\nCaused by Data\nInaccurate Facts\nIncomplete Data\n\n\n\n\n\n7.1 Search KG in Embedding Space\n\n\n","categories":["nlp"]},{"title":"Regular Expression","url":"/2021/08/15/nlp%20learning/Chapter1_regularexpressions/","content":"正则表达式操作\n\nre—- 正则表达式操作\n正则表达式可以拼接；如果 A 和 B 都是正则表达式，则 AB 也是正则表达式。\n正则表达式可以包含普通或者特殊字符。绝大部分普通字符，比如 'A', 'a', 或者 '0'，都是最简单的正则表达式。它们就匹配自身。你可以拼接普通字符，所以 last 匹配字符串 'last'. \n有些字符，比如 '|' 或者 '('，属于特殊字符。 特殊字符既可以表示它的普通含义， 也可以影响它旁边的正则表达式的解释。\n重复修饰符 (*, +, ?, {m,n}, 等) 不能直接嵌套。这样避免了非贪婪后缀 ? 修饰符，和其他实现中的修饰符产生的多义性。要应用一个内层重复嵌套，可以使用括号。 比如，表达式 (?:a{6})* 匹配6个 'a' 字符重复任意次数。\n\n特殊字符有：\n.\n(点) 在默认模式，匹配除了换行的任意字符。如果指定了标签 DOTALL ，它将匹配包括换行符的任意字符。\n\n^\n(插入符号) 匹配字符串的开头， 并且在 [MULTILINE] 模式也匹配换行后的首个符号。\n\n$\n匹配字符串尾或者在字符串尾的换行符的前一个字符，在 MULTILINE模式下也会匹配换行符之前的文本。 foo 匹配 ‘foo’ 和 ‘foobar’，但正则表达式 foo$ 只匹配 ‘foo’。 更有趣的是，在 'foo1\\nfoo2\\n' 中搜索 foo.$，通常匹配 ‘foo2’，但在 [MULTILINE]模式下可以匹配到 ‘foo1’；在 'foo\\n' 中搜索 $ 会找到两个（空的）匹配：一个在换行符之前，一个在字符串的末尾。\n\n*\n对它前面的正则式匹配0到任意次重复， 尽量多的匹配字符串。 ab* 会匹配 'a'，'ab'，或者 'a' 后面跟随任意个 'b'。\n\n+\n对它前面的正则式匹配1到任意次重复。 ab+ 会匹配 'a' 后面跟随1个以上到任意个 'b'，它不会匹配 'a'。\n\n?\n对它前面的正则式匹配0到1次重复。 ab? 会匹配 'a' 或者 'ab'\n\n*?, +?, ??\n'*', '+'，和 '?' 修饰符都是 贪婪的；它们在字符串进行尽可能多的匹配。有时候并不需要这种行为。如果正则式 &lt;.*&gt; 希望找到 '&lt;a&gt; b &lt;c&gt;'，它将会匹配整个字符串，而不仅是 '&lt;a&gt;'。在修饰符之后添加 ? 将使样式以 非贪婪方式或者 :dfn:最小 方式进行匹配； 尽量 少 的字符将会被匹配。 使用正则式 &lt;.*?&gt; 将会仅仅匹配 '&lt;a&gt;'。\n\n{m}\n对其之前的正则式指定匹配 m 个重复；少于 m 的话就会导致匹配失败。比如， a{6} 将匹配6个 'a' , 但是不能是5个。\n\n{m,n}\n对正则式进行 m 到 n 次匹配，在 m 和 n 之间取尽量多。 比如，a{3,5} 将匹配 3 到 5个 'a'。忽略 m 意为指定下界为0，忽略 n 指定上界为无限次。 比如 a{4,}b 将匹配 'aaaab' 或者1000个 'a' 尾随一个 'b'，但不能匹配 'aaab'。逗号不能省略，否则无法辨别修饰符应该忽略哪个边界。\n\n{m,n}?\n前一个修饰符的非贪婪模式，只匹配尽量少的字符次数。比如，对于 'aaaaaa'， a{3,5} 匹配 5个 'a' ，而 a{3,5}? 只匹配3个 'a'。\n\n\n\n\\\n转义特殊字符（允许你匹配 '*', '?', 或者此类其他），或者表示一个特殊序列；特殊序列之后进行讨论。如果你没有使用原始字符串（ r'raw' ）来表达样式，要牢记Python也使用反斜杠作为转义序列；如果转义序列不被Python的分析器识别，反斜杠和字符才能出现在字符串中。如果Python可以识别这个序列，那么反斜杠就应该重复两次。这将导致理解障碍，所以高度推荐，就算是最简单的表达式，也要使用原始字符串。\n\n[]\n用于表示一个字符集合。在一个集合中：\n\n字符可以单独列出，比如 [amk] 匹配 'a'， 'm'， 或者 'k'。\n\n可以表示字符范围，通过用 '-' 将两个字符连起来。比如 [a-z] 将匹配任何小写ASCII字符， [0-5][0-9] 将匹配从 00 到 59 的两位数字， [0-9A-Fa-f] 将匹配任何十六进制数位。 如果 - 进行了转义 （比如 [a\\-z]）或者它的位置在首位或者末尾（如 [-a] 或 [a-]），它就只表示普通字符 '-'。\n\n特殊字符在集合中，失去它的特殊含义。比如 [(+*)] 只会匹配这几个文法字符 '(', '+', '*', or ')'。\n\n字符类如 \\w 或者 \\S (如下定义) 在集合内可以接受，它们可以匹配的字符由 [ASCII]模式决定。\n\n不在集合范围内的字符可以通过 取反 来进行匹配。如果集合首字符是 '^' ，所有 不 在集合内的字符将会被匹配，比如 [^5] 将匹配所有字符，除了 '5'， [^^] 将匹配所有字符，除了 '^'. ^ 如果不在集合首位，就没有特殊含义。\n\n在集合内要匹配一个字符 ']'，有两种方法，要么就在它之前加上反斜杠，要么就把它放到集合首位。比如， [()[\\]{}] 和 []()[{}] 都可以匹配括号\n\n\n|\n`A|B， A 和 B 可以是任意正则表达式，创建一个正则表达式，匹配 A 或者 B. 任意个正则表达式可以用 '|' 连接。它也可以在组合（见下列）内使用。扫描目标字符串时， '|' 分隔开的正则样式从左到右进行匹配。当一个样式完全匹配时，这个分支就被接受。意思就是，一旦 A 匹配成功， B 就不再进行匹配，即便它能产生一个更好的匹配。或者说，'|' 操作符绝不贪婪。 如果要匹配 '|' 字符，使用 \\|， 或者把它包含在字符集里，比如 [|].\n\n(...)（组合），匹配括号内的任意正则表达式，并标识出组合的开始和结尾。匹配完成后，组合的内容可以被获取，并可以在之后用 \\number 转义序列进行再次匹配，之后进行详细说明。要匹配字符 '(' 或者 ')', 用 \\( 或 \\), 或者把它们包含在字符集合里: [(], `[)]``\n\n(?…)\n这是个扩展标记法 （一个 '?' 跟随 '(' 并无含义）。 '?' 后面的第一个字符决定了这个构建采用什么样的语法。\n\n`(?aiLmsux)\n( 'a', 'i', 'L', 'm', 's', 'u', 'x' 中的一个或多个) 这个组合匹配一个空字符串；\n\n(?:…)\n正则括号的非捕获版本。 匹配在括号内的任何正则表达式，但该分组所匹配的子字符串 不能 在执行匹配后被获取或是之后在模式中被引用。\n\n(?aiLmsux-imsx:…)\n('a', 'i', 'L', 'm', 's', 'u', 'x' 中的0或者多个， 之后可选跟随 '-' 在后面跟随 'i' , 'm' , 's' , 'x' 中的一到多个 .) \n\n(?#…)\n注释；里面的内容会被忽略。\n\n\\number:\\1\n匹配数字代表的组合。每个括号是一个组合，组合从1开始编号。比如 (.+) \\1 匹配 'the the' 或者 '55 55', 但不会匹配 'thethe' (注意组合后面的空格)。这个特殊序列只能用于匹配前面99个组合。如果 number 的第一个数位是0， 或者 number 是三个八进制数，它将不会被看作是一个组合，而是八进制的数字值。在 '[' 和 ']' 字符集合内，任何数字转义都被看作是字符。\n\n\\b\n匹配空字符串，但只在单词开始或结尾的位置。一个单词被定义为一个单词字符的序列。注意，通常 \\b 定义为 \\w 和 \\W 字符之间，或者 \\w 和字符串开始/结尾的边界， 意思就是 r'\\bfoo\\b' 匹配 'foo', 'foo.', '(foo)', 'bar foo baz' 但不匹配 'foobar' 或者 'foo3'。\nk = re.findall(r'\\bfoo\\b', 'foo')print(k)k = re.findall(r'\\bfoo\\b', '(foo)')print(k)k = re.findall(r'\\bfoo\\b', 'foo.')print(k)k = re.findall(r'\\bfoo\\b', 'bar foo bar')print(k)k = re.findall(r'\\bfoo\\b', 'foo3')print(k)\noutput:  [‘foo’] [‘foo’] [‘foo’] [‘foo’] []\n\\s\n\n对于 Unicode (str) 样式：\n匹配任何Unicode空白字符（包括 [ \\t\\n\\r\\f\\v] ，还有很多其他字符，比如不同语言排版规则约定的不换行空格）。如果 ASCII 被设置，就只匹配 [ \\t\\n\\r\\f\\v] 。\n\n对于8位(bytes)样式：\n匹配ASCII中的空白字符，就是 [ \\t\\n\\r\\f\\v] 。\n\n\n\n\\w\n\n对于 Unicode (str) 样式：\n匹配Unicode词语的字符，包含了可以构成词语的绝大部分字符，也包括数字和下划线。如果设置了 ASCII 标志，就只匹配 [a-zA-Z0-9_] 。\n\n对于8位(bytes)样式：\n匹配ASCII字符中的数字和字母和下划线，就是 [a-zA-Z0-9_] 。如果设置了 [LOCALE] 标记，就匹配当前语言区域的数字和字母和下划线。\n\n\n\n\nRe.  相关函数\nre.``compile(pattern, flags=0)\n将正则表达式的样式编译为一个 正则表达式对象 （正则对象），可以用于匹配，通过这个对象的方法 [match()], [search()]以及其他如下描述。\n这个表达式的行为可以通过指定 标记 的值来改变。值可以是以下任意变量，可以通过位的OR操作来结合（ | 操作符）。\n序列\nprog = re.compile(pattern)result = prog.match(string)\n等价于\nresult = re.match(pattern, string)\n如果需要多次使用这个正则表达式的话，使用 [re.compile()] 和保存这个正则对象以便复用，可以让程序更加高效。\n注解:通过 [re.compile()]编译后的样式，和模块级的函数会被缓存， 所以少数的正则表达式使用无需考虑编译的问题。\n\nre.search(pattern, string, flags=0)\n扫描整个 字符串 找到匹配样式的第一个位置，并返回一个相应的 [匹配对象]。如果没有匹配，就返回一个 None ； 注意这和找到一个零长度匹配是不同的。\n\nre.match(pattern, string, flags=0)\n如果 string 开始的0或者多个字符匹配到了正则表达式样式，就返回一个相应的 [匹配对象]。 如果没有匹配，就返回 None ；注意它跟零长度匹配是不同的\n\nfullmatch(pattern, string, flags=0)\n如果整个 string 匹配到正则表达式样式，就返回一个相应的 [匹配对象] 。 否则就返回一个 None ；注意这跟零长度匹配是不同的。\n\nsplit(pattern, string, maxsplit=0, flags=0)\n用 pattern 分开 string 。 如果在 pattern 中捕获到括号，那么所有的组里的文字也会包含在列表里。如果 maxsplit 非零， 最多进行 maxsplit 次分隔， 剩下的字符全部返回到列表的最后一个元素。\n\nre.split()切割功能非常强大\n单字符切割\n两个字符以上切割需要放在 [ ] 中\n所有空白字符切割\n使用括号捕获分组，默认保留分割符\n不想保留分隔符，以（?:…）的形式指定\n\n\nre.findall(pattern, string, flags=0)\n对 string 返回一个不重复的 pattern 的匹配列表， string 从左到右进行扫描，匹配按找到的顺序返回。如果样式里存在一到多个组，就返回一个组合列表；就是一个元组的列表（如果样式里有超过一个组合的话）。空匹配也会包含在结果里。\n\nre.sub(pattern, repl, string, count=0, flags=0)\n返回通过使用 repl 替换在 string 最左边非重叠出现的 pattern 而获得的字符串。 如果样式没有找到，则不加改变地返回 string。 repl 可以是字符串或函数；如为字符串，则其中任何反斜杠转义序列都会被处理。 也就是说，\\n 会被转换为一个换行符，\\r 会被转换为一个回车附，依此类推。 未知的 ASCII 字符转义序列保留在未来使用，会被当作错误来处理。 其他未知转义序列例如 \\&amp; 会保持原样。 向后引用像是 \\6 会用样式中第 6 组所匹配到的子字符串来替换。 例如\n\n\n上课笔记&amp;重点\n\n\n\n\n\n\n\n\n\ntxt = 'Column 1 Column 2 Column 3 Column 3546   Column 3s   're.findall('Column [0-9]+ *', txt)\n\nre.search('(Column [0-9]+ *)*', txt)\n\nre.findall('(Column [0-9]+ *)+', txt)\n\n上课示例import retxt = \"Tie best singer ever is Jay Chou，The ticket price for his concert is $100\"x = re.findall('[A-Z]', txt)if(x):    print('find success', x)x = re.search('a\\^b', 'sdasd a^b sab')print(x)\n\ntxt2 = 'The best thing is  love, the other is hate'm = re.findall('the', txt2)print(m)m = re.findall('[tT]he', txt2)print(m)m = re.findall('\\\\b[tT]he\\\\b', txt2)print(m)m = re.findall(r'\\b[tT]he\\b', txt2)print(m)m = re.findall('[^a-zA-Z][tT]he[^a-zA-Z]', txt2)print(m)m = re.findall(r'[^a-zA-Z][tT]he[^a-zA-Z]', txt2)print(m)\n\ntxt3 = '025-12345678 #这是一个南京电话号码'n = re.sub('#.*$','', txt3)print(n)\n\ntxt4 = 'guppy guppies'd = re.findall('guppy|ies', txt4)print(d)d = re.findall('gupp(y|ies)', txt4)print(d) #显示最优先匹配d = re.match('gupp(y|ies)', txt4)print(d)d = re.findall('guppy|guppies', txt4)print(d) #显示最优先匹配\n\ntxt4 = 'once upon a time'l = re.match('[a-z]*', txt4)print(l)\n\nFinate State Automata\n\n\n\n\n\n","categories":["nlp"]},{"title":"Word Vector","url":"/2021/08/15/nlp%20learning/Chapter5_Word%20Vector/","content":"Word Vector\n\n\n1. Logistic Regression for x\nInput:\na document \na fixed set of classes ​\n\n\nOutput: a predicted class ​\n\nInput observation: vector \nWeights: one per feature: ​\n\nSometimes we call the weights \n\nOutput: a predicted class ​\n(multinomial logistic regression: ​\n2. Making  with sigmoids\n\\hat{y}= \\begin{cases}1 & \\text { if } P(y=1 \\mid x)>0.5 & \\text { if } \\mathrm{w} \\cdot \\mathrm{x}+\\mathrm{b}>0 \\\\ 0 & \\text { otherwise } & \\text { if } \\mathrm{w} \\cdot \\mathrm{x}+\\mathrm{b} \\leq 0\\end{cases}\n\\begin{aligned}\nP(y=1) &=\\sigma(w \\cdot x+b) \\\\\n&=\\frac{1}{1+\\exp (-(w \\cdot x+b))} \\\\\nP(y=0) &=1-\\sigma(w \\cdot x+b) \\\\\n&=1-\\frac{1}{1+\\exp (-(w \\cdot x+b))} \\\\\n&=\\frac{\\exp (-(w \\cdot x+b))}{1+\\exp (-(w \\cdot x+b))}\n\\end{aligned}\nL(\\beta)=\\prod^n_{i=1}y_iP(Y=y_i|X=x_i;\\beta)\\\\\n\\begin{array}{l}\nlogL(\\beta)=\\sum^n_{i=1}y_ilogP(Y=1|X=x_i;\\beta)+(1-y_i)logP(Y=0|X=x_i;\\beta)\\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\sum^n_{i=1}\ny_ilog(\\frac{1}{1+e^{-\\beta^Tx}})+(1-y_i)log(\\frac{e^{-\\beta^Tx}}{1+e^{-\\beta^Tx}})\\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\sum^n_{i=1}\ny_i(log(\\frac{1}{1+e^{-\\beta^Tx}})-log(\\frac{e^{-\\beta^Tx}}{1+e^{-\\beta^Tx}}))+log(\\frac{e^{-\\beta^Tx}}{1+e^{-\\beta^Tx}})\\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\sum^n_{i=1}\ny_i\\beta^Tx-log(1+e^{\\beta^Tx})\n\\end{array}\n\\begin{array}{ll}\n\\bigtriangledown_\\beta L(\\beta)=\\sum^n_{i=1}(y_i x - \\frac{x_ie^{\\beta^Tx_i}}{1+e^{\\beta^Tx_i}})\\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n=\\sum^n_{i=1}x_i(y_i - \\sigma(\\beta^Tx_i))\n\\end{array}For SGD:\n\n\\begin{array}{ll}\\\\\n\\beta^{(k+1)}=\\beta^{(k)}-\\alpha \\bigtriangledown L(\\beta)\\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n=\\beta^{(k)}-\\sum^{min\\_batch}_{i=1}x_i(y_i - \\sigma(\\beta^Tx_i))\n\\end{array}\n\nTraining: we learn weights w and b using stochastic gradient descent and cross-entropy loss.\n\n3. How to present the word\n\n把所有积极的词搜集起来，尝试用频率对单词进行表达\n\nSuppose \n\n\n\n\\begin{aligned}\np(+|x)=P(Y=1|x) & = s (\\begin{array}{l}\nW {x+b})\n\\end{array} \\\\\n&=s([2.5,-5.0,-1.2,0.5,2.0,0.7]\\dotproduct[3,2,1,3,0,4.19]+0.1) \\\\\n&=s(0.833) \\\\\n&=0.70 \\\\\np(-|x)=P(Y=0 | x) &=1-s(W  x+b) \\\\\n=& 0.30\n\\end{aligned}\n\nClassification problem : \nx: watermelons\ny : which class belongs to\n\n\n\n\n\n\n用拉平的向量来表达这个图像\n\n\n\n用已有的文本去训练出词向量\n\n\n3.1 One-Hot\n\n用单词的在集合的位置来表示单词\n\nProblem1\n同义词的表达，虽然使用上完全相同，但还是有细微差别\n\n\n\nbig和large虽然很相似，但是用法还是有细微的差别，不能完全等同\n\n\n\n字符级别的相似性和词的相似性是有差异的，对于独热形式，使用内积，两个向量的相似性都是0，则无法用内积衡量两个词的相似性\n\n\n\n以前，可以用一个树状结构来构建词向量，每个词都有其对应的上位词，通过比较这两个节点找到共同的父亲所经历的路径，来进行判别\n但是构建一棵合理的树，需要大量专家知识，并且树的结构很好优化；另一方面由于语言变化迅速，这棵树必须经常更新\n\nProblem2\n造成大量的空间浪费\n\n\n3.2 Distributional RepresentationDef:\n低维的，稠密的词向量表达\n\nTurney &amp; Pantel (2010)“If units of text have similar vectors in a text frequency matrix, then they tend to have similar meanings.”\n\n\nWhen a word w appears in a text, its context is the set of words that appear nearby (within a fixed-size window):\n\nUse the many contexts of w to build up a representation of w\n\n用context表示中心词\nExample：\n\n随着国外新冠患者数量的猛增,让原本松了一口气的我们，再次紧张起来。\n\n治疗一列新冠轻症患者的费用在1万元上下。\n\n肆虐的新冠病毒究竟长什么样?\n\n基因测序等研究结果显示，新冠病毒与SARS冠状病毒同属冠状病毒科的β属冠状病毒\n\n\n\n\n\n\nWord Vectors\nWe will build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts.\nNote: word vectors are sometimes called word embeddings. They are a distributed representation.\n\n4. Similarity\n\\begin{aligned}\n&\\text { euclidean }(u, v)=\\sqrt{\\sum_{i=1}^{n}\\left|u_{i}-v_{i}\\right|^{2}} \\\\\n&\\operatorname{cosine}(u, v)=1-\\frac{\\sum_{i=1}^{n} u_{i} \\times v_{i}}{\\|u\\|_{2} \\times\\|v\\|_{2}}\n\\end{aligned}\n\n\n一般先归一化再使度量相似度\n\n使用cosine的原因：\n欧氏距离能够体现个体数值特征的绝对差异，所以更多的用于需要从维度的数值大小中体现差异的分析，如使用用户行为指标分析用户价值的相似度或差异。\n余弦距离更多的是从方向上区分差异，而对绝对的数值不敏感，更多的用于使用用户对内容评分来区分兴趣的相似度和差异，同时修正了用户间可能存在的度量标准不统一的问题（因为余弦距离对绝对数值不敏感）。\n\n【下面举一个例子，来说明余弦计算文本相似度】\nExample：句子A：这只皮靴号码大了。那只号码合适\n句子B：这只皮靴号码不小，那只更合适\n\n怎样计算上面两句话的相似程度？\n\n基本思路是：如果这两句话的用词越相似，它们的内容就应该越相似。因此，可以从词频入手，计算它们的相似程度。\n\n第一步，分词。\n\n\n句子A：这只/皮靴/号码/大了。那只/号码/合适。\n句子B：这只/皮靴/号码/不/小，那只/更/合适。\n\n第二步，列出所有的词。\n\n这只，皮靴，号码，大了。那只，合适，不，小，很\n\n第三步，计算词频。\n\n句子A：这只1，皮靴1，号码2，大了1。那只1，合适1，不0，小0，更0\n句子B：这只1，皮靴1，号码1，大了0。那只1，合适1，不1，小1，更1\n\n第四步，写出词频向量。\n\n　　句子A：(1，1，2，1，1，1，0，0，0)\n　　句子B：(1，1，1，0，1，1，1，1，1)\n\n到这里，问题就变成了如何计算这两个向量的相似程度。我们可以把它们想象成空间中的两条线段，都是从原点（[0, 0, …]）出发，指向不同的方向。两条线段之间形成一个夹角，如果夹角为0度，意味着方向相同、线段重合,这是表示两个向量代表的文本完全相等；如果夹角为90度，意味着形成直角，方向完全不相似；如果夹角为180度，意味着方向正好相反。因此，我们可以通过夹角的大小，来判断向量的相似程度。夹角越小，就代表越相似。\n\n计算结果中夹角的余弦值为0.81非常接近于1，所以，上面的句子A和句子B是基本相似的\n\n\n由此，我们就得到了文本相似度计算的处理流程是:\nOther metrics\nMatching coefficient\n\n\n\\operatorname{matching}(u, v)=\\sum_{i=1}^{n} \\min \\left(u_{i}, v_{i}\\right)\nJaccard distance\n\n\n\\operatorname{jaccard}(u, v)=1-\\frac{\\operatorname{matchin} \\mathbf{g}(u, v)}{\\sum_{i=1}^{n} \\max \\left(u_{i}, v_{i}\\right)}\nDice distance\n\n\n\\text { dice }(u, v)=1-\\frac{2 \\times \\operatorname{matching}(u, v)}{\\sum_{i=1}^{n} u_{i}+v_{i}}\nOverlap\n\n\n\\operatorname{overlap}(u, v)=1-\\frac{\\text { matching }(u, v)}{\\min \\left(\\sum_{i=1}^{n} u_{i}, \\sum_{i=1}^{n} v_{i}\\right)}\n\n证明上诉距离是否满足这个性质\n\n5. Word2Vec Modelldea:\nWe have a large corpus of text\nEvery word in a fixed vocabulary is represented by a vector.\nGo through each position t in the text, which has a center word c and context (“outside”) words o\nUse the similarity of the word vectors for c and o to calculate the probability of o given c (or vice versa)\nKeep adjusting the word vectors to maximize this probability\nMikolov, T., Sutskever, l., Chen, K., Corrado, G.S.and Dean,J., 2013. Distributed representations of words and phrasesand their compositionality.In Advances in neural information processing systems (pp.3111-3119).\n\n\n\nExample: windows and process for computing the ​\n5.1 Two model variants:\nSkip-grams (SG)Predict context (“outside”) words (position independent) given center word\n\nContinuous Bag of Words (CBOW)Predict center word from (bag of) context words\n\n\n6. SG6.1 Example\nThe quick brown fox jumps over the lazy dog\n\n\n\n\nFor each position ​​, predict context words within awindow of fixed size , given center word ;.\n第一个连乘要扫描整个文档，第二个连乘是滑动窗口的长度\n is all variablesto be optimized\n\n\n\\text { Likelihood }=L(\\theta)=\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m \\atop j \\neq 0} P\\left(w_{t+j} \\mid w_{t} ; \\theta\\right)\nThe objective function is the (average) negative log likelihood:\nsometimes called cost or loss function\n\n\n\n\nJ(\\theta)=-\\frac{1}{T} \\log L(\\theta)=-\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m \\atop j \\neq 0} \\log P\\left(w_{t+j} \\mid w_{t} ; \\theta\\right)\nMinimizing objective function ​ Maximizing predictive accuracy\n\n归一化，消除词表长度的影响\n\nNote: 对于一个词，它既可能是中间词，也可能是背景词\n\n\nQuestion: How to calculate ?\nAnswer: We will use two vectors per word w:\n​ when w is a center word\n​​  when w is a context word\n\n\nThen for a center word c and a context word o:\n\n\nP(o \\mid c)=\\frac{\\exp \\left(u_{o}^{T} v_{c}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}(1) Dot product compares similarity of  and ​​.\n\nu^{T} v=u . v=\\sum_{i=1}^{n} u_{i} v_{i}​    Larger dot product = larger probability\n(2) Exponentiation makes anything positive\n(3) Normalize over entire vocabulary to give probability distribution\n\nThis is an example of the softmax function \n\\operatorname{softmax}\\left(x_{i}\\right)=\\frac{\\exp \\left(x_{i}\\right)}{\\sum_{j=1}^{n} \\exp \\left(x_{j}\\right)}=p_{i}\nThe softmax function maps arbitrary values  to a probability distribution \n“max” because amplifies probability of largest ​\n“soft” because still assigns some probability to smaller ​\nFrequently used in Deep Learning\n\n\n\n\n\n6.3 Word2Vec: skip-gram modelfake task\n借助假的分类任务，从而学到与其他任务相似性的参数\n\n\n\n输入一个读热向量，得到10000个神经元（即一个10000维向量），每个神经元对应一个词的概率\n即相当于在做一个给定一个词，预测另一个词的任务\n而这其中学习的隐藏层的参数其实就是词向量，或者说这个学习到的参数与词的概率分布具有同源分布\n\nProcess\n\n\n首先从隐藏层矩阵挑出相应的中心词向量c\n\n\n\n然后用中心词向量（300dim）乘以刚才的隐藏层矩阵，就可以得到（10000dim）对应每个词的概率分布\n\n\n\n接着，我们对于给定窗口长度，分别在其对应的背景词位置使用softmax，从而得到这些背景词的条件概率分布，再使用交叉熵\n\n\n\nCBOW AND SG\n6.4 Problem\n由于分类任务的巨大性（每次要对所有词向量做内积），计算量巨大\n这将会导致\nRunning gradient descent on a neural network that large is going to be slow.\nNeed a huge amount of training data in order to tune that many weights and avoid over fitting.\n\n\n\n\n7. Solusion\nSubsampling frequent words to decrease the number of training examples.\nModifying the optimization objective with a technique they called “Negative Sampling” , which causes each training sample to update only a small percentage of the model’s weights.\nWord pairs and “phases”\n\n7.1 Subsampling\nSubsampling frequent words to decrease the number of training examples.\n\n\n\n对于频率较高的词，不用采样\n\n\n\n自动调整采样的公式，经常出现的单词采样概率小，不常出现的概率大\n\n\n7.2 Negative Sampling\nTraining a neural network means taking a training example and adjusting all of the neuron weights slightly so that it predicts that training sample more accurately.\nNegative sampling addresses this issue by having each training sample only modify a small percentage of the weights , rather than all of them.\n通过缩小最后的output，来使得计算量下降，并使得参数每次变化不会特别大\n\n\n\n\n\nThe “negative samples” are chosen using a “unigram distribution”. Essentially , the probability for selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples.\n\n\n3. Word pairs and “phases”.\nThe authors pointed out that a word pair like “Boston Globe”(a newspaper) has a much different meaning than the individual words “Boston” and “Globe”.So it makes sense totreat “Boston Globe”, wherever it occurs in the text, as asingle word with its own word vector representation.\n\n8. How to evaluate the generated embeddings?\nSmall windows (C=+| 2) : nearest words are syntacticall similar words in same taxonomy 最近的单词是语法相似在相同分类法中的单词\nLarge windows (c= +|- 5): nearest words are related words in samle semantic field 最近词是语义场中的相关词\n\n\n\n距离平移性，通过平移操作的计算，观测生成的距离的相似性\ndoctor-man+woman=nurse\n\n\n\n\n\n可视化，svd，tsvg，压缩到二维图像去观测\n\n\n\n并没有考虑反义词的现象，这有可能使得相似性不能很好地度量语义的相似\n\n\n\n无法很好度量一些层次关系\n\n9. Other interesting things:\n用词向量看文化\n\n不同语言学的语义有差异，可能会带来翻译不一致的问题\n\n\n混合embedding，使得语义更加丰富\n\n\n","categories":["nlp"]},{"title":"sequence labelling and relation extraction","url":"/2021/08/15/nlp%20learning/Chapter6_IE/","content":"sequence labelling and relation extraction\n\n1. IE1.1 Simplily Introduction\n\n\n从有限文本找到相关文本，并从文本收集信息，最后表示出来。\n\nIE systems extract clear, factual information\n\nRoughly: Who did what to whom when?\n\n\nE.g..\nGathering earnings，profits, headquarters, etc. from company reports\nThe headquarters of Alibaba Group, and the global headquarters of the combined Alibaba Group,are located in Hangzhou.\nheadquarters(“Alibaba Group”,”Hangzhou”)\n表达成结构化形式\n\n\n\n\n\n\nLearn drug-gene product interactions from medical research literature\n\n1.2 Low-level information extraction\nIs now available  and I think popular  in applications like textapp. mail app, etc.\n\nOften seems to be based on regular expressions and name lists\n\n\n\n\n1.3 Named Entity Recognition (NER)\nvery important sub-task: find and classify names in text,\nFor example：\n\n\n\n人名、组织/机构名、地理位置、时间/日期字符值、金额值、领域实体\n\n1.4 The uses:\nNamed entities can be indexed,linked off, etc.\nSentiment can be attributed to companies or products.\n归因产品使用的情绪\n\n\n\nA lot of IE relations are associations between named entities.. For question answering, answers are often named entities.\n\n1.5 Concretely:（具体地）\nMany web pages tag various entities, with links to topic pages, etc.\n\n\n\nGooglel Apple…. smart recognizers for document content\n通过命名实体识别，给新闻阅读带来更好的体验，即给检索到的实体加入相应的url链接\n\n\n\n\n\nRecall and precision are straightforward for tasks like IR and text categorization,where there is only one grain size（晶粒尺寸） (documents)\n\nThe measure behaves a bit funnily for IE/NEP. when there are(boundary errors (which are common):\n\n紫金山森林公园位于南京市玄武区.\nfirst Bank of China\n有可能只识别出Bank of China\nThis counts as both a fp and a fn   这将导致fp和fn上升\n\n\n\n\nSelecting nothing would have been better? \n\nSome other metrics (e.g., Muc scorer) give partial credit(according to complex rules)\n\n\n1.6 Sequence Models for Named Entity Recognition\n\nTraining\nCollect a set of representative training documents\nLabel each token for its entity class or other (O)\nDesign feature extractors appropriate to the text and classes\nTrain a sequence classifier to predict the labels from the data\n\n\nTesting/Classifying\nReceive a set of testing documents\nRun sequence model inference to label each token\nAppropriately output the recognized entities\n\n\n\n\n\n第一种encoding会出现边界问题，比如Mengqiu Huang这个人应该是一个整体，但识别时会被分成两部分\nC个类别，那么label有C+1种，对于运算的空间相比于第二种小\n造成这种原因是一下子出现三个PER，无法判别是否能打包成一个实体，但是通常意义上讲，紧挨着的实体不是同一个类别，所以IO对于大样本更适合。\n\n\n第二种，当读到Begin是开始，读到I，则是紧接上一个\n\n有2C+1个label，效率比较低，但带来了更高的准确率\n\n\n更多模型：IOBE,IOBS，但是要考虑训练开销于准确率的权衡\n\n\n1.7 Features1.7.1 Features for sequence labeling 序列标签的特征\nWords\nCurrent word\nPrevious/next word (context)\n\n\nOther kinds of inferred linguistic classification.  语义级别特征、语法级别特征\nPart-of-speech tags\n\n\nLabel context\nPrevious (and perhaps next) label   （的特征）\n\n\n\n1.7.2 Features: Word substrings\n\n只要出现xazo就是药，出现field就是地点，出现冒号就是电影\n这种维度的特征对下游任务十分有效\n\n\n\n1.7.3 Word Shapes\nMap words to simplified representation that encodes attributes such as length, capitalization，numerals ,Greek letters,internal punctuation, etc.\n不同形状的词就蕴含了信息\n\n\n1.8 Maximum entropy Markov models (MEMMs) or Conditional Markov models1.8.1 Sequence problems\nMany problems in NLP have data which is a sequence of characters,words，phrases,lines, or sentences ….\nwe can think of our task as one of labeling each item\n\n\n\n进来一个序列，对每一个文本块进行识别\n\n1.8.2 MEMM Inference in Systems\nFor a Conditional Markov Model(CMM) a.k.a. a Maximum Entropy Markov Model (MEMM), the classifier makes a single decision at a time, conditioned on evidence from observations and previous decisions\nA larger space of sequences is usually explored  via search\n\n1.8.3 Scoring individual labeling decisions is no more complex than standard classification decisions\nWe have some assumed labels to use for prior positions\n\nWe use features of those and the observed data (which can includecurrent, previous, and next words) to predict the current label \n\n\n\n\n\n尽可能保证使用贪心的策略，每次都是使得当前的词最大\n\n1.9 Search1.9.1 Greedy Inference\nGreedy inference:\nWe just start at the left, and use our classifier at each position to assign a label\nThe classifier can depend on previous labeling decisions as well as observed data\n\nAdvantages:\nFast, no extra memory requirements \nVery easy to implement\nWith rich features including observations to the right, it may perform quite well\n\nDisadvantage:\nGreedy. \nWe make commit errors we cannot recover from\n\n1.9.2 Beam Search\nBeam inference:\nAt each position keep the top k complete sequences.\nExtend each sequence in each local way.\nThe extensions compete for the k slots at the next position.\n\nAdvantages:\nFast; beam sizes of 3-5 are almost as good as exact inference in many cases.\nEasy to implement (no dynamic programming required).\n\nDisadvantage:\nInexact: the globally best sequence can fall off the beam.\n\n\n\n1.9.3 Viterbi Inference\niterbi inference:\nDynamic programming or memoization.\nRequires small window of state influence (e.g., past two states are relevant).\n\nAdvantages:\nExact: the global best sequence is returned.\n\nDisadvantage:\nHarder to implement long-distance state-state interactions (but beaminference tends not to allow long-distance resurrection of sequences any way).\n\n1.10 CRFs参考资料：CRF条件随机场的原理、例子、公式推导和应用 - 知乎 (zhihu.com)\n\nAnother sequence model: Conditional Random Fields (CRFs)\nA whole-sequence conditional model rather than a chaining of local models.\n\n\nP(c \\mid d, \\lambda)=\\frac{\\exp \\sum \\lambda_{i} f_{i}(c, d)}{\\sum_{c^{\\prime}} \\exp \\sum_{i} \\lambda_{i} f_{i}\\left(c^{\\prime}, d\\right)}\nThe space of c’s is now the space of sequences\nBut if the features f, remain local,the conditional sequence likelihood can be calculated exactly using dynamic programming\n\n\nTraining is slower, but CRFs avoid causal-competition biases\nThese (or a variant using a max margin criterion) are seen as the state-of-the-art these days … but in practice usually work much the same as MEMMs.\n\n2. Extracting relations from text\nCompany report: “International Business Machines Corporation (IBM or thecompany) was incorporated in the State of New Vork on June 16,1911,as the Computing-Tabulating-Recording Co.(C-T-R.)…”\nExtracted Complex Relation:\nCompany-Founding\nCompany IBM\nLocation New york\nDate June 16,1911\nOriginal-Name Computing-Tabulating-Recording Co.\n\n\nBut we will focus on the simpler task of extracting relation triples\nFounding-year(IBM,1911)\nFounding-location(IBM,New York)\n从文本中抽取出关系\n\n\n\n\n2.1 Extracting relation triples from text\n2.2 Why Relation Extraction?\nNER：find classify——关系最终也会变成一个分类问题\n\nCreate new structured knowledge graphs, useful for any app\n\nAugment current knowledge graphs\n\nAdding words to WordNet thesaurus，facts to FreeBase or DBPedia\n\n\nSupport question answering\n\nThe grand daughter of which actor starred in the movie”E,T.”?(acted-in ?x “E.T.”)(is-a ?y actor)(granddaughter-of ?x?y)\n\n\nBut which relations should we extract?\n\n2.3 Automated Content Extraction (ACE)\n\nPhysical-Located PER-GPE\nHe was in Tennessee  \n\n\nPart-Whole-Subsidiary ORG-ORG\nXYZ, the parent company of ABC\n\n\nPerson-Social-Family   PER-PER\nJohn’ s wife Yoko\n\n\nOrg-AFF-Founder   PER-ORG\nsteve Jobs , co-founder of Apple…\n\n\n\n2.4 UMLS: Unified Medical Language System\n134 entity types, 54 relations\n\n\nExtracting UMLS relations from a sentence\nDoppler echocardiography can be used to diagnose left anterior descending artery stenosis in patients with type 2 diabetes\nEchocardiography, Doppler DIAGNOSES Acquired stenosis\n\n\n\n2.5 Databases of Wikipedia Relations\n3. How to build relation extractors\nHand written patterns\nSupervised machine learning\nSemi supervised and unsupervised\nBootstrapping (using seeds)\nDistant supervision\nUnsupervised learning from the web\n\n\n\n3.1 Rules for extracting IS-A relation\nEarly intuition from Hearst (1992)\n“Agar is a substance prepared from a mixture of red algae（红脂）,such as Gelidium（凝胶）, for laboratory orindustrial use”\n\n\nWhat does Gelidium mean?\nHow do you know?\n\nHearst’s Patterns for extracting IS-A relations\n\n表示a是b的模板 \n“Y such as X((,X)*(, and | or)X)”\n“such Y as X”\n“x or other Y”\n“X and other Y”\n“Y including x”\n“Y,especially X”\n\n\n\n3.1.1 Extracting Richer Relations Using Rules\nIntuition: relations often hold between specific entities. \nlocated-in(ORGANIZATION,LOCATION)\nfounded (PERSON，ORGANIZATION)\ncures (DRUG, DISEASE)\n\n\nStart with Named Entity tags to help extract relation!\n在已经知道命名实体的类别情况下，会很容易知道他们之间的关系\n\n\n\n但这种情况也不一定\n\n\n\nWho holds what office in what organization?\nPERSON , POSITION of ORG\nGeorge Marshall , Secretary of State of the United States\n\n\nPERSON named|appointed|chose| etc PERSON Prep? POSITION\nTruman appointed Marshall Secretary of State\n\n\nPERSON [be]? named|appointed| etc ..) Prep? ORG POSITION\nGeorge Marshall was named US Secretary of State\n\n\n\n\n\n3.1.2 Summary: Hand-built patterns for relationsPlus:\n\nHuman patterns tend to be high-precision. \nCan be tailored to specific domains\n\nMinus\n\nHuman patterns are often low-recall\nA lot of work to think of all possible patterns!. Don’t want to have to do this for every relation!. e’d like better accuracy\n\n3.2 Supervised machine learning for relations\n\nChoose a set of relations we’d like to extract\nChoose a set of relevant named entities\nFind and label data\nChoose a representative corpus\nLabel the named entities in the corpus\nHand-label the relations between these entities\nNLP标注，最终导出csv等结构化数据\n\n\nBreak into training, development , and test\n\n\n\nStep\nFind all pairs of named entities (usually in same sentence)\n\nDecide if  entities are related\n\n先看有没有关系，如果没有关系直接过滤掉\n\n\nIf yes,classify the relation\n\n\nWhy the extra step?\n有没有关系——局部特征，从而使得把特征进行打包，在进行分类\n\nFaster classification training by eliminating most pairs.\n\nCan use disjist（不相容） feature-sets appropriate for each task.\n\n\n\n对于每个句子进行实体识别后，抽取成相关的一条数据集如上\n\n\n\n\n\n通过句法的依赖关系，得到实体之间的依存关系，如上\n\n3.3 Gazeteer and trigger word features for relation extraction\nTrigger list for family: kinship terms\nparent , wife, husband, grandparent, etc. [from WordNet]\n\n\nGazeteer:\n\nLists of useful geo or geopolitical words\nCountry name list\nOther sub-entities\n\n\n\n\nAmerican Airlines, a unit of AMR, immediately matched the move, spokesman Tim wagner said.\n\n\n\n3.4 Classifiers for supervised methods\nNow you can use any classifier you like\n\nMaxEnt\nNaive Bayes. \nSVM\n\n\nTrain it on the training set, tune on the dev set, test on the test set\n\n\n3.5 Summary: Supervised Relation ExtractionPlus:\n\nCan get high accuracies with enough hand-labeled training data,if test similar enough to training\n\nMinus:\n\nLabeling a large training set is expensive\n\nSupervised models are brittle,don’t generalize well to different genres\n\n\n","categories":["nlp"]},{"title":"IR","url":"/2021/08/15/nlp%20learning/Chapter8_IR/","content":"IR\n\n1. HMM Exercise\nH M M : \\hat{g}=\\underset{y \\in Y}{\\operatorname{argmax}} P(x, y)\nTransition\n\n\n\\left.\\begin{array}{r}N\\rightarrow V \\\\ \\downarrow \\\\ c\\end{array}\\right\\} 9\\quad \\left.\\begin{array}{r}P\\rightarrow V \\\\ \\downarrow \\\\ a\\end{array}\\right\\} 9\n\\left.\\begin{array}{r}N-D \\\\ \\downarrow \\\\ a\\end{array}\\right\\} 1\nP(V \\mid N)=\\frac{9}{9+1}=\\frac{9}{10}\nP(D \\mid N)=0.1\nEmission\n\n\nP(a\\mid v)=0.5 \\quad P(a\\mid D)=1\nfor :\n\n\n\\begin{array}{r}N\\rightarrow ? \\\\ \\downarrow \\\\ a\\end{array}\n\\begin{aligned}\n?=& \\underset{i \\in\\{V, D\\}}{\\operatorname{argmax}} \\alpha P(i \\mid N) P(a \\mid i) \\\\\n=& \\operatorname{argmax}\\{(0.9 \\times 0.5) \\alpha, (0.1 \\times 1)\\alpha\\} \\\\\n=& V\n\\end{aligned}\n分析原因：因为HMM做了局部归一化，导致HMM更容易转移到具有比较少转移状态的状态。E:/third\n\n我们可以看到hmm的状态不容易进行跳转\n\n\n2. CRF vs. HMM\n\\begin{array}{l}\nH M M: P(\\vec{y}, \\vec{x})=P\\left(y_{1} \\mid \\operatorname{start} \\right)\\prod_{i=1}^{n-1} P\\left(y_{i-1} \\mid y_{i}\\right) \nP\\left(\\operatorname{end} \\mid y_{n}\\right) \\prod_{i=1}^{n} P\\left(x_{i} \\mid y_{i}\\right) \\\\\n\\log P(x, y)=\\log P(y_1 \\mid \\text { start })+\\sum_{i=1}^{n-1} \\log P\\left(y_{i-1} \\mid y_{i}\\right)\\\\\n+\\log P\\left(\\text { end } \\mid y_{n}\\right)+\\sum_{i=1}^{n} \\log P\\left(x_{i} \\mid y_{i}\\right) \\\\\n\\end{array}\nwe can easily get the equation as flowing:\n\n\n\\sum_{i=1}^{n} \\log P\\left(x_{i} \\mid y_{i}\\right)=\\sum_{s, t} \\log P(t \\mid s) \\cdot N_{s t}(x, y), \\quad\ns.t:\\text { s: tags},\\quad t=\\text { words }\nwhy?\n\nExample:\n\n​ : the dog ate the homework \n\n\n\n\\begin{array}{l}\nN_{D, \\text{the}}(x, y)=2 \\\\\nN_{N, \\text{dog}}(x, y)=1 \\\\\nN_{V, \\text{ate}}(x, y)=1 \\\\\nN_{N, {\\text {homework }}(x, y)=1} \\\\\nN_{s, t}(x, y)=0, \\quad\\text {(s,t) is other}\n\\text { combination }\n\\end{array}\n\\begin{array}{l}\n\\underset{i=1}{\\overset{n}\\sum}\\log P(x_i\\mid y_i)&=\\quad\\log \nP(\\text{the}\\mid\\text{D})+\\log P(\\text{dog}\\mid\\text{N})\\\\&+\\log P(\\text{ate}\\mid\\text{V})+\\log P(\\text{the}\\mid\\text{D})+\\log P(\\text{homework}\\mid\\text{N})\\\\\n&=\\quad \\log \nP(\\text{the}\\mid\\text{D})\\times 2+\\log P(\\text{dog}\\mid\\text{N})\\times 1+\\log P(\\text{ate}\\mid\\text{V})\\times 1\\\\&+\\log P(\\text{homework}\\mid\\text{N})\\times 1\\\\\n&=\\quad \\underset{s,t}{\\sum}\\log P(t\\mid s)\\cdot N(x,y)\n\\end{array}\n同理，我们可以对式子的第一、二、三项做变化：\n\n\n\\log P(y_1\\mid start)=\\underset{s}{\\sum}\\log P(t\\mid s)\\cdot N_{\\text{start,s}}(x,y)\n\\sum_{i=1}^{n-1} \\log P\\left(y_{i-1} \\mid y_{i}\\right)=\\underset{s,s'}{\\sum}\\log P(s'\\mid s)\\times N_{s,s'}(x,y)\n\\log P\\left(\\text { end } \\mid y_{n}\\right)=\\underset{s}{\\sum}log P(end\\mid s)\\times N_{s,end}(x,y)\n所以，我们可以对原式做变换：\n\n\n\\begin{array}{l}\n\\log P(x, y)&=&\\underset{s}{\\sum}\\log P(t\\mid s)\\cdot N_{\\text{start,s}}(x,y)+\\underset{s,s'}{\\sum}\\log P(s'\\mid s)\\times N_{s,s'}(x,y)\\\\&+&\\underset{s}{\\sum}log P(end\\mid s)\\times N_{s,end}(x,y)+\\underset{s,t}{\\sum}\\log P(t\\mid s)\\times N(x,y)\\\\\n&=& \\left[\\begin{array}{c}\n\\log p(t \\mid s) \\\\\n\\vdots \\\\\n\\log p(s \\mid \\text { start }) \\\\\n\\vdots \\\\\n\\log P\\left(s^{\\prime} \\mid s\\right) \\\\\n\\vdots \\\\\n\\log P(\\text { end } \\mid s)\\\\\n\\vdots\n\\end{array}\\right]^{t} \\cdot\\left[\\begin{array}{c}\nN_{s, t}(x, y) \\\\\n\\vdots \\\\\nN_{s t a r t,s}{(x, y)} \\\\\n\\vdots \\\\\nN_{s, s^{\\prime}}(x, y) \\\\\n\\vdots \\\\\nN_{s, \\text { end }}(x, y) \\\\\n\\vdots\n\\end{array}\\right]\\\\\n&=& w^t \\psi(x,y)\n\\end{array}\n对于CRF而言​是可以学习的参数\n长度：tagswords(sL), tags​​ tages(s\\ +2s(start and end))\n\n训练时，最大化后验概率\n\n\n\nw^*=\\underset{w}{\\operatorname{argmax}}P(\\vec{y}\\mid\\vec{x})=\\underset{w}{\\operatorname{argmax}}\\frac{P(\\vec{x},\\vec{y})}{\\underset{y'}{\\sum}P(x,y')}\n取log后：\n\n\nw^*=\\underset{w}{\\operatorname{argmax}}\\log P(x^n\\mid \\hat{y}^n)-\\sum_{y'}P(y'\\mid x^n)P(x^n,y')\n根据梯度上升：\n\n\n\\theta\\rightarrow\\theta+\\eta\\nabla(\\theta)\n\\frac{\\partial \\theta(w)}{\\partial w_{s,t}}=N_{s,t}(x^n,y^n)-\\sum_{y'}P(y'\\mid x^n)N_{s,t}(x^n,y^n)3. CRFE:/third\n\n假设\n\n\nP(\\boldsymbol{y}, \\boldsymbol{x})=f(\\boldsymbol{y}, \\boldsymbol{x})=h\\left(y_{1} , \\boldsymbol{x}\\right) +g\\left(y_{1}, y_{2}\\right)+h\\left(y_{2} , \\boldsymbol{x}\\right)+\\ldots \n+h\\left(y_{n} , \\boldsymbol{x}\\right)+g\\left(y_{n-1}, y_{n}\\right)\n其实就是表示成边的条件概率以及状态条件概率\n则可以计算后验概率：\n\n\nP(\\boldsymbol{y}|\\boldsymbol{x})=\\frac{1}{Z(x)}exp\\left(\\underset{i,k}{\\sum}\\lambda_k t_k(y_{i-1}, y_i,x,i)+\\underset{i,l}{\\sum}u_l t_l(y_{i-1}, y_i,x,i)\\right)\nZ(\\boldsymbol{x})=\\sum_\\boldsymbol{y'}exp\\left(\\underset{i,k}{\\sum}\\lambda_k t_k(y'_{i-1}, y'_i,x,i)+\\underset{i,l}{\\sum}u_l t_l(y'_{i-1}, y'_i,x,i)\\right)\n假设有个转移特征，个状态特征，，记：\n\n\nf_{k}\\left(y_{i-1}, y_{i}, x, i\\right)=\\left\\{\\begin{array}{ll}\nt_{k}\\left(y_{i-1}, y_{i}, x, i\\right), & k=1,2, \\cdots, K_{1} \\\\\ns_{i}\\left(y_{i}, x, i\\right), & k=K_{1}+l ; l=1,2, \\cdots, K_{2}\n\\end{array}\\right.\n然后对转移与状态特征在各个位置求和：\n\n\nf_{k}(y, x)=\\sum_{i=1}^{n} f_{k}\\left(y_{i-1}, y_{i}, x, i\\right), \\quad k=1,2, \\cdots, K\n用表示特征的权值：\n\n\nw_{k}=\\left\\{\\begin{array}{ll}\n\\lambda_{k}, & k=1,2, \\cdots, K_{1} \\\\\n\\mu_{l}, & k=K_{1}+l ; l=1,2, \\cdots, K_{2}\n\\end{array}\\right.\n于是条件随机场可以用以下式子表示：\n\n\n\\begin{aligned}\nP(y \\mid x) &=\\frac{1}{Z(x)} \\exp \\sum_{k=1}^{K} w_{k} f_{k}(y, x) \\\\\nZ(x) &=\\sum_{y‘} \\exp \\sum_{k=1}^{K} w_{k} f_{k}(y’, x)\n\\end{aligned}\n为权值向量：\n\n\nw=\\left(w_{1}, w_{2}, \\ldots, w_{K}\\right)^{\\mathrm{T}}\n以表示全局特征向量即：\n\n\nF(y, x)=\\left(f_{1}(y, x), f_{2}(y, x), \\cdots, f_{K}(y, x)\\right)^{\\mathrm{T}}\n则条件随机场可以写成 与  的内积的形式:\n\n\nP_{w}(y \\mid x)=\\frac{\\exp (w \\cdot F(y, x))}{Z_{w}(x)}\n其中\n\n\nZ_{w}(x)=\\sum_{y'} \\exp (w \\cdot F(y', x))3.1 Inference:\n\\mathbf{y}_{\\text {best }}=\\operatorname{argmax}_{\\mathbf{y}^{\\prime}} \\exp \\left(\\sum_{k=1}^{n} w^{\\top} f_{k}\\left(\\mathbf{x}, \\mathbf{y}^{\\prime}\\right)\\right)\nlf y consists of 5 variables with 30 values each, how expensive are these?Need to \nconstrain the form of our CRFs to make it tractableE:/third\nP(\\mathbf{y} \\mid \\mathbf{x}) \\propto \\prod_{k} \\exp \\left(\\phi_{k}(\\mathbf{x}, \\mathbf{y})\\right)\n\n\nP(\\mathbf{y} \\mid \\mathbf{x}) \\propto \\exp \\left(\\phi_{o}\\left(y_{1}\\right)\\right) \\prod_{i=2}^{n} \\exp \\left(\\phi_{t}\\left(y_{i-1}, y_{i}\\right)\\right) \\prod_{i=1}^{n} \\exp \\left(\\phi_{e}\\left(x_{i}, y_{i}\\right)\\right)E:/thirdE:/third\n\n因为最好不要随意依赖，所以我们要给它加位置信息E:/third\n\nNotation: omit  from the factor graph entirely (implicit)\n\nDon’t include initial distribution, can bake into other factors\nSequential CRFs:\n\n\n\\begin{gathered}\nP(\\mathbf{y} \\mid \\mathbf{x})=\\frac{1}{Z} \\prod_{i=2}^{n} \\exp \\left(\\phi_{t}\\left(y_{i-1}, y_{i}\\right)\\right) \\prod_{i=1}^{n} \\exp \\left(\\phi_{e}\\left(y_{i}, i, \\mathbf{x}\\right)\\right) \\\\\nP(\\mathbf{y} \\mid \\mathbf{x}) \\propto \\exp w^{\\top}\\left[\\sum_{i=2}^{n} f_{t}\\left(y_{i-1}, y_{i}\\right)+\\sum_{i=1}^{n} f_{e}\\left(y_{i}, i, \\mathbf{x}\\right)\\right]\n\\end{gathered}3.2 Inference\n\n\\begin{aligned}\n& \\max _{y_{1}, \\ldots, y_{n}} e^{\\phi_{t}\\left(y_{n-1}, y_{n}\\right)} e^{\\phi_{e}\\left(y_{n}, n, \\mathbf{x}\\right)} \\cdots e^{\\phi_{e}\\left(y_{2}, 2, \\mathbf{x}\\right)} e^{\\phi_{t}\\left(y_{1}, y_{2}\\right)} e^{\\phi_{e}\\left(y_{1}, 1, \\mathbf{x}\\right)} \\\\\n=& \\max _{y_{2}, \\ldots, y_{n}} e^{\\phi_{t}\\left(y_{n-1}, y_{n}\\right)} e^{\\phi_{e}\\left(y_{n}, n, \\mathbf{x}\\right)} \\cdots e^{\\phi_{e}\\left(y_{2}, 2, \\mathbf{x}\\right)} \\max _{y_{1}} e^{\\phi_{t}\\left(y_{1}, y_{2}\\right)} \\underbrace{e^{\\phi_{e}\\left(y_{1}, 1, \\mathbf{x}\\right)}}\\\\\n=&\\max _{y_{3}, \\ldots, y_{n}} e^{\\phi_{t}\\left(y_{n-1}, y_{n}\\right)} e^{\\phi_{e}\\left(y_{n}, n, \\mathbf{x}\\right)} \\cdots \\max _{y_{2}} e^{\\phi_{t}\\left(y_{2}, y_{3}\\right)} \\underbrace{e^{\\phi_{e}\\left(y_{2}, 2, \\mathbf{x}\\right)} \\max _{y_{1}} e^{\\phi_{t}\\left(y_{1}, y_{2}\\right)} \\operatorname{score}_{1}\\left(y_{1}\\right)}\\\\\n=&\\max _{y_{3}, \\ldots, y_{n}} e^{\\phi_{t}\\left(y_{n-1}, y_{n}\\right)} e^{\\phi_{e}\\left(y_{n}, n, \\mathbf{x}\\right)} \\cdots \\max _{y_{2}} e^{\\phi_{t}\\left(y_{2}, y_{3}\\right)}score_2(y_2)\\\\\n=&\\max _{y_n}score_n(y_n)\n\\end{aligned}\nscore_i(y_i)=\\left\\{\\begin{array}{cc}\ne^{\\phi_{e}\\left(y_{i}, i, \\mathbf{x}\\right)},\\quad\\text{i=1}\\\\\ne^{\\phi_{e}\\left(y_{i}, i, \\mathbf{x}\\right)} \\max _{y_{i-1}} e^{\\phi_{t}\\left(y_{i-1}, y_{i}\\right)} \\operatorname{score}_{i-1}\\left(y_{1}\\right),\\quad\\text{i$\\neq$ 1}\n\\end{array}\\right.3.3 Training\nLogistic regression: \nMaximize $\\mathcal{L}\\left(\\mathbf{y}^{}, \\mathbf{x}\\right)=\\log P\\left(\\mathbf{y}^{} \\mid \\mathbf{x}\\right)$\nGradient is completely analogous to logistic regression:\n\n\n\\frac{\\partial}{\\partial w} \\mathcal{L}\\left(\\mathbf{y}^{*}, \\mathbf{x}\\right)=\\sum_{i=2}^{n} f_{t}\\left(y_{i-1}^{*}, y_{i}^{*}\\right)+\\sum_{i=1}^{n} f_{e}\\left(y_{i}^{*}, i, \\mathbf{x}\\right)\n\\text { intractable } \\quad-\\mathbb{E}_{\\mathbf{y}}\\left[\\sum_{i=2}^{n} f_{t}\\left(y_{i-1}, y_{i}\\right)+\\sum_{i=1}^{n} f_{e}\\left(y_{i}, i, \\mathbf{x}\\right)\\right]\nforward backward Algorithm\n\n\n\\frac{\\partial}{\\partial w} \\mathcal{L}\\left(\\mathbf{y}^{*}, \\mathbf{x}\\right)=\\sum_{i=1}^{n} f_{e}\\left(y_{i}^{*}, i, \\mathbf{x}\\right)-\\sum_{i=1}^{n} \\sum_{s} P\\left(y_{i}=s \\mid \\mathbf{x}\\right) f_{e}(s, i, \\mathbf{x})\n拟牛顿法：\n\n\nP_{w}(y \\mid x)=\\frac{\\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)}{\\sum_{y} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)}\n学习优化目标：\n\n\n\\min _{w \\in \\mathcal{R}^{n}} f(w)=\\sum_{x} \\tilde{P}(x) \\log \\sum_{y} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)-\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)\n梯度函数：\n\n\ng(w)=\\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) f(x, y)-E_{\\tilde{p}}(f)\nBFGS算法：\n输入：特征函数;经验分布\n输出：最优化参数值；最优化模型\n(1) 选的初始点​,取​为正定对称矩阵，置\n(2) 计算 ​ 若 ​, 則停止计算: 否则转 (3)(3) 戈 ​ 求出 ​(4) 以为搜索: 求 使得：​\n\n\n\nf\\left(w^{(k)}+\\lambda_{k} p_{k}\\right)=\\min _{\\lambda \\geqslant 0} f\\left(w^{(k)}+\\lambda p_{k}\\right)​      (5) 置​\n​      (6) 计算，若，则停止计算；否则，按下式计算\n\nB_{k+1}=B_{k}+\\frac{y_{k} y_{k}^{\\mathrm{T}}}{y_{k}^{\\mathrm{T}} \\delta_{k}}-\\frac{B_{k} \\delta_{k} \\delta_{k}^{\\mathrm{T}} B_{k}}{\\delta_{k}^{\\mathrm{T}} B_{k} \\delta_{k}}​      其中，\n\ny_{k}=g_{k+1}-g_{k}, \\quad \\delta_{k}=w^{(k+1)}-w^{(k)}​     (7) 査 , 转 (3).\n4. Information Retrieval4.1 Introduction\nInformation Retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers)\n\n4.2 Term-document incidence matricesE:/third\n4.3 Incidence vectors\nSo we have a 0/1 vector for each term.\n\nTo answer query: take the vectors for Brutus, Caesar and Calpurnia (complemented)(反码) ​​​ bitwise AND\n\n110100  AND 110111 AND 101111 = 100100E:/third\n\n\n4.4 Problem: Can’t build the matrix\nConsider  million documents, each with about 1000 words.\nAvg 6 bytes/word including spaces/punctuation\n6GB of data in the documents.\n\n\nSay there are ​ distinct terms among these.\n​ matrix has half a trillion 0’s and 1’s.\nBut it has no more than one billion 1’s\nmatrix is extremely sparse\n\n\nWhat’s a better representation?\nWe only record the 1 positions.\n\n\n\n4.5 Inverted index\nFor each term t , we must storkkkkke a list of all documents that contain t\nIdentify each doc by a docID , a document serial number\n\n\nCan we used fixed size arrays for this?E:/thirdE:/third\n\n4.5  Initial stages of text processing\nTokenization\nCut character sequence into word tokens\nDeal with “ John’s”, a state of the art solution\n\n\n\n\nNormalization\nMap text and query term to same form\nYou want U.S.A. and USA to match\n\n\n\n\nStemming\nWe may wish different forms of a root to match\nauthorize authorization\n\n\n\n\nStop words\nWe may omit very common words (or not)\nthe, a, to, of\n\n\n\n\n\n4.5.1 Indexer steps: Token sequence\nSequence of (Modified token, Document ID) pairs.E:/third\n\n4.5.2 Indexer steps: Sort\nSort by terms\nAnd then docID\nCore indexing stepE:/third\n\n\n\n4.5.3 Indexer steps: Dictionary &amp; Postings\nMultiple term entries in a single document are merged.\nSplit into Dictionary and Postings\nDoc. frequency information is added.E:/third\n\n4.5.4 Where do we pay in storage? Pointers TermsE:/third\n4.6 Query processing with an inverted index4.6.1 Query processing: AND\nConsider processing the query:\nBrutus AND Caesar\nLocate Brutus in the Dictionary;\nRetrieve its postings.\n\n\nLocate Caesar in the Dictionary;\nRetrieve its postings.\n\n\n“Merge” the two postings (intersect the document sets):E:/third\n\n\n\n\n\nThe merge\n\nWalk through the two postings simultaneously, in time linear in the total number of postings entriesE:/third\n\nIf the list lengths are  and  , the merge takes  operations.E:/third\n\n\n4.7 The Boolean Retrieval Model &amp; Extended Boolean Models\nExercise : Adapt the merge for the\n\nBrutus AND NOT Caesar\nBrutus OR NOT Caesar\n\n\nCan we still run through the merge in time ? What can we achieve?\n\nWhat about an arbitrary Boolean formula? \n\n(Brutus OR Caesar) AND NOT (Antony OR Cleopatra)\n\n\nCan we always merge in “linear” time?\n\nLinear in N(N is the total posting size)\n\n\nCan we do better?\n\n4.8 Query optimization4.8.1 merge\nQuery: Brutus AND Calpurnia AND CaesarE:/third\n\n从最少频数的两个query开始合并\n\nProcess in order of increasing freq\n\nstart with smallest set, then keep cutting furtherE:/third\n\n\n\n4.8.2 More general optimization\ne.g., madding OR crowd ) AND ignoble OR strife\nGet doc. freq.’s for all terms.\nEstimate the size of each OR by the sum of its doc. freq.’s (conservative)\nProcess in increasing order of OR sizes.\n\n","categories":["nlp"]},{"title":"复习课","url":"/2021/08/15/nlp%20learning/review/","content":"复习课\n\n复习课大类别按照复习课顺序而非上课顺序。\n\nClassification\n\nbag of words representation\n\nNaive Bayes\n\nsmooth\n\n\nQA\n\nHistory\nparadigms of QA\nIR-Based QA\nKB-Based QA\nPattern Based\nParsing Based\n\n\n\n\nChatBot\n\n画框图\n\n\nInformation Extraction\n\nNER\nSequence labeling\n\n\nHMM\n\n三种算法\nPOS tagging\n\n\n\n1. 文本分类\n从bag of word这种基于词频的内容开始，统计文档中的某些词的词频对文档进行分类。\n统计文档的单词，并获得有关文档的字典。 然后我们可以从文档中获取每个单词的频率。一般我们只选取有用的词来统计。\n\n\n\n采用的方法是朴素贝叶斯方法。如下是朴素贝叶斯的公式：\n\n\n\n根据上面的公式，我们可以对有某些特征的物品进行类别分类，如下是一个水果分类问题。\n\n\n\n存在着有的特征可能没有值的情况，比如上一题里面Orange里面就没有Long。可以用拉普拉斯平滑的方式解决。\n训练过程，首先建立词典，然后计算先验，即统计所有文档中，类别为的文档频率。然后计算类条件概率，即先统计文档的所有词数，然后统计文档中的个数。\n测试过程中\n\n\n\n注意：如果题目说要统计unk词，注意词表数+1\n\n\n\\begin{aligned}\n\\hat{P}\\left(w_{u} \\mid c\\right) &=\\frac{\\operatorname{count}\\left(w_{u}, c\\right)+1}{\\left(\\sum_{w \\in V} \\operatorname{count}(w, c)\\right)+|V+1|} \\\\\n&=\\frac{1}{\\left(\\sum_{w \\in V} \\operatorname{count}(w, c)\\right)+|V+1|}\n\\end{aligned}\n总而言之，是否需要平滑，需要看题目要求\n将上述分类的方法部署到自然语言领域：\n\n要求：计算\n\n\n\n\\begin{gathered}\nF_{\\beta}=\\frac{\\left(\\beta^{2}+1\\right) P R}{\\beta^{2} P+R} \\\\\n\\mathrm{~F}_{1}=\\frac{2 P R}{P+R}\n\\end{gathered}2. QA\n问答系统的发展历史：\n\n1961年,BASEBALL\n2003年，TAP Search\n2010年前后：Wolfram Alpha，Siri，Watson\n2012,Google的蜂鸟算法\n2015RankBrain\n2019智能音箱\n\n\nQA的分类：\n\n基于IR的方法\n\n基于知识图谱的方法\n\n基于混合的方法\n\n\n\n要求：需要非常熟悉地掌握几类方法的设计模式以及优缺点\n比如，基于IR的QA需要知道代表工作有哪些。基于IR流程设计的QA系统,主要分成三步：\n\n用知识图谱的QA可以解决的问题是语义鸿沟和词法鸿沟。基于知识图谱的语义解析的方法分为几类，基于知识图谱端到端的方法分为几类。如下是基于知识图谱的QA系统构建方法，也是Siri的构建方法：\n\n下面是介绍基于IR的QA系统的具体的设计流程，包含了很多的IR的思想 ：\n需要知道每个大框架里面是什么，每个小的步骤是什么。\n\nQuestion Processing:\n\n需要详细了解每个小的框图里面有什么内容：\n23.2 Factoid Question Answering 819页进行了讲解\n基于模板的QA也是重要的，模板的生成以及实例化，让写一个模板生成的架构应该也是可以写出来的：\n包含两步：\nStep 1: Template generation linguistics processing\nStep 2: Template matching and instantiation NER\n让你写一个模板生成的架构应该也是可以写出来的。\n\n基于parsing的架构图，下面的图是非常重要的，因为包含了各个步骤，包括短语映射，资源映射，语义组合，查询生成。\n\n会出现两个主要问题：词法鸿沟和语义鸿沟。词法鸿沟是指文本到知识图谱之间的映射需要定义，语义鸿沟指的是查询时候的问题（这里没讲明白）。我们需要想到怎么去解决这些方法，不管是用bert还是怎么的。\n\n出现效果较差的原因：一个是由数据引起的，数据错的，和数据是不完整和缺失的。查询的话包括了对查询的错误理解，过于严格的查询，失败的排序算法等。\n只需要了解即可\n\n不同空间不同任务：推理，查询，加速。\n\n3. ChatBot相比较QA增加了两个模块，对于对话的管理，以及对话策略的执行。\n平时的聊天机器人主要是两个状态：一个是基于领域的，封闭的。比如帮你去订酒店，订机票什么的。还有一个就是开放式的。封闭的是基于规则模板，统计组件或者是一些Deep Learning的方法。\n\n考试的时候会让画框图，但是注意不要画bert，画bert后面的组件没法说。像上面的就方便说每一个组件是干什么的。首先是对语音的理解， 状态追踪（需要很清楚是干什么的），对话策略的执行与学习，最后是进行生成。同时还有一个知识库，知识库的作用是什么？\n\n\n在设计系统的时候从整门课的角度去思考，这门课主要两个部分，自然语言的理解，自然语言的生成。前面的技术包括POS，NER,等。\n4. 信息的抽取找到文本，收集information，这些信息是服务于人和算法的。\n\n两个核心任务，NER的识别关系的提取。这个PPT比较老了。\n需要关注一个新的方向：事件抽取。除了NER和关系抽取外，说一个信息抽取其他的任务。比如多模态知识抽取。最近的任务就是在关注事件抽取，需要了解概念\nNER需要识别人名，地名，组织名之类的。\n\n\n核心的思想还是原来的那个训练模型的思路：\n\n需要细化，知道怎么标注，怎么训练。比如标志位，常见的Feature，常见的分类器，可以使用最简单的隐马尔科夫模型。比如出现一个隐马尔科夫的题目。下面可能会有一个NER的题目，如果不会做可以直接用隐马尔科夫。但是需要弄清楚的是怎么用分类器去做。目标函数，每一步过程与作用是什么。不需要隐马尔科夫的推导过程，但是需要知道怎么HMM是怎么解码的。\n\n\n\n做关系的抽取也是一样的。构建合理的分类器并且构建合理的feature。\n\n\n整个课程有三个地方涉及到计算，一个是贝叶斯，一个是HMM 和维特比算法涉及到，并且还有一个作业。TFIDF，隔壁班级没有讲（但是看他们的ppt上面是有的）\n\n5. HMM\n推导需要完全熟练的掌握，输入是什么输出是什么\n\n\n5.1 Maximun a posterior inference(MAP inference)\n\nHMM有两个独立性假设：\n观测序列之间是独立的 (A B 独立 有P(A, B) = P(A)P(B)，所以计算联合概率的时候才能用叠乘 )\n当前状态仅依赖于先前的状态\n\n\nNumber of states = K, Number of observations = M\n：Initial probabilities over states (K*K matrix)\n：Transition probabilities (K*M matrix)\n\nInput , Output ​ that corresponds to\n\n\n\nargmax_\\bold{y}P(\\bold{y}|\\bold{x},\\pi,A,B)=argmax_\\bold{y}P(\\bold{y},\\bold{x},\\pi,A,B)\n\\begin{array}{ll}\nP(\\bold{y},\\bold{x})\n&=P(y_1,\\ldots,y_n,x_1,\\ldots,x_n)\\\\\n&=P(\\bold{x}|\\bold{y})P(\\bold{y})\\\\\n&=(\\prod^n_{i=1}P(x_i|y_i))\\\\\n&\\text{注释：这里用了条件独立假设，以及可见符号的概率只与当前状态有关}\\\\ \n&\\times P(y_1)P(y_2|y_1)P(y_3|y_1,y_2)\\ldots P(y_n|y_1,\\ldots,y_{n-1})\\\\\n&=(\\prod^n_{i=1}P(x_i|y_i))P(y_1)\\prod^n_{i=2}P(y_i|y_1,\\ldots,y_{i-1})\\\\\n&=(\\prod^n_{i=1}P(x_i|y_i))P(y_1)\\prod^n_{i=2}P(y_i|y_{i-1})\\\\\n&\\text{注释：这里用了隐马尔可夫一阶假设，即当前状态的概率只与上一个状态有关}\\\\\n&=P(y_1)\\prod^n_{i=2}P(y_i|y_{i-1})\\prod^n_{i=1}P(x_i|y_i)\n\\end{array}不同模型的描述：\n\n5.2 DP推导：\n\\begin{array}{ll}\n\\max_{y_1,\\ldots,y_n} P(\\bold{y},\\bold{x})\n&=\\max_{y_1,\\ldots,y_n}P(y_1,\\ldots,y_n,x_1,\\ldots,x_n)\\\\\n&=\\max_{y_1,\\ldots,y_n}P(y_1)\\prod^n_{i=2}P(y_i|y_{i-1})\\prod^n_{i=1}P(x_i|y_i)\\\\\n&=\\max_{y_1,\\ldots,y_n}P(y_n|y_{n-1})y(x_n|y_n)\\ldots P(y_2|y_1)P(x_2|y_2)P(y_1)P(x_1|y_1)\\\\\n&=\\max_{y_2,\\ldots,y_n}P(y_n|y_{n-1})y(x_n|y_n)\\ldots P(y_3|y_2)P(x_3|y_3)\\max_{y_1}P(y_2|y_1)P(x_2|y_2)P(y_1)P(x_1|y_1)\\\\\n&=\\max_{y_2,\\ldots,y_n}P(y_n|y_{n-1})y(x_n|y_n)\\ldots P(y_3|y_2)P(x_3|y_3)\\max_{y_1}P(y_2|y_1)P(x_2|y_2)Score(y_1)\\\\\n&\\text{Initialize: } score_1(y_1)=P(y_1)P(x_1|y_1)\\\\\n&=\\max_{y_3,\\ldots,y_n}P(y_n|y_{n-1})y(x_n|y_n)\\ldots P(y_4|y_3)P(x_4|y_4) \\max_{y_2}(P(y_3|y_2)P(x_3|y_3)\\max_{y_1}P(y_2|y_1)P(x_2|y_2)Score(y_1))\\\\\n&=\\max_{y_3,\\ldots,y_n}P(y_n|y_{n-1})y(x_n|y_n)\\ldots P(y_4|y_3)P(x_4|y_4)\\max_{y_2}(P(y_3|y_2)P(x_3|y_3)Score(y_2))\\\\\n&\\text{Update: }score_2(y_2)=\\max_{y_1}P(y_2|y_1)P(x_2|y_2)Score(y_1)\\\\\n&=max_{y_n}score(y_n)\\\\\n\\end{array}\nscore(y_i)=\n\\left\\{\n\\begin{array}{cc}\nP(y_i)P(x_i|y_i),&\\text{i=1}\\\\\n\\max_{y_{i-1}}P(y_i|y_{i-1})P(x_i|y_i)Score(y_{i-1}),&\\text{i=2,$\\ldots$,n}\n\\end{array}\n\\right.\n\n不用动态规划前，算法复杂度为，即要遍历的路径数量，使用动态规划后变为​，即对于每次迭代只需要计算k个联合概率，每个联合概率需要计算k次乘法，而迭代数为n次，所以时间复杂度如上\n\n5.3 Vitebri Algorithm\n​\nInitialization\nfor each hidden  ​​​\n\n\nRecursion\nfor t = 2 to n, for each :\n\n即t时刻隐藏状态j的联合概率为上一个状态转移的最大值所激发可见符号的概率\n\n\n​\n找到路径\n\n\n\n\nEnd\n\n\n​​\nfor t=n-1 to 1(path back tracking)\n$w^(t)=\\psi_{w^(t+1)}(t+1)$\n\n\nend\n\n5.4 Supervised learning details\n can be estimated separately just by counting\n\ns denotes label, x denotes word, n denotes the number of total words\n\nInitial prob\n\n\n\\pi_s=\\frac{count(start\\rightarrow s)}{n}\nTransition prob\n\n\nA_{s',s}=\\frac{count(s\\rightarrow s')}{count(s)}\nEmission prob\n\n\nB_{s,x}=\\frac{count\\left(\n\\begin{array}{c}\ns\\\\\n\\downarrow\\\\\nx\n\\end{array}\n\\right)}{count(s)}5.5 tri-gram markov\n\\begin{array}{ll}\nP(\\bold{y},\\bold{x})\n&=P(y_1,\\ldots,y_n,x_1,\\ldots,x_n)\\\\\n&=P(\\bold{x}|\\bold{y})P(\\bold{y})\\\\\n&=(\\prod^n_{i=1}P(x_i|y_i))\\\\\n&\\text{注释：这里用了条件独立假设，以及可见符号的概率只与当前状态有关}\\\\ \n&\\times P(y_1)P(y_2|y_1)P(y_3|y_1,y_2)\\ldots P(y_n|y_1,\\ldots,y_{n-1})\\\\\n&=(\\prod^n_{i=1}P(x_i|y_i))P(y_1)P(y_2|y_1)\\prod^n_{i=3}P(y_i|y_1,\\ldots,y_{i-1})\\\\\n&=(\\prod^n_{i=1}P(x_i|y_i))P(y_1)P(y_2|y_1)\\prod^n_{i=3}P(y_i|y_{i-2},y_{i-1})\\\\\n&\\text{注释：这里用了隐马尔可夫二阶假设，即当前状态的概率只与上一个状态有关}\\\\\n&=P(y_1)P(y_2|y_1)\\prod^n_{i=3}P(y_i|y_{i-2},y_{i-1})\\prod^n_{i=1}P(x_i|y_i)\n\\end{array}\n\\begin{array}{ll}\n\\max_{y_1,\\ldots,y_n} P(\\bold{y},\\bold{x})\n&=\\max_{y_1,\\ldots,y_n}P(y_1,\\ldots,y_n,x_1,\\ldots,x_n)\\\\\n&=\\max_{y_1,\\ldots,y_n}P(y_1)P(y_2|y_1)\\prod^n_{i=3}P(y_i|y_{i-2},y_{i-1})\\prod^n_{i=1}P(x_i|y_i)\\\\\n&=\\max_{y_1,\\ldots,y_n}P(y_n|y_{n-2},y_{n-1})y(x_n|y_n)\\ldots P(y_3|y_1,y_2)P(x_3|y_3)P(y_2|y_1)P(x_2|y_2)P(y_1)P(x_1|y_1)\\\\\n&=\\max_{y_2,\\ldots,y_n}P(y_n|y_{n-2},y_{n-1})y(x_n|y_n)\\ldots P(y_4|y_2,y_3)P(x_4|y_4)\\max_{y_1}P(y_3|y_1,y_2)P(x_3|y_3)P(y_2|y_1)P(x_2|y_2)P(y_1)P(x_1|y_1)\\\\\n&=\\max_{y_2,\\ldots,y_n}P(y_n|y_{n-2},y_{n-1})y(x_n|y_n)\\ldots P(y_4|y_2,y_3)P(x_4|y_4)\\max_{y_1}P(y_3|y_1,y_2)P(x_3|y_3)Score(y_1,y_2)\\\\\n&\\text{Initialize: } score_1(y_1,y_2)=P(y_2|y_1)P(x_2|y_2)P(y_1)P(x_1|y_1)\\\\\n&=\\max_{y_3,\\ldots,y_n}P(y_n|y_{n-2},y_{n-1})y(x_n|y_n)\\ldots P(y_5|y_3, y_4)P(x_5|y_5) \\max_{y_2}P(y_4|y_2,y_3)P(x_4|y_4)\\max_{y_1}P(y_3|y_1,y_2)P(x_3|y_3)Score(y_1,y_2)\\\\\n&=\\max_{y_3,\\ldots,y_n}P(y_n|y_{n-2},y_{n-1})y(x_n|y_n)\\ldots P(y_5|y_3,y_4)P(x_5|y_5)\\max_{y_2}P(y_4|y_2,y_3)P(x_4|y_4)Score(y_2,y_3))\\\\\n&\\text{Update: }score(y_2,y_3)=\\max_{y_1}P(y_3|y_1,y_2)P(x_3|y_3)Score(y_1,y_2)\\\\\n&\\vdots\\\\\n    &=\\max_{y_{n-2},y_{n-1},y_n}P(y_n|y_{n-2},y_{n-1})P(x_n|y_n)\\max_{y_{n-3}}P(y_{n-1}|y_{n-3},y_{n-2})P(x_{n-1}|y_{n-1})Score(y_{y_{n-3}},y_{n-2})\\\\\n&\\text{Update: }score(y_{n-2},y_{n-1})=\\max_{y_{n-3}}P(y_{n-1}|y_{n-3},y_{n-2})P(x_{n-1}|y_{n-1})Score(y_{y_{n-3}},y_{n-2})\\\\\n&=\\max_{y_{n-2},y_{n-1},y_n}P(y_n|y_{n-2},y_{n-1})P(x_n|y_n)score(y_{n-2},y_{n-1})\\\\\n&=\\max_{y_{n-1},y_n}\\max_{y_{n-2}}P(y_n|y_{n-2},y_{n-1})P(x_n|y_n)score(y_{n-2},y_{n-1})\\\\\n&=\\max_{y_{n-1},y_{n}}score(y_{n-1},y_{n})\n\\end{array}\nscore(y_{i},y_{i+1})=\n\\left\\{\n\\begin{array}{cc}\nP(y_2|y_1)P(x_2|y_2)P(y_1)P(x_1|y_1),&\\text{i=1}\\\\\n\\max_{y_{i-1}}P(y_{i+1}|y_{i-1},y_{i})P(x_{i+1}|y_{i+1})score(y_{i-1},y_{i}),&\\text{i=2,$\\ldots$,n-1}\n\\end{array}\n\\right.5.6 HOMEWORK\n\n关键的点\n作业需要非常熟悉的掌握。\n朴素贝叶斯，以及平滑的方法。\n\nNER以及IE的过程熟悉掌握。\n\n遇到难算的地方，直接用分数写成乘法的形式也是可以的，不需要展开来算。\n开放题，有两个选择，选择其中一个写就行。设计一个QA，设计一个IR，或者让你谈一谈Bert的语言模型的特点\n所有的题目都应该是英文作答。\n\n","categories":["nlp"]},{"title":"Phrase queries and positional indexes","url":"/2021/08/15/nlp%20learning/Chapter9_tfidf/","content":"Phrase queries and positional indexes\n\nPhrase queries and positional indexes1. Phrase Queries\n如何解决短语索引的信息检索？\n\nThus the sentence I went to university at southeast of China is not a match.\n\nThe concept of phrase queries has proven easily understood by users; one of the few advanced search ” ideas that works\n\n\nMany more queries are implicit phrase queries:\nnatural language processing\n\n\nFor this, it no longer suffices to store only\n\\&lt;term : docs &gt; entries\n倒排无法实现\n\n\n\n1.1 Solution 1: Biword indexes\nIndex every consecutive pair of terms in the text as a phrase\n\n两两组合，进行存储\n\n\nFor example the text “Friends , Romans, Countrymen” would generate the biwords\n\nfriends romans\nromans countrymen\n\n\nEach of these biwords is now a dictionary term\n\nTwo word phrase query processing is now immediate.\n\n这种方法对于两个单词的组合\n\n\n对于长索引，我们可以进行两两拆分，从而形成多个二元词组的合取\n\nstandford university palo alto\nstandford unversity and university palo and palo alto\n\n\n除非极端情况，会造成FN，大部分情况就合适的\n但是这种方法会造成大量的存储开销\n\n1.2 Solution 2: Positional Indexes\nIn the postings, store, for each term the position(s) in which tokens of it appear:\n&lt;term , number of docs containing termdoc1: position1, position2 …doc2: position1, position2 …etc.&gt;\n存储了文档的位置\n\n\nPositional Index Example\nWhich of docs 1,2,4,5 could contain to be or not to be\n\n&lt;be :993427;\n1: 7, 18, 33, 72, 86,231;\n2: 3, 149；\n4: 17, 191, 291, 430,434;\n5: 363, 367,...&gt;\n我们可以发现4和5有可能是存在这句话的，因为他们的差分序列含有4这一项\n一般而言，两个词的信息都是存在的，那么这种情况如何处理？\nto:\n2:1,17,74,222,551;  4:8,16,190,429,433;     7:13,23,191…\n\n\nbe:\n1:17,19 ;    4:17,191,291,430,434; 5:14,19,101…\n\n\n算法思路，利用两级倒排索引，先从1开始，然后看to没有1,所以删除1，再看2，发现be，没有2，所以删除2，这样就一起搜索到4；\n从4里我们通过to be只差1的关系，可以得到一堆候选\n4:8,16,190,429,433;  \n4:17,191,291,430,434; \n\n\n紧接着再通过to be or not to be剩余的位置关系筛选到几个\n4:429,433;\n4:430,434;\n\n\n会占据2-4倍的存储开销：A positional index is 2-4 as large as a  nonpositional index\n\nPositional index size 35 50% of volume of original text\n\nCaveat: all of this holds for “English like” languages\n\n\n\n2. Ranked Retrieval2.1 Problem with Boolean search:\nThus far, our queries have all been Boolean.\nDocuments either match or don’t.\n对于布尔型排序会过于绝对\n\n\nGood for expert users with precise understanding of their needs and the collection.\n\nAlso good for applications: Applications can easily consume 1000sof results\n好处，对于专家可以写很多精准的查询，查到就有，查不到就一定有\n封闭世界：有则一定能查到；开放世界：查不到可能是由于知识库的缺失\n\n\nNot good for the majority of users.\n\nMost users incapable of writing Boolean queries (or they are,but they think it’s too much work).\n普通人无法使用布尔查询\n\n\nMost users don’t want to wade through 1000s of results.\nThis is particularly true of web search.\n\n\n如何解决这种要么很多答案，要么找不到答案这种情况\n\n\n\n\nFeast or Famine : Not A Problem In Ranked Retrieval\n\nWhen a system produces a ranked result set, large result sets are not an issue\n\nIndeed, the size of the result set is not an issue\nWe just show the top k ( ≈ 10) results\nWe don’t overwhelm the user\nPremise: the ranking algorithm works\n\n\n\n2.2 Scoring as The Basis Of Ranked Retrieval\nWe wish to return in order the documents most likely to beuseful to the searcher\n\nHow can we rank order the documents in the collection withrespect to a query\n\nAssign a score say in [0, 1] to each document\n\nThis score measures how well document and query “match“\n\nLet’s start with a one term query\n\nIf the query term does not occur in the document: scoreshould be 0\n\n首先document不包含关键字的肯定score=0，\n\n\nThe more frequent the query term in the document, thehigher the score (should be ) really？\n\n不同document含有的term频次不一样，对于单个词出现越多越相关，但是对于多个词就不一定\n\n\n\n3.3 Scoring with the Jaccard Coefficient\nA commonly used measure of overlap of two sets A and B isthe Jaccard coefficient\n\n\n\\begin{array}{ll}\njaccard (A,B) = |A\\cap B | / |A\\cup B| \\\\\njaccard (A,A) = 1\\\\                                   \njaccard (A,B) = 0\\text{ if }A\\cap B = 0\\\\\n\\end{array}\nA and B don’t have to be the same size.\n\nAlways assigns a number between 0 and 1.\n\nWhat is the query document match score that the Jaccard coefficient computes for each of the two documents below？\n\nQuery : ides of march\nDocument 1: caesar died in march\nJ(Q,D1)=1/6\n\n\nDocument 2: the long march\nJ(Q,D2)=1/5\n\n\n\n\n\n3.3.1 Issues With Jaccard For Scoring\nIt doesn’t consider term frequency (how many times a termoccurs in a document)\n\nRare terms in a collection are more informative than frequentterms\n出现次数越少信息熵越大\n\n\nJaccard doesn’t consider this information\n\n\nWe need a more sophisticated way of normalizing for length\n\nWe can use \n\n\n\n3.4 Term Frequency Weighting3.4.1 Recall: Term-document Incidence Matrix\n\nEach document is represented by a binary vector\n\nConsider the number of occurrences of a term in a document:\n\nEach document is a count vector in :a column below\n\n\n\n\n3.4.2 Term Frequency ​​​\nThe term frequency ​​​ of term ​​​ in document ​​​ is defined as the number of times that ​​​ occurs in ​​​.\n词t在文章d出现的次数\n\n\nWe want to use tf when computing query-document match scores. But now?\n出现9次和出现10次其实并不是差很多，其实出现次数与查询并不线性相关\n\n\n\n3.4.3 Log-frequency Weighting\nThe log frequency weight of term t in d is\n\n\nw_{t, d}=\\left\\{\\begin{array}{cc}\n1+\\log _{10} \\mathrm{tf}_{t, d}, & \\text { if } \\mathrm{tf}_{t, d}>\\mathrm{O} \\\\\n\\mathrm{O}, & \\text { otherwise }\n\\end{array}\\right.\n区分出现一次和出现零次\nScore for a document-query pair: sum over terms t in both q and d:\n计算quiery和document里共同出现的词的分数之和\n\n\n\n\n\\text{score}=\\sum_{t \\in q \\cap d}\\left(1+\\log \\mathrm{tf}_{t, d}\\right)\nThe score is 0 if none of the query terms is present in the document.\n\n3.4.4 (Inverse) Document Frequency WeightingDocument Frequency\nRare terms are more informative than frequent terms\nRecall stop words\n\n\n对于信息量比较高的词我们将会给予更高的权重\n\n3.4.5 idf Weight\n从所有语料里进行搜索，df越高，这个术语含有的信息越低\n\n​​ is the document frequency of ​​ : the number of documents that contain ​​\n\n​​ is an inverse measure of the informativeness of ​​\n​\n\n\n一个词在所有文档出现的的频率\nWe define the idf (inverse document frequency) of  by\nWe use  instead of  to “dampen“(抑制) the effect of idf.\n\n\n\n\n\\text{Idf}_{t}=\\log _{10}\\left(N / \\mathrm{df}_{t}\\right)\n是文档数，是t出现的文档数\nWill turn out the base of the log is immaterial不重要\n\n\n3.4.6 Effect of idf on Ranking\nQuestion: Does idf have an effect on ranking for one-term queries, like\n\niPhone\n\n\nidf has no effect on ranking one term queries\n\nidf affects the ranking of documents for queries with at least two terms\n如果quiery只包含一个单词，那么idf对其没有影响；如果包含多个单词，则有影响\ntf越大与当前document越相关，term的信息量\n\n\nidf使得多个词的搜索的TF计算有权重，相当于对TF进行了加权求和\n\n3.4.7 Collection vs. Document Frequency\ncollection frequency 相当于把term在所有语料出现的次数进行了求和\n\n\n\nidf为何要和文档有关？\nthe 在100个文档有关，那么相当于信息量为0\n东南大学在其中一篇文档的出现100次，其实信息量很大\n所以我们不能采用collection feq，要采用doc feq\n\n\n\n3.4.8 tf-idf Weighting\nThe tf-idf weight of a term is the product of its tf weight and its idf weight.\n\n\n\\mathrm{w}_{t, d}=\\left(1+\\log \\mathrm{tf}_{t, d}\\right) \\times \\log _{10}\\left(N / \\mathrm{df}_{t}\\right)\nBest known weighting scheme in information retrieval\nNote: the “-“ in tf-idf is a hyphen, not a minus sign!\nAlternative names: tf.idf, \n\n\nIncreases with the number of occurrences within a document\nIncreases with the rarity of the term in the collection\n\n这样我们就可以把原来的布尔矩阵换成tf-idf值\n\n\n3.4.9 Final Ranking of Documents for a Query\n\\operatorname{Score}(q, d)=\\sum_{t \\in q \\cap d} \\operatorname{tf} . \\mathrm{idf}_{t, d}Binary → count → weight matrix\n\nEach document is now represented by a real-valued vector of tf-idf weights \n\n3.5 The Vector Space Model (VSM)3.5.1 Documents As Vectors\nNow we have a |V|-dimensional vector space\nTerms are axes of the space\nDocuments are points or vectors in this space\nVery high-dimensional: tens of millions of dimensions when you apply this to a web search engine\nThese are very sparse vectors – most entries are zero\n向量表示过于稀疏，维度过大\n\n3.5.2 Queries As Vectors\nKey idea 1: Do the same for queries: represent them as vectors in the space\nidf和document计算方式相同,tf可以理解为把quiery当作一个document然后去计算他的tf\n\n\nKey idea 2: Rank documents according to their proximity（(时间或空间)邻近） to the query in this space\nproximity = similarity of vectors\nproximity ≈ inverse of distance\n距离的倒数和相似性有区别\n\n\n\n3.5.3 Formalizing Vector Space Proximity\nEuclidean distance is a bad idea, because Euclidean distance is large for vectors of different lengths.\n\nThe Euclidean distance between  and  is large even though the distribution of terms in the query  and the distribution of terms in the document ​​ are very similar.\n\n简言之就是相似度应该更考虑特征的分布而不是考虑实值，因为文档的长度一定是更长的，可能会造成每个维度上的值都比较大。\n\n\n\n\n3.5.4 Use Angle Instead Of Distance\nThe Euclidean distance between the two documents can be quite large\nThe angle between the two documents is 0, corresponding to maximal similarity.\nKey idea: Rank documents according to angle with query.\n\n3.5.5 From Angles To Cosines\nThe following two notions are equivalent.\nRank documents in decreasing order of the angle between query and document\nRank documents in increasing order of cosine(query,document)\n\n\nCosine is a monotonically decreasing function for the interval ​\n\n3.5.6 computing cosines\nA vector can be (length-) normalized by dividing each of its components by its length - for this we use the ​ norm:\n\n\n\\|\\vec{x}\\|_{2}=\\sqrt{\\sum_{i} x_{i}^{2}}\nDividing a vector by its  norm makes it a unit (length) vector (on surface of unit hypersphere)\nEffect on the two documents ​ and ​ ( ​ appended to itself) from earlier slide: they have identical vectors after length normalization.\nLong and short documents now have comparable weights\n\n\n\n\n\n is the tf-idf weight of term  in the query  is the tf-idf weight of term  in the document  is the cosine similarity of  and  or, equivalently, the cosine of the angle between  and .\n\nFor length-normalized vectors, cosine similarity is simply the dot product (or scalar product):\n\n\n\n\\cos (\\vec{q}, \\vec{d})=\\vec{q} \\cdot \\vec{d}=\\sum_{i=1}^{|V|} q_{i} d_{i}\nfor ​ length-normalized.\nCosine Similarity Illustrated\n\n\n3.5.7 Cosine similarity amongst 3 documents\nHow similar are the novels “SaS: Sense and SensibilityPaP: Pride and PrejudiceWH: Wuthering Heights”?\nTerm frequencies (counts)\n\n\n\nNote: To simplify this example, we don’t do idf weighting.\n\n\n3.6 Calculating if-idf Cosine Scores in an IR System3.6.1 tf-idf Weighting Has Many Variants\n\nMany search engines allow for different weightings for queries vs. documents\nSMART Notation: denotes the combination in use in an engine, with the notation ddd.qqq using the acronyms from the previous table\nA very standard weighting scheme is: lnc.ltc\nDocument : logarithmic tf (l as first character), no idf and cosine normalization\nQuery : logarithmic tf (l in leftmost column), idf (t in second column), cosine normalization …\n\n3.6.2 tf-idf example: lnc.ltc\nDocument: car insurance auto insurance\nQuery: best car insurance\n\n\n\nExercise: what is N, the number of docs?\n\n\nN=10^{2.3}\\times 5000=99763\n\\begin{aligned}\n&\\text { Doc length }=\\sqrt{1^{2}+0^{2}+1^{2}+1.3^{2}} \\approx 1.92 \\\\\n&\\text { Score }=0+0+0.27+0.53=0.8\n\\end{aligned}Computing cosine scores\n4. Evaluating Search Engines\nAssume 10 rel docs in collection\n\n\n\nR：1/10，1/10，1/10，1/5，3/10，3/10，2/5，2/5，2/5，2/5\nP：1      ，0.5，   0.33，0.5，0.6   ， 0.5，  4/7，0.5，4/9，2/5  \n\n\n4.1 Two Current Evaluation Measures…\nMean average precision (MAP)\nAP: Average of the precision value obtained for the top k documents, each time a relevant doc is retrieved\nAvoids interpolation, use of fixed recall levels\nDoes weight most accuracy of top returned results\nMAP for set of queries is arithmetic average of APs\nMacro-averaging: each query counts equally\n简言之就是对于一个问答集合，算他们的平均正确率\n\n\nR-precision\nIf have known (though perhaps incomplete) set of relevant documents of size Rel, then calculate precision of top Rel docs returned\n如果已知（尽管可能不完整）一组大小为Rel的相关文档，则计算返回的top Rel文档的精度\n\n\nPerfect system could score 1.0.\n\n\n\n","categories":["nlp"]},{"title":"Gray-Level Grouping","url":"/2021/08/15/Image%20processing/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1/absolute%20discounting%20back-off%E7%AE%97%E6%B3%95/","content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n1. Abstract\n本文提出了一种新颖的彩色图像直方图均衡方法：该方法利用颜色分量之间的相关性，并借用语言统计工程中的多级平滑技术进行图像增强。 多级平滑技术旨在有效地处理不可见颜色值的问题，其利用一种单独考虑或者综合考虑的方法解决。最终将这项技术应用于 图片的HSI 颜色空间，此处将其应用于 HSI 颜色空间以获得强度概率和给定强度的饱和概率，而色调保持不变。\n此外，为消除色域问题，本文提出了，在基于保持色调不变的情况下，对非线性变换的技术进行了扩展的方法。 这是论文中提出的第二种方法。 \n接着，本文将以上两种均衡图像方法与其他熟知的方法进行比较。 为此，本文提出了一种根据处理后图像的视觉吸引力和客观品质因数来判断的图像均衡质量评价算法，例如：将所得颜色直方图与多元均匀概率密度函数之间的熵和 Kullback-Leibler 散度估计算法。\n\n2. Introduction\n图像增强旨在从人类视觉的角度改进图像。 通过增加图像动态范围的方式，锐化图像的边缘、边界和对比度等图像特征，而不改变图像中固有信息内容 。 为此，开发了多种技术。 其中包括对比度处理、降噪、边缘锐化和锐化、过滤、伪着色、图像插值和放大。\n对比度操纵技术可以分为全局或自适应。 全局技术对所有图像像素应用变换，而自适应技术使用随局部图像特征自适应变化的输入-输出变换。 更常见的全局技术比如线性对比度拉伸、直方图均衡和多通道过滤。 最常见的自适应技术是自适应直方图均衡化 (AHE) 和对比度限制自适应直方图均衡化 (CLAHE) [2,3]。  AHE 将不同的灰度变换局部应用于每个小图像区域，因此需要确定区域大小。  CLAHE 通过限制局部对比度增益来改进刚刚描述的技术。 已确定后一种方法的两个缺点，即对图像平滑区域中产生不可避免的噪声增强和对比度增益。\n本文主要使用全局的方法对图像进行增强处理。更准确来说，为了共同均衡HSI色彩空间的饱和度以及强度两个分量，本文利用从统计语言建模中借用的一元和二元概率的概念以及概率平滑均衡化彩色直方图。直方图均衡化方法部分建立在Pitas and Kiniklis建议的基础上，但它通过平滑必要的概率进行扩展，以抵消看不见的颜色分量组合的影响，这源于颜色空间的维度和图像中存在的颜色数量通常有限。 此外，开发了第二种方法，以通过利用 Naik 和 Murthy [6] 中提出的转换，来消除色域问题。 将所提出方法的性能与 Pitas 和 Kiniklis [5,7] 提出的方法以及每个颜色分量的单独均衡进行比较。 比较不仅使用主观度量（即均衡图像在视觉上的吸引力如何），还使用客观的品质因数，例如所得颜色直方图与相应的多元均匀概率密度之间的熵和 Kullback-Leibler 散度 功能。\n论文的大纲如下。 在第 2 节中，简要介绍了彩色图像直方图均衡方法。 在第 3 节中，描述了本文提出的基线直方图均衡方法和新算法。 实验结果在第 4 节中展示，最后在第 5 节中得出结论。 RGB 和 HSI 颜色空间的简要描述在附录 A 中给出。\n\n3. Algorithms颜色空间介绍\nHSV\n\n\n\nRGB转化到HSV的算法:max=max(R,G,B);min=min(R,G,B);V=max(R,G,B);S=(max-min)/maxif(R = max) H =(G-B)/(max-min) 60;if(G = max) H = 120+(B-R)/(max-min) 60;if(B = max) H = 240 +(R-G)/(max-min) 60;if(H &lt; 0) H = H+ 360;\n\nHSV(hue,saturation,value)颜色空间的模型对应于圆柱坐标系中的一个圆锥形子集，圆锥的顶面对应于V=1. 它包含RGB模型中的R=1，G=1，B=1 三个面，所代表的颜色较亮。色彩H由绕V轴的旋转角给定。红色对应于 角度0° ，绿色对应于角度120°，蓝色对应于角度240°。在HSV颜色模型中，每一种颜色和它的补色相差180° 。 饱和度S取值从0到1，所以圆锥顶面的半径为１。HSV颜色模型所代表的颜色域是CIE色度图的一个子集，这个 模型中饱和度为百分之百的颜色，其纯度一般小于百分之百。在圆锥的顶点(即原点)处，V=0,H和S无定义， 代表黑色。圆锥的顶面中心处S=0，V=1,H无定义，代表白色。从该点到原点代表亮度渐暗的灰色，即具有不同 灰度的灰色。对于这些点，S=0,H的值无定义。可以说，HSV模型中的V轴对应于RGB颜色空间中的主对角线。 在圆锥顶面的圆周上的颜色，V=1，S=1,这种颜色是纯色。HSV模型对应于画家配色的方法。画家用改变色浓和 色深的方法从某种纯色获得不同色调的颜色，在一种纯色中加入白色以改变色浓，加入黑色以改变色深，同时 加入不同比例的白色，黑色即可获得各种不同的色调。\n\n\nHSI\nHSI色彩空间是从人的视觉系统出发，用色调(Hue)、色饱和度(Saturation或Chroma)和亮度 (Intensity或Brightness)来描述色彩。HSI色彩空间可以用一个圆锥空间模型来描述。用这种 描述HIS色彩空间的圆锥模型相当复杂，但确能把色调、亮度和色饱和度的变化情形表现得很清楚。\n通常把色调和饱和度通称为色度，用来表示颜色的类别与深浅程度。由于人的视觉对亮度的敏感 程度远强于对颜色浓淡的敏感程度，为了便于色彩处理和识别，人的视觉系统经常采用HSI色彩空间， 它比RGB色彩空间更符合人的视觉特性。在图像处理和计算机视觉中大量算法都可在HSI色彩空间中 方便地使用，它们可以分开处理而且是相互独立的。因此，在HSI色彩空间可以大大简化图像分析 和处理的工作量。HSI色彩空间和RGB色彩空间只是同一物理量的不同表示法，因而它们之间存在着 转换关系。\nHSI 色彩模型是从人的视觉系统出发，用 H 代表色相 (Hue)、S 代表饱和度 (Saturation) 和 I 代表亮度 (Intensity) 来描述色彩。饱和度与颜色的白光光量刚好成反比，它可以说是一个颜色鲜明与否的指标。因此如果我们在显示器上使用 HIS 模型来处理图像，将能得到较为逼真的效果。 \n色相 (Hue)：指物体传导或反射的波长。更常见的是以颜色如红色，橘色或绿色来辨识，取 0 到 360 度的数值来衡量。 \n饱和度 (Saturation)：又称色度，是指色彩的强度或纯度。饱和度代表灰色与色调的比例，并以 0% (灰色) 到 100% (完全饱和) 来衡量。 \n亮度 (Intensity)：是指颜色的相对明暗度，通常以 0% (黑色) 到 100% (白色) 的百分比来衡量。\n\n\n\n3.1 Separate equalization of the three color components—Method I\n​：以每个像素的灰度值作为随机变量\n：灰度取值范围，本文为256\n\n\n\\begin{equation}\nf_{\\mathbf{x}}\\left(x_{k}\\right)=P\\left\\{\\mathbf{x}=x_{k}\\right\\}=\\frac{N\\left(x_{k}\\right)}{\\sum_{m=0}^{L-1} N\\left(x_{m}\\right)} \\quad \\forall k=0,1, \\ldots, L-1\n\\end{equation}\ny_{k}=F_{\\mathbf{x}}\\left(x_{k}\\right)=P\\left\\{\\mathbf{x} \\leqslant x_{k}\\right\\}=\\sum_{m=0}^{k} f\\left(x_{m}\\right) \\quad \\forall k=0,1, \\ldots, L-1\n(1)为概率密度函数，（2）为分布函数\n\n3.2. 3-D Equalization in the RGB space—Method II\n​​：原来的像素点随机变量\n​​：经过（4）式变换后的像素点\n\n\n\\begin{aligned}\ny_{\\mathrm{R}_{k} \\mathrm{G}_{s} \\mathrm{~B}_{t}} &=F_{\\mathbf{x}_{\\mathrm{R}} \\mathrm{x}_{\\mathrm{G}} \\mathbf{x}_{\\mathrm{B}}}\\left(x_{\\mathrm{R}_{k}}, x_{\\mathrm{G}_{\\mathrm{s}}}, x_{\\mathrm{B}_{t}}\\right)=P\\left\\{\\mathbf{x}_{\\mathrm{R}} \\leqslant x_{\\mathrm{R}_{k}}, \\mathbf{x}_{\\mathrm{G}} \\leqslant x_{\\mathrm{G}_{\\mathrm{s}}}, \\mathbf{x}_{\\mathrm{B}} \\leqslant x_{\\mathrm{B}_{t}}\\right\\} \\\\\n&=\\sum_{i=0}^{k} \\sum_{j=0}^{s} \\sum_{m=0}^{t} f\\left(x_{\\mathrm{R}_{i}}, x_{\\mathrm{G}_{j}}, x_{\\mathrm{B}_{m}}\\right) \\quad \\forall k, s, t=0,1, \\ldots, L-1\n\\end{aligned}\n\\begin{aligned}\ny_{\\mathrm{R}_{k^{\\prime}} \\mathrm{G}_{j} \\mathrm{~B}^{\\prime}} &=\\sum_{i=0}^{k^{\\prime}} \\sum_{j=0}^{s^{\\prime}} \\sum_{m=0}^{t^{\\prime}} \\frac{1}{L^{3}} \\\\\n&=\\frac{\\left(k^{\\prime}+1\\right)\\left(s^{\\prime}+1\\right)\\left(t^{\\prime}+1\\right)}{L^{3}} \\forall k^{\\prime}, s^{\\prime}, t^{\\prime}=0,1, \\ldots, L-1,\n\\end{aligned}\n具体而言，对于每一个像素 变换后为 ，应满足，且尽可能小。\n\n3.3. Equalization of the intensity component in the HSI space— Method III\n​​：对应每一个像素的随机变量，式（5）为其概率密度函数\n\n\nf_{\\mathrm{x}_{\\mathrm{I}}}\\left(x_{\\mathrm{I}_{k}}\\right)= \\begin{cases}12 \\times x_{\\mathrm{I}_{k}}^{2} & \\text { for } 0 \\leqslant x_{\\mathrm{I}_{k}} \\leqslant 0.5 \\\\ 12 \\times\\left(1-x_{\\mathrm{I}_{k}}\\right)^{2} & \\text { for } 0.5 \\leqslant x_{\\mathrm{I}_{k}} \\leqslant 1\\end{cases}3.4. 2-D Equalization for intensity and saturation in the HSI space — Method IV\n​：对应每一个像素的随机变量\n\n​：取亮度和饱和度作为研究对象的随机变量\n\n​：联合分布函数\n对于给定n个任意的随机变量，的形式为\n\n\n\n\\mathbf{y}_{1}=F\\left(\\mathbf{x}_{1}\\right), \\quad \\mathbf{y}_{2}=F\\left(\\mathbf{x}_{2} \\mid \\mathbf{x}_{1}\\right), \\ldots, \\mathbf{y}_{n}=F\\left(\\mathbf{x}_{n} \\mid \\mathbf{x}_{n-1}, \\ldots, \\mathbf{x}_{1}\\right)\n其均衡后的像素为​：其中​， \n\n\ny_{\\mathrm{I}_{k}}=F\\left(x_{\\mathrm{I}_{k}}\\right)=P\\left\\{\\mathbf{x}_{\\mathrm{I}} \\leqslant x_{\\mathrm{I}_{k}}\\right\\}=\\sum_{m=0}^{k} f\\left(x_{\\mathrm{I}_{m}}\\right)=\\sum_{m=0}^{k} P\\left(\\mathbf{x}_{\\mathrm{I}}=x_{\\mathrm{I}_{m}}\\right)\ny_{\\mathrm{S}_{t}}=F\\left(x_{\\mathrm{S}_{t}} \\mid x_{\\mathrm{I}_{k}}\\right)=\\sum_{m=0}^{t} f\\left(x_{\\mathrm{S}_{m}} \\mid x_{\\mathrm{I}_{k}}\\right)=\\sum_{m=0}^{t} \\frac{P\\left(\\mathbf{x}_{\\mathrm{I}}=x_{\\mathrm{I}_{k}}, \\mathbf{x}_{\\mathrm{S}}=x_{\\mathrm{S}_{m}}\\right)}{P\\left(\\mathbf{x}_{\\mathrm{I}}=x_{\\mathrm{I}_{k}}\\right)}​        其实就是亮度的分布函数，以及给定亮度，饱和度的条件概率分布函数\n\n但作者发现由于二维随机变量分布的稀疏性（L*M种可能），所以需要对原来的概率密度进行平滑处理\n\n\n\nP\\left(\\mathbf{x}_{\\mathrm{I}}=x_{\\mathrm{I}_{k}}\\right)=P\\left(x_{\\mathrm{I}_{k}}\\right)= \\begin{cases}\\frac{N\\left(x_{1 /}\\right)-b_{1}}{N} & \\text { if } N\\left(x_{\\mathrm{I}_{k}}\\right)>0 \\\\ b_{\\mathrm{I}} \\frac{L-n_{0}}{N} \\frac{1}{\\sum_{L: N\\left(\\mathrm{x}_{i}\\right)=0}^{1} 1} & \\text { if } N\\left(x_{\\mathrm{I}_{k}}\\right)=0\\end{cases}\nP\\left(\\mathbf{x}_{\\mathrm{S}}=x_{\\mathrm{S}_{t}} \\mid \\mathbf{x}_{\\mathrm{I}}=x_{\\mathrm{I}_{k}}\\right)=P\\left(x_{\\mathrm{S}_{t}} \\mid x_{\\mathrm{I}_{k}}\\right)\n是灰度和饱和度的可取的数值个数\n是图片像素总数\n对于指定颜色的像素个数\n为图片中为出现亮度值得个数\n是给定亮度下，未出现饱和度值得个数，\n：某种（亮度，饱和度）出现的次数\n\n​\n\n3.5. 2-D Equalization in Intensity-saturation components of the HSI space with gamut elimination—Method V\n利用非线性变换的方法解决色域问题\n\n\nf_{m, n}(x)= \\begin{cases}\\delta_{1}+\\left(m-\\delta_{1}\\right)\\left(\\frac{x-\\delta_{1}}{m-\\delta_{1}}\\right)^{n}, & \\delta_{1} \\leqslant x \\leqslant m \\\\ \\delta_{2}-\\left(\\delta_{2}-m\\right)\\left(\\frac{\\delta_{2}-x}{\\delta_{2}-m}\\right)^{n}, & m \\leqslant x \\leqslant \\delta_{2}\\end{cases}\n：为两个常数。在标准的S-type 对比度增强里，.​\n\n​：是像素强度值归一的结果\n\n​，其中：\n\n当​说明像素值可能超过了色域范围，颜色向量则会转换到CMY空间\n\nx_{\\mathrm{C}}=1-x_{\\mathrm{R}}, \\quad x_{\\mathrm{M}}=1-x_{\\mathrm{G}},  \\quad x_{\\mathrm{Y}}=1-x_{\\mathrm{B}}\n然后，每个像素值通过进行缩放，再将像素值转化会RGB空间\n\n\n\n\nResults\n同时利用主观的方式以及客观的方式\n主观方式：视觉吸引力和不必要的颜色失真\n客观方式：交叉熵以及KL散点\n\n\n熵代表一个随机变量的不确定程度，因为熵越大代表随机变量的分布越均匀，所以本文最大化交叉上\n\n\n\\begin{aligned}\nH\\left(\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\ldots, \\mathbf{x}_{n}\\right)=&-\\sum_{x_{1}} \\sum_{x_{2}} \\cdots \\sum_{x_{n}} P\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right) \\\\\n& \\times \\log _{2} P\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)\n\\end{aligned}\nKL散度用于衡量两个概率分布的差别，在本文实验中KL散度用于衡量原始直方图和均衡后的直方图与均匀分布的相似度。\n\n\n\\begin{aligned}\n&D\\left(f\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right) \\| g\\left(x_{u}, y_{u}, \\ldots, z_{u}\\right)\\right) \\\\\n&\\quad=\\sum_{x_{1}} \\sum_{x_{2}} \\cdots \\sum_{x_{n}} P\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right) \\\\\n&\\quad \\times \\log _{2}\\left(\\frac{P\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)}{g\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)}\\right)\n\\end{aligned}\n为n-dim均匀分布\n\n\n\n\n\n\n\n\n\n","categories":["ImageProcessing"]},{"title":"Motion & Tracking","url":"/2021/08/15/cv/8.%20Motion%20&%20Tracking/","content":"Motion &amp; Tracking\n\n1. Recall: Histograms of oriented gradients (HOG)\nPartition image into blocks and compute histogram of gradient orientations in each block\n\n\n\n对光照不敏感，一定程度上可以容忍一些变化\n\n1.1 Pedestrian detection with HOG\nTrain a pedestrian template using a linear support vector machine\n\n\n\nAt test time, convolve feature map with template(SVM)\nFind local maxima of response\nFor multi-scale detection, repeat over multiple levels of a HOG pyramid\n\n\n1.2 Window-based detection: strengths\nSliding window detection and global appearance descriptors: Simple detection \nprotocol to implement\nGood feature choices critical\nPast successes for certain classes\n\n\n\n1.3 High computational complexity\nFor example: 250,000 locations x 30 orientations x 4 scales = 30,000,000 evaluations!\nIf training binary detectors independently, means cost increases linearly with number of classes\n\n对于一些方形的框不能有针对进行目标检测，因为物体不一定都是呈矩形分布的\n\nNon-rigid, deformable（非刚性的、可变形的物体） objects not captured well with representations assuming a fixed 2d structure; or must assume fixed viewpoint\n\n对非刚性形变不具有鲁棒性\n\n\n\n\n\nIf considering windows in isolation, context is lost、\n丢失上下文信息\n\n\n\n\n\nIn practice, often entails large, cropped training set (expensive)\nRequiring good match to a global appearance description can lead to sensitivity to partial occlusions\n需要标记\n\n\n\n\n2. Discriminative part-based models\nSingle rigid template usually not enough to represent a category\nMany objects (e.g. humans) are articulated(铰接式), or have parts that can vary in configuration(结构)\n\n\n\n\n\nMany object categories look very different from different viewpoints, or from instance to instance\n不同视角带来的变化\n\n\n\n\n2.1 Solution\n先用全局做响应，再用局部算子做相应\n不管是哪个部分存在，都可以判定为目标检测成功\n\n\n\n\n虽然空间组合发生变化，但是部件仍能检测出来\n\n3. Object proposals3.1 Main idea:\nLearn to generate category-independent regions/boxes that have object-like properties.\nLet object detector search over “proposals”, not exhaustive sliding windows\n找有目标的窗口\n\n\n\n\n\n多尺度显著性\n人眼在观测物体时，会有关注点\n\n\n\n\n\n颜色对比度\n一般物体检测周围环境的颜色存在明显的变化\n\n\n\n\n\n边缘密度，一般来说一个物体的边缘是闭合的\n\n\n\n超像素跨越性：把相似的像素点聚类在一起叫超像素，一个超像素不应该属于两个类。一个框不可能跨越超像素，否则框内无目标\n\n\n\n\n只需要1000个窗口，就能把目标框出来\n\n3.2 Summary\nObject recognition as classification task\nBoosting (face detection ex)\nSupport vector machines and HOG (person detection ex)\nSliding window search paradigm\nPros and cons\nSpeed up with attentional cascade\nDiscriminative part-based models, object proposals\n\n\n\n\n\n4. Motion and Tracking4.1 From images to videos\nA video is a sequence of frames captured over time\nNow our image data is a function of space (𝑥,𝑦)and time (𝑡)\n\n\n4.2 Motion is a powerful perceptual cue\nSometimes, it is the only cue\n每一帧图片具有强相关性，运动可以带来丰富的信息\n下图在运动时可以看到两个圆\n\n\n\n\n\nEven “impoverished” motion data can evoke a strong percept\n下图可以看出一个运动的人\n\n\n\n\n4.3 Uses of motion in computer vision\n3D shape reconstruction\n多角度拍摄\n\n\nObject segmentation\nLearning and tracking of dynamical models\n目标追踪\n\n\nEvent and activity recognition\n\n4.4 Motion field\nmotion field is the projection of the 3D scene motion into the image\n运动场是3D场景运动到图像中的投影\n\n\n\n\n\n4.5 Motion estimation: Optical flow\nDefinition: optical flow is the apparent motion of brightness patterns in the image\n\n明显亮度模式的运动\n光流（optical flow）是空间运动物体在观察成像平面上的像素运动的瞬时速度。\n\n\nIdeally, optical flow would be the same as the motion field\n\nHave to be careful: apparent motion can be caused by lighting changes without any actual motion\nThink of a uniform rotating sphere under fixed lighting vs. a stationary sphere under moving illumination\n一种是均匀光照对选装球体的影响\n一种是光照变化，但是物体不动\n\n\nGOAL:Recover image motion at each pixel from optical flow\n\n4.6 Estimating optical flow\n\n时间很小，位移矢量近似于速度矢量\n\nGiven two subsequent frames, estimate the apparent motion field u(x,y), v(x,y) between them\n\nu,v分别是横向速度和纵向速度\n\n\n\n\n\nKey assumptions\nBrightness constancy: projection of the same point looks the same in every frame\n亮度恒定不变。相同的投影点在不同帧间运动时，其亮度不会发生改变。\n\n\nSmall motion:points do not move very far\n时间连续或运动是“小运动”。即时间的变化不会引起目标位置的剧烈变化，相邻帧之间位移要比较小。\n\n\nSpatial coherence:points move like their neighbors\n空间相关性：相邻的点相似\n\n\n\n\n\n4.6.1 Key Assumptions: small motions\n\n相邻帧，某个区域的像素是逐渐变化的\n\n4.6.2 Key Assumptions: spatial coherence\n\n空间上的一致性，在小领域上运动趋势是相似的\n\n4.6.3 Key Assumptions: brightness Constancy\n4.7 The brightness constancy constraint\n亮度恒定\n\n\nI(x, y, t-1)=I(x+u(x, y), y+v(x, y), t)\n\nBrightness Constancy Equation:\nI(x, y, t-1)=I(x+u(x, y), y+v(x, y), t)\nLinearizing the right side using Taylor expansion:\n\n\n\\begin{aligned}\n&\\quad I(x+u, y+v, t) \\approx I(x, y, t-1)+I_{x}u(x, y)+I_{y} v(x, y)+I_{t} \\\\ \\\\\n&\\quad I(x+u, y+v, t)-I(x, y, t-1)=I_{x} \\cdot u(x, y)+I_{y} \\cdot v(x, y)+I_{t} \\\\ \\\\\n&\\text { Hence, } \\quad I_{x} \\cdot u+I_{y} \\cdot v+I_{t} \\approx 0 \\rightarrow \\nabla I \\cdot[u ,v]^{T}+I_{t}=0\n\\end{aligned}\nt方向求导即为：\n\n\nF_t=\\left[\\begin{array}{cc}\n1 & 1\\\\\n1 & 1\n\\end{array}\\right]，\\ \nF_{t-1}=\\left[\\begin{array}{cc}\n-1 & -1\\\\\n-1 & -1\n\\end{array}\\right]\nCan we use this equation to recover image motion (u,v) at each pixel?\n\n\n\\nabla I \\cdot[u, v]^{T}+I_{t}=0\n增量和梯度方向垂直的话，增量就无影响\n\n\n\\nabla I \\cdot[u, v]^{T}=0,\\text{for any u, v, if }\\nabla I \\perp [u,v]\nHow many equations and unknowns per pixel?\n\nOne equation (this is a scalar equation!), two unknowns (u,v)\n无法求解参数\n\n\nThe component of the flow perpendicular（垂直） to the gradient (i.e., parallel to the edge) cannot be measured\n\n\n\n\n会有多个解，与实际运动就会不一致\n\n4.8 The aperture problem\n孔径问题指在运动估计（Motion Estimation）中无法通过单个算子【计算某个像素值变化的操作，例如：梯度】准确无误地评估物体的运行轨迹。原因是每一个算子只能处理它所负责局部区域的像素值变化，然而同一种像素值变化可能是由物体的多种运行轨迹导致。\n\n\n\n在小孔里看是平行运动，但实际三维运动却不是\n\n\n\n三维是旋转，但是二维看起来是向上走\n這就是「區域(local)」 和「 全域 (global)」 視覺處理的差別。我們的視覺系統區域上 (locally) 可以有孔徑問題的錯覺，但是當我們觀察的範圍是全域 (globally)的時候，卻又分析的出來三張紙條不同的移動方向。\n\n4.9 Solving the ambiguity\nHow to get more equations for a pixel?\nSpatial coherence constraint:\nAssume the pixel’s neighbors have the same (u,v)\nIf we use a 5x5 window, that gives us 25 equations per pixel\n\n\n\n\n\\begin{gathered}\n0=I_{t}\\left(\\mathrm{p}_{\\mathrm{i}}\\right)+\\nabla I\\left(\\mathrm{p}_{\\mathrm{i}}\\right) \\cdot\\left[\\begin{array}{ll}\nu & v\n\\end{array}\\right] \\\\ \\\\\n{\\left[\\begin{array}{cc}\nI_{x}\\left(\\mathrm{p}_{1}\\right) & I_{y}\\left(\\mathrm{p}_{1}\\right) \\\\\nI_{x}\\left(\\mathrm{p}_{2}\\right) & I_{y}\\left(\\mathrm{p}_{2}\\right) \\\\\n\\vdots & \\vdots \\\\\nI_{x}\\left(\\mathrm{p}_{25}\\right) & I_{y}\\left(\\mathrm{p}_{25}\\right)\n\n\\end{array}\\right]\\left[\\begin{array}{l}\nu \\\\\nv\n\\end{array}\\right]=-\\left[\\begin{array}{c}\nI_{t}\\left(\\mathrm{p}_{1}\\right) \\\\\nI_{t}\\left(\\mathrm{p}_{2}\\right) \\\\\n\\vdots \\\\\nI_{t}\\left(\\mathrm{p}_{25}\\right)\n\\end{array}\\right]}\n\\end{gathered}\nOverconstrained linear system\n\n\n\\left[\\begin{array}{cc}\nI_{x}\\left(\\mathrm{p}_{1}\\right) & I_{y}\\left(\\mathrm{p}_{1}\\right) \\\\\nI_{x}\\left(\\mathrm{p}_{2}\\right) & I_{y}\\left(\\mathrm{p}_{2}\\right) \\\\\n\\vdots & \\vdots \\\\\nI_{x}\\left(\\mathrm{p}_{25}\\right) & I_{y}\\left(\\mathrm{p}_{25}\\right)\n\\end{array}\\right]\\left[\\begin{array}{l}\nu \\\\\nv\n\\end{array}\\right]=-\\left[\\begin{array}{c}\nI_{t}\\left(\\mathrm{p}_{1}\\right) \\\\\nI_{t}\\left(\\mathrm{p}_{2}\\right) \\\\\n\\vdots \\\\\nI_{t}\\left(\\mathrm{p}_{25}\\right)\n\\end{array}\\right] \\quad \\begin{array}{cc}\nA & d=b \\\\\n25 \\times 2 & 2 \\times 1\n\\end{array}\nLeast squares solution for $d$ given by $\\left(A^{T} A\\right) d=A^{T} b$\n\n\n\\begin{gathered}\n{\\left[\\begin{array}{cc}\n\\sum I_{x} I_{x} & \\sum I_{x} I_{y} \\\\\n\\sum I_{x} I_{y} & \\sum I_{y} I_{y}\n\\end{array}\\right]\\left[\\begin{array}{l}\nu \\\\\nv\n\\end{array}\\right]=-\\left[\\begin{array}{c}\n\\sum I_{x} I_{t} \\\\\n\\sum I_{y} I_{t}\n\\end{array}\\right]} \\\\\nA^{T} A\n\\end{gathered}\nThe summations are over all pixels in the $\\mathrm{K} \\times \\mathrm{K}$ window\n\nOptimal $(u, v)$ satisfies Lucas-Kanade equation\n\n\n\n\\begin{gathered}\n{\\left[\\begin{array}{cc}\n\\sum I_{x} I_{x} & \\sum I_{x} I_{y} \\\\\n\\sum I_{x} I_{y} & \\sum I_{y} I_{y}\n\\end{array}\\right]\\left[\\begin{array}{l}\nu \\\\\nv\n\\end{array}\\right]=-\\left[\\begin{array}{c}\n\\sum I_{x} I_{t} \\\\\n\\sum I_{y} I_{t}\n\\end{array}\\right]} \\\\\nA^{T} A\n\\end{gathered}\nWhen is this solvable? I.e., what are good points to track?\n$A^TA$​ should be invertible\n不一定可逆\n\n\n$A^TA$​​ should not be too small due to noise \neigenvalues $\\lambda_{1}$ and $\\lambda_{2}$ of $A^{\\top} A$​ should not be too small\n如果$A^TA$值很小，如果有噪音，就会造成很大的扰动，所以特征值不能太小\n\n\n$A^TA$​ should be well-conditioned\n$\\lambda_{1} / \\lambda_{2}$​ should not be too large $\\left(\\lambda_{1}=\\right.$​ larger eigenvalue $)$​​\n\n\n\n\nDoes this remind you of anything?\nCriteria for Harris corner detector\n\n\n\n4.10 Recall: second moment matrix\nEstimation of optical flow is well-conditioned precisely for regions with high “cornerness”:\n\n\n4.10.1 Low texture region\n\n对于平滑区域和边缘都不好检测光流估计\n角点会较为容易检测，因为他的梯度在各个方向都有变化\n\n\n\n4.10.2 The aperture problem resolved\n\n\n用找交点的方式，来进行约束\n\n4.11 Errors in Lucas-Kanade\nThe motion is large (larger than a pixel)\nA point does not move like its neighbors\n柔性物体的变化\n\n\nBrightness constancy does not hold\n\nRevisiting the small motion assumption\n\nIs this motion small enough?\nProbably not—it’s much larger than one pixel\nHow might we solve this problem?\n\n\n意思是对于一些比较大的运动怎么进行测量？\n\n4.12 Reduce the resolution!\n\n利用下采样，那么原来偏移两个像素的运动，就会变成偏移一个像素，从而提高鲁棒性\n\n4.13 Coarse-to-fine optical flow estimation\n\n先将图片进行下采样\n\n\n\n然后从最低分辨率的图片开始进行光流估计，然后在进行上采样\n对于低分辨率求得的u,v将作为下一层的初始值\n\n\n\n\n4.14 A point does not move like its neighbors\nMotion segmentation\n\n\n\n先分块，再用聚类的方法，找真正的方向，把图像分为不同的层，作为整体目标的考虑\nBrightness constancy does not hold\nFeature matching\n\n\n先检测关键点，就可以追踪关键点的轨迹\n\n5. Feature Tracking\n\n\n通过找到图像的关键点，然后最终图像关键点，从而形成特征追踪\n\n5.1 Single object tracking\n\n可以有效的解决遮挡问题\n\n5.2 Multiple object tracking\n\n可能遇到的问题\n实体重叠\n实体分开（id不能搞混）\n\n\n\n5.3 Tracking with a fixed camera\n\n因为用固定的相机拍摄，当人运动时会导致尺度会发生变化\n\n5.4 Tracking with a moving camera\n\n运动的相机背景发生变化\n\n5.5 Tracking with multiple cameras\n\n角度变化\n\n5.6 Challenges in Feature tracking\nFigure out which features can be tracked\nEfficiently track across frames\n\n\nSome points may change appearance over time\ne.g., due to rotation, moving into shadows, etc.\n\n\nDrift: small errors can accumulate as appearance model is updated\n两帧有小的误差，小的误差累积成大的误差\n\n\nPoints may appear or disappear.\n特征点消失与出现\n\n\n\n5.7 What are good features to track?\nIntuitively, we want to avoid smooth regions and edges. But is there a more is principled way to define good features?\n稳定好计算\nKey idea: “good” features to track are the ones whose motion can be estimated reliably\nWhat kinds of image regions can we detect easily and consistently?\n\n5.8 Motion estimation techniques\nOptical flow\nRecover image motion at each pixel from spatio-temporal image brightness variations (optical flow)\n\n\nFeature-tracking\n\nExtract visual features (corners, textured areas) and “track” them over multiple frames\n\n\n特征跟踪：可以用光流算法来帮助最终跟踪\n\n\n5.9 Optical flow can help track features\nOnce we have the features we want to track, lucas-kanadeor other optical flow algorithsmcan help track those features\n\n\n\n6. Shi-Tomasifeature tracker6.1 Simple KLT tracker\nFind a good point to track (harriscorner)\nFor each Harris corner compute motion (translation or affine) between consecutive frames.\nLink motion vectors in successive frames to get a track for each Harris point\nIntroduce new Harris points by applying Harris detector at every m (10 or 15) frames\n检查是否有新的好的特征点\n\n\nTrack new and old Harris points using steps 1‐3\n\n6.2 Recall: Challenges in Feature tracking\nFigure out which features can be tracked\nSome points may change appearance over time\nDrift: small errors can accumulate as appearance model is updated\n所以要找一些比较稳定的特征点作为最终对象\n\n\nPoints may appear or disappear.\n\nNeed to be able to add/delete tracked points\n\n\nCheck consistency of tracks by affine registration to the first observed instance of the feature\n\nAffine model is more accurate for larger displacements\n\n6.3 2D transformations\n可参考阅读 2D transformation review\n\n\n6.3.1 Translation\nLet the initial feature be located by (x, y).\n\nIn the next frame, it has translated to (x’, y’).\n\nWe can write the transformation as:\n\n\n\n\\begin{array}{ll}\nx'=x+b_1\\\\\ny'=y+b_2\n\\end{array}\nWe can write this as a matrix transformation using homogeneous coordinates:\n\n\n\n\\left[\\begin{array}{l}\nx^{\\prime} \\\\\ny^{\\prime}\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n1 & 0 & b_{1} \\\\\n0 & 1 & b_{2}\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny \\\\\n1\n\\end{array}\\right]\nNotation:\n\n\nW(\\boldsymbol{x} ; \\boldsymbol{p})=\\left[\\begin{array}{lll}\n1 & 0 & b_{1} \\\\\n0 & 1 & b_{2}\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny \\\\\n1\n\\end{array}\\right]\nThere are only two parameters:\n\n\n\\boldsymbol{p}=\\left[\\begin{array}{l}\nb_{1} \\\\\nb_{2}\n\\end{array}\\right]\nThe derivative of the transformation w.r.t. $\\mathbf{p}$ :\n\n\n\\frac{\\partial W}{\\partial \\boldsymbol{p}}(\\boldsymbol{x} ; \\boldsymbol{p})=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right]\nThis is called the Jacobian.\n\n6.3.2 Similarity motion\nRigid motion includes scaling + translation.\nWe can write the transformations as:\n\n\n\\begin{array}{ll}\nx'=ax+b_1\\\\\ny'=ay+b_2\n\\end{array}\nW(\\boldsymbol{x} ; \\boldsymbol{p})=\\left[\\begin{array}{lll}a & 0 & b_{1} \\\\ 0 & a & b_{2}\\end{array}\\right]\\left[\\begin{array}{l}x \\\\ y \\\\ 1\\end{array}\\right]\n\\boldsymbol{p}=\\left[\\begin{array}{lll}a & \\mathrm{~b}_{1} & \\mathrm{~b}_{2}\\end{array}\\right]^{T}\n\\frac{\\partial W}{\\partial p}(\\boldsymbol{x} ; \\boldsymbol{p})=\\left[\\begin{array}{lll}x & 1 & 0 \\\\ y & 0 & 1\\end{array}\\right]6.3.3 Affine motion\nAffine motion includes scaling + rotation + translation.\n\n\n\\begin{aligned}\n&W(\\boldsymbol{x} ; \\boldsymbol{p})=\\left[\\begin{array}{lll}\na_{1} & a_{2} & b_{1} \\\\\na_{3} & a_{4} & b_{2}\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny \\\\\n1\n\\end{array}\\right] \\\\ \\\\\n&\\boldsymbol{p}=\\left[\\begin{array}{llll}\na_{1} & \\mathrm{a}_{2} & \\mathrm{~b}_{1} & a_{3} & a_{4} & b_{2}\n\\end{array}\\right]^{T} \\\\ \\\\\n&\\frac{\\partial W}{\\partial p}(\\boldsymbol{x} ; \\boldsymbol{p})=\\left[\\begin{array}{llllll}\nx & y & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & x & y & 1\n\\end{array}\\right]\n\\end{aligned}6.4 Iterative KLT tracker\nGiven a video sequence, find all the features and track them across the video.\nFirst, use Harris corner detection to find features and their location $\\boldsymbol{x}$. For each feature at location $\\boldsymbol{x}=\\left[\\begin{array}{ll}x &amp; y\\end{array}\\right]^{T}$​\nChoose a descriptor create an initial template for that feature: $T(\\boldsymbol{x})$​.\n注意初始帧数会对每个特征计算一个描述符模板，用于比较往后特征描述符和该模板的差距\nOur aim is to find the $\\boldsymbol{p}$ that minimizes the difference between the template $T(\\boldsymbol{x})$ and the description of the new location of $\\boldsymbol{x}$​​​ after undergoing the transformation.\n在特征对应的这样一个小区域，进行最小化变化前后描述符之间的差值\n\n\n\n\n\\sum_{x}[I(W(\\boldsymbol{x} ; \\boldsymbol{p}))-T(x)]^{2}\nFor all the features $x$ in the image $I$,\n\n$I(W(\\boldsymbol{x} ; \\boldsymbol{p}))$ is the estimate of where the features move to in the next frame after the transformation defined by $W(\\boldsymbol{x} ; \\boldsymbol{p})$. Recall that $\\boldsymbol{p}$​ is our vector of parameters.\nSum is over an image patch around $\\boldsymbol{x}$​.\n\n\nWe will instead break down $\\boldsymbol{p}=\\boldsymbol{p}_{\\mathbf{0}}+\\Delta \\boldsymbol{p}$\n\nLarge $+$ small $/$ residual motion\nWhere $\\boldsymbol{p}_{\\mathbf{0}}$ is going to be fixed and we will solve for $\\Delta \\boldsymbol{p}$, which is a small value.\nWe can initialize $\\boldsymbol{p}_{\\mathbf{0}}$ with our best guess of what the motion is and initialize $\\Delta \\boldsymbol{p}$​​ as zero.\n\n\n\n\n\\begin{aligned}\n& \\sum_{x}\\left[I\\left(W\\left(\\boldsymbol{x} ; \\boldsymbol{p}_{\\mathbf{0}}+\\Delta \\boldsymbol{p}\\right)\\right)-T(x)\\right]^{2} \\\\\n\\approx & \\sum_{x}\\left[I\\left(W\\left(\\boldsymbol{x} ; \\boldsymbol{p}_{\\mathbf{0}}\\right)\\right)+\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}} \\Delta \\boldsymbol{p}-T(x)\\right]^{2}\n\\end{aligned}\nIt’s a good thing we have already calculated what $\\frac{\\partial W}{\\partial p}$ would look like for affine, translations and other transformations!\n\nSo our aim is to find the $\\Delta \\boldsymbol{p}$ that minimizes the following:\n\nJ=\\underset{\\Delta p}{\\operatorname{argmin}} \\sum_{x}\\left[I\\left(W\\left(x ; p_{0}\\right)\\right)+\\nabla I \\frac{\\partial W}{\\partial p} \\Delta p-T(x)\\right]^{2}\nWhere $\\nabla I=\\left[\\begin{array}{ll}I_{x} &amp; I_{y}\\end{array}\\right]$\n\nDifferentiate wrt $\\Delta \\boldsymbol{p}$​ and setting it to zero:\n\n\n\n\\frac{\\partial J}{\\partial \\Delta p}=2\\sum_{x}\\left[\\nabla I \\frac{\\partial W}{\\partial p}\\right]^{T}\\left[I\\left(W\\left(x ; p_{0}\\right)\\right)+\\nabla I \\frac{\\partial W}{\\partial p} \\Delta p-T(x)\\right]\n\\sum_{x}\\left[\\nabla I \\frac{\\partial W}{\\partial p}\\right]^{T}\\left[I\\left(W\\left(x ; p_{0}\\right)\\right)+\\nabla I \\frac{\\partial W}{\\partial p} \\Delta p-T(x)\\right]=0\nSolving for $\\Delta \\boldsymbol{p}$ in:\n\\sum_{x}\\left[\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}}\\right]^{T}\\left[I\\left(W\\left(\\boldsymbol{x} ; \\boldsymbol{p}_{\\mathbf{0}}\\right)\\right)+\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}} \\Delta \\boldsymbol{p}-T(x)\\right]=0\nwe get:\n\n\n\\sum_{x}\\left[\\left[\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}}\\right]^{T}I\\left(W\\left(\\boldsymbol{x} ; \\boldsymbol{p}_{\\mathbf{0}}\\right)\\right)+\\left[\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}}\\right]^T\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}} \\Delta \\boldsymbol{p}-\\left[\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}}\\right]^TT(x)\\right]=0\n\\sum_{x}\\left[\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}}\\right]^{T}I\\left(W\\left(\\boldsymbol{x} ; \\boldsymbol{p}_{\\mathbf{0}}\\right)\\right)+\\sum_{x}\\left[\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}}\\right]^T\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}} \\Delta \\boldsymbol{p}-\\sum_{x}\\left[\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}}\\right]^TT(x)=0\n\\sum_{x}\\left[\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}}\\right]^T\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}} \\Delta \\boldsymbol{p}=\\sum_{x}\\left[\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}}\\right]^T\\left[T(x)-I\\left(W\\left(\\boldsymbol{x} ; \\boldsymbol{p}_{\\mathbf{0}}\\right)\\right)\\right]\n\\left(\\sum_{x}\\left[\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}}\\right]^T\\nabla I \\frac{\\partial W}{\\partial\\boldsymbol{p}}\\right)  \\Delta \\boldsymbol{p}=\\sum_{x}\\left[\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}}\\right]^T\\left[T(x)-I\\left(W\\left(\\boldsymbol{x} ; \\boldsymbol{p}_{\\mathbf{0}}\\right)\\right)\\right]\n\\Delta \\boldsymbol{p}=H^{-1} \\sum_{x}\\left[\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}}\\right]^{T}\\left[T(x)-I\\left(W\\left(\\boldsymbol{x} ; \\boldsymbol{p}_{\\mathbf{0}}\\right)\\right)\\right]\nwhere $H=\\sum_{x}\\left[\\nabla I \\frac{\\partial W}{\\partial p}\\right]^{T}\\left[\\nabla I \\frac{\\partial W}{\\partial p}\\right]$\n\n\nH=\\sum_{x}\\left[\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}}\\right]^{T}\\left[\\nabla I \\frac{\\partial W}{\\partial \\boldsymbol{p}}\\right]\nH matrix for translation transformations\n\nRecall that\n\n$\\nabla I=\\left[\\begin{array}{ll}I_{x} &amp; I_{y}\\end{array}\\right]$ and\nfor translation motion, $\\frac{\\partial W}{\\partial p}(\\boldsymbol{x} ; \\boldsymbol{p})=\\left[\\begin{array}{ll}1 &amp; 0 \\ 0 &amp; 1\\end{array}\\right]$Therefore,\n\n\n\\begin{aligned}\nH&=\\sum_{x}\\left[\n\\left[\\begin{array}{ll}\nI_{x} & I_{y}\n\\end{array}\\right]\n\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right]\\right]^{T}\\left[\\begin{array}{ll}\nI_{x} & I_{y}\n\\end{array}\\right]\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right] \\\\\n&={\\sum_{x}\\left[\\begin{array}{ll}\nI_{x}^{2} & I_{x} I_{y} \\\\\nI_{x} I_{y} & I_{y}^{2}\n\\end{array}\\right]} \\begin{array}{l}\n\\text { That's the Harris corner } \\\\\n\\text { detector we learnt in } \\\\\n\\text { class!!! }\n\\end{array}\n\\end{aligned}\nH matrix for affine transformations\n\n\n\\begin{aligned}\nH&=\\sum_{x}\\left[\n\\left[\\begin{array}{ll}\nI_{x} & I_{y}\n\\end{array}\\right]\n\\left[\\begin{array}{llllll}\nx & y & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & x & y & 1\n\\end{array}\\right]\\right]^{T}\\left[\\begin{array}{ll}\nI_{x} & I_{y}\n\\end{array}\\right]\n\\left[\\begin{array}{llllll}\nx & y & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & x & y & 1\n\\end{array}\\right] \\\\\n&={\\sum_{x}\\left[\\begin{array}{llllll}\nx & y & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & x & y & 1\n\\end{array}\\right]^T\\left[\\begin{array}{ll}\nI_{x}^{2} & I_{x} I_{y} \\\\\nI_{x} I_{y} & I_{y}^{2}\n\\end{array}\\right]}\\left[\\begin{array}{llllll}\nx & y & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & x & y & 1\n\\end{array}\\right]\\\\\n&=\\sum_{\\mathbf{x}}\\left[\\begin{array}{cccccc}\nI_{x}^{2} & I_{x} I_{y} & x I_{x}^{2} & y I_{x} I_{y} & x I_{x} I_{y} & y I_{x} I_{y} \\\\\nI_{x} I_{y} & I_{y}^{2} & x I_{x} I_{y} & y I_{y}^{2} & x I_{y}^{2} & y I_{y}^{2} \\\\\nx I_{x}^{2} & y I_{x} I_{y} & x^{2} I_{x}^{2} & y^{2} I_{x} I_{y} & x y I_{x} I_{y} & y^{2} I_{x} I_{y} \\\\\ny I_{x} I_{y} & y I_{y}^{2} & x y I_{x} I_{y} & y^{2} I_{y}^{2} & x y I_{y}^{2} & y^{2} I_{y}^{2} \\\\\nx I_{x} I_{y} & x I_{y}^{2} & x^{2} I_{x} I_{y} & x y I_{y}^{2} & x^{2} I_{y}^{2} & x y I_{y}^{2} \\\\\ny I_{x} I_{y} & y I_{y}^{2} & x y I_{x} I_{y} & y^{2} I_{y}^{2} & x y I_{y}^{2} & y^{2} I_{y}^{2}\n\\end{array}\\right]\n\\end{aligned}6.5 Overall KLT tracker algorithm\nGiven the features from Harris detector:\n这里应该指的是得到特征的坐标信息以及特征信息\n对于追踪而言可以直接用光流法最终特征，但光流法是有误差的\n因为存在噪声，所以需要去比较10帧前后的特征变化，一般来讲经过2D变换后仍能找到特征\n存在一种情况，也就是该特征已经消失，则此时一定找不到一种合适小运动，使得特征进行有效的变换\n\n\n\n\nInitialize $\\boldsymbol{p}_{\\mathbf{0}}$ and $\\Delta \\boldsymbol{p}$.\nCompute the initial templates $T(x)$ for each feature.\nTransform the features in the image $I$ with $W\\left(\\boldsymbol{x} ; \\boldsymbol{p}_{\\mathbf{0}}\\right)$.\nMeasure the error: $I\\left(W\\left(\\boldsymbol{x} ; \\boldsymbol{p}_{\\mathbf{0}}\\right)\\right)-T(x)$.\nCompute the image gradients $\\nabla I=\\left[\\begin{array}{ll}I_{x} &amp; I_{y}\\end{array}\\right]$.\nEvaluate the Jacobian $\\frac{\\partial W}{\\partial p}$.\nCompute steepest descent $\\nabla I \\frac{\\partial W}{\\partial p}$.\nCompute Inverse Hessian $H^{-1}$\nCalculate the change in parameters $\\Delta \\boldsymbol{p}$\nUpdate parameters $\\boldsymbol{p}_{\\mathbf{0}}=\\boldsymbol{p}_{\\mathbf{0}}+\\Delta \\boldsymbol{p}$\nRepeat 2 to 10 until $\\Delta \\boldsymbol{p}$ is small.\n\n\n$\\Delta \\boldsymbol{p}$如果一直很大，则把该特征删去\n\n总的来说，该算法是为了持续监视特征的一个算法，每隔10帧左右进行依次运算，当该运算指的是在给定两张图片，给定了一开始计算的特征模板，然后每隔10fp做一次判别，从当前帧的前第十帧的某一个特征点进行2D变换到当前帧就可以得到当前帧的小区域描述符，通过最小化两者的rms，找到符合的小$\\Delta p$说明该特征完好，否则该特征可能已经消失，则不再对该特征进行追踪\n\n\n","categories":["CV"]},{"title":"Chatbots","url":"/2021/08/15/nlp%20learning/Chapter12_Chatbots/","content":"Chatbots\n\n1. What is Chatbots /Dialogue1.1 Two kind of conversational agents(Task-based) Dialogue Agents(Close domain)\n\nPersonal assistant, help users achieve a certain task in vertical domains, e.g., education and medical\n\nCombination of rules and statistical components\n\nFrames with slots and values: a set of slots, to be filled with information of a given type. Each associated with a question to the user.\n\n\nChatbots (Open domain)\n\nNo specific goal, focus on humanlike conversations\n\nFor fun, or even for therapy\n\nRule-based: Pattern-action rules (ELIZA) + A mental model (PARRY): The first system to pass the Turing Test!\n\nCorpus-based: Information Retrieval (XiaoIce) or Neural encoder-decoder (variants of E2E seq2seq model) (BlenderBot)\n\n\n\n2. Properties of Human Conversation2.1 propertites\n\nTurns\n\nWe call each contribution a “turn”\n\n\nInterruptions and Barge-in:Allowing the user to interrupt\n\nEnd-pointing\n\nThe task for a speech system of deciding whether the user hasstopped talking.\n\n\n\n\nEach turn (utterance in a dialogue is a kind of action\n\nConstatives: committing the speaker to something’s being the case\n\n使说话者确信某事是真实的\nanswering , claiming , confirming , denying , disagreeing , stating \n\n\nDirectives: attempts by the speaker to get the addressee to do\n演讲者试图让收件人这样做\nadvising , asking , forbidding , inviting , ordering , requesting\n\n\nCommissives: committing the speaker to some future course of action\n让演讲者对未来的行动做出承诺\npromising, planning, vowing, betting, opposing\n\n\nAcknowledgments: express the speaker’s attitude regarding the hearerwith respect to some social action \n表达说话人对听话人关于某些社会行为的态度\napologizing , greeting , thanking , accepting an acknowledgment \n\n\n\n2.1 Grounding\nParticipants in conversation or any joint activity need to establish common ground.\n对话应该建立于对事实的一致理解上\n\n\nPrinciple of closure . Agents performing an action require evidence, sufficient forcurrent purposes, that they have succeeded in performing it \n\n封闭原则。执行某项操作的代理需要足够的证据，证明他们已成功执行该操作\n\n\nSpeech is an action too! So speakers need to ground each other’s utterances.\n\n语言也是行动！因此，演讲者需要让对方的话语有根据。\nGrounding : acknowledging that the hearer has understood 承认听者已经理解\n\n\n\n2.1.1 Grounding: Establishing Common Ground\n\n通过一些特殊的连贯词可以缓解尴尬。\n\n\n2.2 Conversations have structure\nLocal structure between adjacent speech acts, from the field of conversational analysis 从会话分析的角度看相邻言语行为之间的局部结构\n\nCalled adjacency pairs: \n\nQUESTION … A NSWER\nPROPOSAL … A CCEPTANCE /R EJECTION\nCOMPLIMENTS (“Nice jacket!”)… DOWNPLAYER (“Oh, this old thIng?”)\n\n\n\n2.3 Another kind of structure: Subdialogues\nCorrection subdialogue 子对话的正确性\n\n要保证可以任意插入子对话，且子对话的结果是对的，并且可能影响到原来的对话\nAgent : OK. There’s #two non stops# 直达站\nClient: #Act actually#, what day of the week is the 15th?\nAgent : It’s a Friday\nClient: Uh hmm. I would consider staying there an extra day til Sunday. \nAgent : OK…OK. On Sunday I have …\n\n\nClarification Subdialogues\n\nUser : What do you have going to UNKNOWN WORD on the 5th?\nSystem: Let’s see, going where on the 5th\nUser: Going to Hong Kong. 澄清去哪里\nSystem : OK, here are some flights…\n\n\nPresequences 可能在正式开始对话有个开场\n\nUser : Can you make train\nSystem : Yes I can\nUser : Great, I’d like to reserve a seat on the 4pm train to New York.\n\n\n\n2.4 Conversational Initiative\n对话的主动权\n\nSome conversations are controlled by one person 有些对话由一个人控制\n\nA reporter interviewing a chef asks questions, and the chef responds.一位采访厨师的记者问了一些问题，厨师回答。\nThis reporter has the conversational initiative This reporter has the conversational initiative\n\n\nMost human conversations have mixed initiative : 大多数人际对话都有混合的主动性：\nI lead, then you lead, then I lead.\n\n\nMixed initiative is very hard for NLP systems, which often default to simpler styles that can be frustrating for humans: 混合主动对NLP非常困难，因为NLP经常默认采用更简单的模式\nUser initiative (user asks or commands, system\nSystem initiative (system asks user questions to fill out a form, user can’t change the direction)\n\n\n\n2.5 Even harder problems: Inference\n推理\nAgent: And, what day in May did you want to travel?\n\nClient : OK, uh, I need to be there for a meeting that’s from the 12th to the 15th.\n\n\n3. Rule based Chatbots3.1 Eliza pattern/transform rules\n大概思想就是提取关键词，然后根据库里关键词对应的答案进行排序\n\ns/. I’M ( depressed|sad ) ./I AM SORRY TO HEAR YOU ARE 1/\n\ns/. I AM ( depressed|sad ) ./WHY DO YOU THINK YOU ARE 1/\n\ns/. all ./IN WHAT WAY?/\n\ns/. always ./CAN YOU THINK OF A SPECIFIC EXAMPLE?/\n\n\n\n0 means *\nThe 3 is the constituent # in pattern\n\nexample\n\nYou hate me\nWHAT MAKES YOU THINK I HATE YOU\n\n\n\n\nUser: I know everybody laughed at me\n“I” is very general:\n\nI: (I *)—&gt; (YOU SAY YOU)\nELIZA: YOU SAY YOU KNOW EVERYBODY LAUGHED AT YOU\n\n\n“Everybody” is more specific and interesting\n\nEverybody: (Everybody *)—&gt; (WHO IN PARTICULAR ARE YOU THINKING\nELIZA: WHO IN PARTICULAR ARE YOU THINKING OF?\n\n\nImplementation: keywords stored with their rank\nEverybody 5 (list of transformation rules\nI                   0 (list of transformation rules\n\n\n\n\n3.1.1 Memory\n如何对对话进行存储-&gt;用于进行后续答案提取，下面举的是提取MY关键词，随机选择一种变换进行存储。\n\n\n\nWhenever “MY” is highest keyword 对一些语句进行存储\nRandomly select a transform on the MEMORY list\nApply to sentence \nStore on a (first in first out) queue\n\n\nLater, if no keyword matches a sentence 如果没有关键词匹配，就返回队首\nReturn the top of the MEMORY queue instead\n\n\n\n3.1.2 Ethical implications: Anthropomorphism and Privacy\n大致是一些人沉迷于和聊天机器人谈话，还要求他们的对话具有隐私性\n\n3.2 PARRY: A computational model of schizophrenia\nAnother chatbot with a clinical psychology focus\nUsed to study schizophrenia 精神分裂症\n\nSame pattern response structure as Eliza\n\nBut a much richer:\ncontrol structure 控制结构\nlanguage understanding capabilities 语言理解能力\nmodel of mental state. 心理状态模型。\nvariables modeling levels of Anger, Fear, Mistrust\n模拟愤怒、恐惧、不信任程度的变量\n\n\n\n\n\n3.2.1 model of mental state 心理状态模型\nAffect variables\nFear (0 20)           Anger (0 20)         Mistrust (0 15)\n\nStart with all variables low\n\nAfter each user turn\nEach user statement can change Fear and Anger\nE.g., Insults increases Anger, Flattery decreases Anger\nMentions of his delusions increase Fear\n\n\n\n\n\n3.2.2 Parry’s responses depend on mental state\n4. Corpus based Chatbots\nDialogue as a Markov Decision Process (MDP)\nGiven state 𝒔, select action 𝒂 according to (hierarchical) policy 𝝅 \n给定状态𝒔, 选择动作𝒂 根据（分级）政策𝝅\n\n\nReceive reward 𝒓, observe new state s s′\n观察新的状态，收到响应奖励r\n\n\nContinue the cycle until the episode terminates.\n继续循环直到对话情节结束\n\n\n\n\nGoal of dialogue learning: find optimal 𝝅 to maximize expected rewards\n\n对话学习的目标：找到最佳𝝅 最大化预期回报\n\n\nA unified view: dialogue as optimal decision making Dialogue\n\n\n\n4.1 Two architectures for corpus based chabots\nResponse by retrieval  检索响应\nUse information retrieval to grab a response (that is appropriate to the context) from some corpus 使用信息检索从一些语料库中获取响应（适合上下文）\n\n\nResponse by generation 生成响应\n\nUse a language model or encoder decoder to generate the response given the dialogue context  在给定对话上下文的情况下，使用语言模型或编码器-解码器生成响应\n\n\nModern corpus based chatbots are very data-intensive \n\n现代基于语料库的聊天机器人非常数据密集\n\n\nThey commonly require hundreds of millions or billions of words\n\n\n\n4.2 Response by retrieval: classic IR method\nGiven a user turn  , and a training corpus  of conversation\nFind in  the turn  that is most similar ( tf idf cosine) to q\nSay r\n\n\n\\operatorname{response}(q, C)=\\underset{r \\in C}{\\operatorname{argmax}} \\frac{q \\cdot r}{|q||r|}\n深度学习方法，只是换了计算相似度的方法：\n\n\n\\begin{aligned}\nh_{q} &=\\operatorname{BERT}_{Q}(\\mathrm{q})[\\mathrm{CLS}] \\\\\nh_{r} &=\\operatorname{BERT}_{R}(\\mathrm{r})[\\mathrm{CLS}] \\\\\n\\operatorname{response}(q, C) &=\\underset{r \\in C}{\\operatorname{argmax}} h_{q} \\cdot h_{r}\n\\end{aligned}4.2.1 Response by retrieving and refining knowledge\nCan generate responses from informative text rather than dialogue?\n利用IR进行信息检索得到答案\nTo respond to turns like “Tell me something about Beijing”\nXiaoIce collects sentences from public lectures and news articles.\nAnd searches them using IR based on query expansion from user’s turn\n\n\nAugment encoder decoder model\n增强编码解码模型\nuse IR to retrieve passages from Wikipedia\nconcatenate each Wikipedia sentence to the dialogue context with a separator token. 使用分隔符标记将每个Wikipedia句子连接到对话上下文。\nGive as encoder context to the encoder decoder model, which learns to incorporate text into its response 为编码器-解码器模型提供编码器上下文，该模型学习将文本合并到其响应中\n\n\n\n\n\n4.3 Response by generation\nThink of response production as an encoder-decoder task\nGenerate each token  of the response by conditioning on the encoding of the entire query  and the response so far ​\n生成每个token是基于整个查询的编码q，以及过去的的token\n\n\n\n\n\\hat{r}_{t}=\\operatorname{argmax}_{\\mathrm{w} \\in \\mathrm{V}} P\\left(w \\mid q, r_{1} \\ldots r_{t-1}\\right)\n\n\nAlternative approach: fine tune a large language model on conversational data\n\nThe Chirpy Cardinal system (Paranjape et al., 2020):\n\nfine tunes GPT 2\non the E MPATHETIC D IALOGUES dataset ( Rashkin et al., 2019)\n\n\nOngoin research problems: Neural chatbots can get repetitive, bland, and inconsistent 神经聊天机器人会变得重复、乏味和不一致\n\n\n\n4.4 Challenge: The blandness problem\n\n回答过于无趣\n\nBlandness problem: cause and remedies 原因和改善\n\nCommon MLE objective (maximum likelihood) 都使用最大似然函数进行优化\n\n\n\n利用互信息改善 Mutual information objective:\n同时优化双方的条件概率，使得答案和问题更相关\n\n\n\n\nMutual Information for Neural Network Generation\nMutual information objective:\n\n\n\n\n\n希望Targe能生成Source的概率应该最小，从而增加约束\n\nSample outputs\n\n\n\n4.5 Challenge: The consistency problem\n即前后不一致问题\n\nE2E systems often exhibit poor response consistency :\n\n\n\n\nConversational data:\n原因出在数据集本身就不是1对1关系\n\n\n\n\n\n\n解决方法，将每个问答编码加入个人化信息，使得答案唯一，减少歧义性。\n\n\n4.5.1 Personal modeling as multi-task learning\n\nImproving personalization with multiple losses\n\n\n\n优化，使人物角色可以“预测”自己的反应\n做法如图各自经过一个隐藏层，然后最后cat后再经过一个隐藏层作为最后输出，优化四部分的损失，增加对身份的约束\n\n4.6 Challenge: Long conversational context\n对长对话无记忆性\n\nIt can be challenging for LSTM/GRU to encode very long context (i.e. more than 200 words: Khandelwal + 18\n\nHierarchical Encoder Decoder (HRED) Serban + 16\n\nEncodes: utterance (word by word) + conversation (turn by turn) 编码：话语（逐字）+对话（逐句）就是既要把当前的话语编码，也要把历史所有语料进行编码\n\n\n\n\n\nHierarchical Latent Variable Encoder Decoder (VHRED) Serban+ 17\nAdds a latent variable to the decoder 向解码器添加一个潜在变量\nTrained by maximizing variational lower bound on the log likelihood Related\n通过最大化对数似然相关函数的变分下界进行训练\n\n\n\n\n\n\n5. Hybrid Architectures\nChirpy Cardinal (Paranjape et al., 2020) response generation from a series of different generators\nGPT 2 finetuned on EmpatheticDialogues\nGPT 2 finetuned to paraphrase content 解析内容 from Wikipedia  \nRule based movie or music generators that produce scripted conversation about a movie or a musician 基于规则的电影或音乐生成器，用于生成有关电影或音乐家的脚本式对话\nasking the user’s opinion about a movie,\ngiving a fun fact,\nasking the user their opinion on an actor in the movie.\n\n\n\n6. The Frame based (“GUS”) Dialogue Architecture 基于框架的（“GUS”）对话架构\nSometimes called “ task based dialogue agents”\n\nSystems that have the goal of helping a user solve a task like making a travel reservation or buying a product 旨在帮助用户解决旅行预订或购买产品等任务的系统\n\n\nArchitecture:\n\nFirst proposed in the GUS system of 1977\nA knowledge structure representing user intentions 表示用户意图的知识结构\nOne or more frames (each consisting of slots with values 一个或多个帧（每个帧由带值的插槽组成）\n\n\n\n6.1 The Frame\nA set of slots , to be filled with information of a given type\n一组插槽，用于填充给定类型的信息\n\n\nEach associated with a question to the user\n每个都与用户的一个问题相关联\n\n\nSometimes called a domain ontology\n有时称为领域本体\n\n\n\n\n6.2 Control structure for GUS frame architecture\nSystem asks questions of user, filling any slots that user specifies\n\nUser might fill many slots at a time:\n\nI want a flight from San Francisco to Denver one way leaving after five p.m . on Tuesday\n\n\nWhen frame is filled, do database query\n\n\n6.3 GUS slots have condition action rules attached\nGUS插槽附带了条件操作规则\n\nSome rules attached to the DESTINATION slot for the plane booking frame\n\n飞机预订框架中的目的地时段附加的一些规则\n\n\nOnce the user has specified the destination\n一旦用户指定了目的地\nEnter that city as the default StayLocation for the hotel booking frame.\n输入该城市作为酒店预订框架的默认位置。\n\n\nOnce the user has specified DESTINATION DAY for a short trip\n\n一旦用户指定了短途旅行的目的地日期\nAutomatically copy as ARRIVAL DAY. 自动复制为到达日期\n\n\nFrames like:\n\nCar or hotel reservations\nGeneral route information\nWhich airlines fly from Boston to San Francisco? ,\n\n\nInformation about airfare practices\nDo I have to stay a specific number of days to get a decent airfare?.\n\n\nFrame detection:\nSystem must detect which slot of which frame user is filling\n系统必须检测用户正在填充哪个帧的哪个插槽\nAnd switch dialogue control to that frame.\n并将对话控制切换到该帧。\n\n\n\n6.3.1 GUS: Natural Language Understanding for filling dialog slots\nDomain classification  领域分类\n\nAsking weather? Booking a flight? Programming alarm clock?\n\n\nIntent Determination  意图确定\n\nFind a Movie, Show Flight, Remove Calendar Appt\n\n\nSlot Filling  插槽填充\n\nExtract the actual slots and fillers 提取实际插槽和填充器\n\n\n\n\n\n6.4.2 How to fill slots-&gt;Rule based Slot filling\nWrite regular expressions or grammar rules\nWake me (up) | set (the|an ) alarm | get me up\n\nDo text normalization\n\nA template is a pre-built response string 模板是预构建的响应字符串\n\nTemplates can be fixed \n\n“Hello, how can I help you?”\n\n\nOr have variables\n“What time do you want to leave CITY ORIG?”\n“Will you return to CITY ORIG from CITY DEST?”\n\n\n\n6.5 A more sophisticated 复杂的 version of the frame based architecture6.5.1 A Multi-turn Task oriented Dialogue state Architecture\n6.5.2 Components in a dialogue state architecture\n对话状态体系结构中的组件\nLU:extracts slot fillers from the user’s utterance using machine learning\nLU：使用机器学习从用户的话语中提取槽填充\n\n\nDialogue state tracker:maintains the current state of the dialogue (user’s most recent dialogue act, set of slot filler constraints from user has expressed so far).\n对话状态跟踪器：维护对话的当前状态（用户最近的对话行为，用户迄今为止表达的一组插槽填充约束）。\n\n\nDialogue policy: decides what the system should do or say next\n对话政策：决定系统下一步应该做什么或说什么\nGUS policy: ask questions until the frame was full then report back the results of some database query\nGUS策略：询问问题，直到框架已满，然后报告一些数据库查询的结果\n\n\nMore sophisticated: know when to answer questions, when to ask a clarification question, when to make a suggestion,etc\n更复杂：知道何时回答问题、何时提出澄清问题、何时提出建议等\n\n\n\n\nNLG: produce more natural, less templated utterances\n\n6.6 Dialogue Acts\nCombine the ideas of speech acts and grounding into a single representation\n将言语行为和基础的思想结合成一个单一的表达\n\n\n\n\n\n6.7 How to fill slots - Machine learning Slot filling\nMachine learning classifiers to map words to semantic frame fillers\n机器学习分类器将单词映射到语义框架填充词\n\n\nGiven a set of labeled sentences\nInput:\nI want to fly to San Francisco on Monday please”\n\n\nOutput: \nDestination: SF\nDepart-time: Monday\n\n\n\n\nBuild a classifier to map from one to the other Requirements: Lots of labeled data\n构建一个分类器，将一个需求映射到另一个需求：大量标记数据\n\n\n\n6.7.1 Slot filling as sequence labeling: BIO tagging\nThe BIO tagging paradigm\n\nIdea: Train a classifier to label each input word with a tag that tells us what slot (if any) it fills\n\n想法：训练一个分类器，用一个标签标记每个输入单词，告诉我们它填充了什么槽（如果有的话）\n\n\n\n\n\nWe create a B and I tag for each slot type \nAnd convert the training data to this format\n\nSlot filling using contextual embeddings\n\nOnce we have the BIO tag of the sentence\n\n\nWe can extract the filler string for each slot\n我们可以提取每个插槽的填充字符串\n\n\nAnd then normalize it to the correct form in the ontology\n然后将其规范化为本体中的正确形式\n\n\nLike “SFO” for San Francisco\nUsing homonym dictionaries (SF=SFO=San Francisco)\n使用同名词典（SF=SFO=San Francisco\n\n\n\n6.8 The task of dialogue state tracking\n可以简单理解为一直保留原来的slot直到满\n\n\n\n\nDialogue act interpretation algorithm: 对话行为解释算法：\n1 of N supervised classification to choose inform N个监督分类中的1个选择信息\nBased on encodings of current sentence + prior dialogue acts 基于当前句子的编码+先前的对话行为\n\n\nSimple dialogue state tracker: 简单对话状态跟踪器：\nRun a slot filler after each sentence 在每个句子后运行插槽填充程序\n\n\n\nAn special case of dialogue act detection:Detecting Correction Acts\n\nIf system misrecognizes an utterance\nUser might make a correction\nRepeat themselves\nRephrasing 重新措辞\nSaying “no” to a confirmation question\n\n\n\nFeatures for detecting corrections in spoken dialogue\n\n6.9 Dialogue Policy\nAt turn  predict action  to take, given entire history:\n\n\n\\hat{A}_{i}=\\underset{A_{i} \\in A}{\\operatorname{argmax}} P\\left(A_{i} \\mid\\left(A_{1}, U_{1}, \\ldots, A_{i-1}, U_{i-1}\\right)\\right.\nSimplify by just conditioning on the current dialogue state (filled frame slots) and the last turn by system and user: 只需调节当前对话状态（填充的帧槽）和最后一圈以及系统和用户的圈数即可简化\n\n\n\\hat{A}_{i}=\\underset{A_{i} \\in A}{\\operatorname{argmax}} P\\left(A_{i} \\mid \\text { Frame }_{i-1}, A_{i-1}, U_{i-1}\\right)6.9.1 Policy example: Confirmation and Rejection\nDialogue systems make errors\nSo they to make sure they have understood user\nTwo important mechanisms:\nconfirming understandings with the user\nrejecting utterances 话语 that the system is likely to have misunderstood.\n\n\n\n6.9.2 ConfirmationExplicit confirmation strategy\n\nImplicit confirmation strategy\n\nConfirmation strategy tradeoffs\n\nExplicit confirmation makes it easier for users to correct the system’s misrecognitions since a user can just answer “no” to the confirmation question.\nBut explicit confirmation is also awkward and increases the length of the conversation ( Danieli and Gerbino 1995, Walker et al. 1998).\n\n6.9.3 Rejection\nProgressive prompting for rejection\nDon’t just repeat the question “When would you like to leave?” Give user guidance about what they can say:\n\n\nUsing confidence to decide whether to confirm:\nASR or NLU systems can assign a confidence value indicating how likely they are that they understood the user.\nAcoustic log-likelihood of the utterance 声音的对数似然性\nProsodic features 韵律特征\nRatio of score of best to second-best interpretation 最佳口译得分与次优口译得分之比\n\n\nSystems could use set confidence thresholds:\n\n\n\\begin{array}{lll}\n","categories":["nlp"]},{"title":"题目","url":"/2021/08/15/knowledge%20engineering/%E9%A2%98%E7%9B%AE/","content":"题目\n1. XML RdfAuthoring guidelines:\n\nAll elements must have an end tag. 标签有头有尾\nAll elements must be cleanly nested (overlapping elements are not allowed). 所有元素必须不能重复\nAll attribute values must be enclosed in quotation marks. \nEach document must have a unique first element, the root node.\n大小写敏感\n\n&lt;book&gt;\t&lt;title&gt;Knowledge Graph&lt;/Title&gt; &lt;!--尾标签有误--&gt;\t&lt;author&gt;\t\t&lt;firstName&gt;Guilin&lt;/firstName&gt;\t\t&lt;lastName&gt;Qi&lt;/lastName&gt;\t\t&lt;email&gt;gqi@seu.edu.cn&lt;/email&gt;\t\tThis is some text inside an XML element.\t&lt;/author&gt;\t&lt;author&gt;\t\t&lt;firstName&gt;Tianxing&lt;lastName&gt;\t\t&lt;/firstName&gt;Wu&lt;/lastName&gt; &lt;!--嵌套出错--&gt;\t&lt;email&gt;tianxingwu@seu.edu.cn&lt;/email&gt;&lt;/author&gt;&lt;!--缺少&lt;/book&gt;--&gt;\n&lt;!--Typed Literals:--&gt;“Beantown”^^xsd:string“The Bay State” ^^xsd:string&lt;!--Plain literal and literals with language tags:--&gt;“France” “France”@en “France”@fr“法国”@zh “Frankreich”@de&lt;!--Equalities for Literals:--&gt; “001”^^xsd:integer = “1”^^xsd:integer “123.0”^^xsd:decimal = “00123”^^xsd:integer (based on datatype hierarchy)&lt;!--上面这两种形式等价，因为integer是decimal的父节点--&gt;\n\nNaming conflicts exist in different XML documents\n\nXML document_1&lt;table&gt;&lt;tr&gt;\t&lt;td&gt;Apples&lt;/td&gt;    &lt;td&gt;Bananas&lt;/td&gt;&lt;/ tr&gt;&lt;/table&gt;\n\nSolution \n\nXML document_1&lt;h:table&gt;\t&lt;h:tr&gt;\t\t&lt;h:td&gt;Apples&lt;/h:td&gt;        &lt;h:td&gt;Bananas&lt;/h:td&gt;    &lt;/h:tr&gt;&lt;/h:table&gt;\n\n2.Naming conflicts exist in different XML documents\n\nXML document_2&lt;table&gt;\t&lt;name&gt;African Coffee Table&lt;/name&gt;    &lt;width&gt;8G&lt; /width&gt;&lt;length&gt;120&lt;/length&gt;&lt; /table&gt;\n\nanswer\n\nXML document_2&lt; f:table&gt;\t&lt;f:name &gt;African Coffee lable&lt;/f:name&gt;    &lt;f:width&gt;80&lt;/f:width&gt;\t&lt;f:length&gt;120&lt;/f:length&gt;&lt;/f:table&gt;\n\n\nDoes the datatype “德国” equals to “德国” @ zh ?\nAnswer：不相同，因为他们位于的层次结构不同\n\n\n\n\n\nURI 英文字符   IRI任何编码\n\nEqualities for Literals:“0013^^xsd:integer =“13”^^xsd:integer“123.0^^xsd:decimal= “00123”1^^xsd:integer (based on data type hierarchy)\nExercise\nDoes the datatype“德国”equals to“德国”@ zh ?\n\n不一样\n\nRDF表示N元关系\n\n用一个节点中介\n\n\n\n利用一个空节点\n\n\n\n@prefix sw: &lt;http://www.semanticweb.org/ontology-9/&gt;sw:John sw:is_a sw:professors;\t\tsw:has_id sw:987654321;\t\tsw:has_name sw:John Doe.\n\n\n\n\n\n\n\nRepresent the following sentence graphically by means of the blank node:Wikipedia said that Tolkien wrote Lord of the Rings.\n\n\n\nPlease represent the following sentences graphically by means of blank nodes.Jonathon says cherries taste sweet.1月14日，国家药监局宣布酚酞片停止生产\n\n\n\nGiven：\n\nex:happilyMarriedWith rdfs:subPropertyOf ex:isMarriedTo . ex:isMarriedTo rdfs:domain ex:Person . ex:isMarriedTo rdfs:range ex:Person . ex:pascal ex:happilyMarriedWith ex:lisa .\n\n可推出：\n\nex:pascal ex:isMarriedTo ex:lisa . ex:pascal rdf:type ex:Person . ex:lisa rdf:type ex:Person .\n\nWhat can be inferred from the following triples using RDFS semantics?\n\nex:Postgraduate_student rdfs:subClassOf ex:Student ex:Professor rdfs:subClassOf ex:Academic_staff ex:Supervise rdfs:domain ex:Professor ex:Supervise rdfs:range ex:Postgraduate_student ex:John ex:Supervise ex:Mary\nex:John rdf:type ex:Professor ex:Mary rdf:type ex:Postgraduate_student ex:John rdf:type ex:Academic_staffex:Mary rdf:type ex:Studentt\n\nWhat can be inferred from the following triples using RDFS semantics?\n\nex:Undergraduate_student rdfs:subClassOf ex:Student .ex:Postgraduate_student rdfs:subClassOf ex:Student .ex:Professor rdfs:subClassOf ex: Academic_staff .ex:Academic_staff rdfs:subClassOf ex:University_staff .ex:Teach rdfs:domain ex:Academic_staff .ex:Teach rdfs:range ex:Student .ex:Supervise rdfs:domain ex:Professor .ex:Supervise rdfs:range ex:Postgraduate_student .ex:John ex:Supervise ex:Mary .\nex:Professor rdfs:subClassOf ex: University_staff .ex:John rdf:type ex:Professor .ex:Mary rdf:type ex:Postgraudate_student .ex:John rdf:type ex:Academic_staff .ex:John rdf:type ex:University_staff .ex:Mary rdf:type ex:Student .\n\nsw:John sw:is_a sw:professors . sw:John sw:has_name “John Doe” . sw:John sw:has_id “987654321” .\nsw:John sw:is_a sw:professors ; \t\tsw:has_name “John Doe” ; \t\tsw:has_id “987654321” .\n\nUse a RDF graph to represent the sentence “John is working in a company located in Sydney” (bland node can be considered).\n\n\n\nDoes the datatype “3.14”^^xsd:string equal to “+03.14”^^xsd:string? If not, why?\n\nAnswer:不是；这是字符串形式，字符串要考虑每个character，明显两个字符串不一样。\n\n\n2. OWL Equivalence between classes, individuals, and properties: 一些等价关系\nexp:Athlete owl:equivalentClass exp:SportsPlayer . exp:obtain owl:equivalentProperty exp:acquire . exp:SportsPlayerA owl:sameAs exp:Thomas .\nDisjointness between classes: 类别不相交关系\nexp:Man owl:disjointWith exp:Woman .\nIntersection of classes: 交集\nexp:Mother rdf:type owl:class ; \t\t   owl:intersectionOf (exp:Woman exp:Parent) .\n\n这里括号里面可以有更多类别\n\nNegation of a class: 类别取反\nexp:ABC rdf:type owl:class ; \t\towl:complementOf exp:Meat .\nProperty definitions: object property and data property.对属性的定义（谓词）\nex:friendOf rdf:type owl:ObjectProperty . ex:age rdf:type owl:DataProperty .\nTransitive property: 传递性\nexp:ancestor rdf:type owl:TransitiveProperty .\n\nGiven triples\n\nexp:Thmoas exp:ancestor exp:Jimmy . exp:Jimmy exp:ancestor exp:Michael .\n\nThen what can we infer from them?\n\nexp:Thmoas exp:ancestor exp:Michael .\nFunctional property:\nexp:hasMother rdf:type owl:FunctionalProperty .\n\nPlease explain using natural language:Everyone has only one mother.\n\n谓词的函数特性表示他的range取值只能有一个\n\n\ninverse functional property 具有反函数特性\nexp:postgraduateSupervisor rdf:type owl:InverseFunctionalProperty .\n\nPlease explain using natural language:\n\nEach post-graduate student has only one supervisor.\n\n这表示谓语的定义域只能有一个\n\nSymmetric property\nexp:friendOf rdf:type owl:SymmetricProperty .\n\nGiven a triple\n\nexp:Thmoas exp:friendOf exp:Lisa .\n\nThen what can we infer from it?\n\nexp:Lisa exp:friendOf exp:Thmoas .\nInverse relations between properties\nexp:ancestor owl:inverseOf exp:descendant .\n\nGiven a triple\n\nexp:Thmoas exp:ancestor exp:Jimmy .\n\nThen what can we infer from it?\n\nexp:Jimmy exp:descendant exp:Thmoas .\nConstraints on properties: universal quantifier ∀任意（谓词限定词）\nexp:Person rdf:type owl:Restriction ; \t\t   owl:onProperty exp:hasMother ; #表示Person可以充当hasMother的主语           owl:allValuesFrom exp:Women . #表示前一句谓语的range的取值范围\n\nWhat do we know from the above triples?\nIf the subject of exp:hasMother comes from the class exp:Person, then the object can be only from the class exp:Women\n\nConstraints on properties: existential quantifier ∃ 存在\nexp:SemanticWebPapers rdf:type owl:Restriction ; \t\t\t\t\t  owl:onProperty exp:publishedIn ; \t\t\t\t\t  owl:someValuesFrom exp:AAAIPapers . #存在一部分的发表在AAAIPapers\n\nWhat do we know from the above triples?\nIf the subject of exp:publishedIn comes from the class exp:SemanticWebPapers, then the object may be (at least one) from the class exp:AAAIPapers.\n\nA part of the Semantic Web papers were published in AAAI.\n\n\nConstraints on properties: cardinalities \nexp:Person rdf:type owl:Restriction ; \t\t   owl:onProperty exp:hasParent ; \t\t   owl:cardinality “2”^^xsd:integer . #限定hasParent的range只能有两个\n\nPlease explain using natural language:\nEach person should have two parents.\n\ncardinality也可以换成owl:maxCardinality/ owl:minCardinality，表示最多或者最少\n\nGiraffe is a kind of animal which only eats leaves.\n\n\n@prefix sw: &lt;http://semanticweb.org/&gt; @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; . sw:Giraffe rdfs:subClassOf _:x ._:x rdfs:subClassOf sw:Animal ; \trdf:type owl:Restriction ; \towl:onProperty sw:eat ; \towl:allValuesFrom sw:Leaf .\n\nPlease write the corresponding RDF triples (in Turtle) of the following sentence: “Childless Persons are the persons who are not parents”. Note that classes “ChildlessPerson”, “Person”, “Parent” are defined in the namespace http://semanticweb.org/ with the prefix “SW”.\n\n_:x0 rdf:type owl:class;\t owl:complement owl:Parent._:x1 rdf:type owl:class;\t owl:intersectionOf (sw:Person _:x0);\t owl:equivalentClass sw:ChildlessPerson.\n\nExercise 1:The father of each person should be a man\n\nexp:Person rdf:type owl:Restriction ; \t\t   owl:onProperty exp:hasFather ;\t\t\t    \t\t   \t\t   owl:allValuesFrom exp:Men .\n\nPlease useRDF triples (in Turtle) to represent the following sentence: “Each human has only one biological mother who should be a woman” with the classes: “Human”and “Women”, and the property“biologicalMotherOf ”. Each class or property can be defined in the namespace http://semanticweb.org/ with the prefix “SW”.\n\n@prefix sw: &lt;http://semanticweb.org/&gt; @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; .sw:biologicalMotherOf rdfs:domain sw:Women ;                      rdfs:range  sw:Human ;                      rdf:type    owl:InverscFunctionalProperty .\n5) Please write the corresponding RDF triples (in Turtle) of the following sentence: “Grandpas are the men who are grandparents”. Specific classes are defined in the namespace   http://semanticweb.org/. Prefixes are given as follows:   @prefix sw: http://semanticweb.org/ .   @prefix rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# .   @prefix owl: http://www.w3.org/2002/07/owl# .\n_:x  rdf:type  owl:class  ;     owl:intersectionOf  (sw:Man  sw:Grandparent)  ;     owl:equivalentClass  sw:Grandpa .\n3. Propositional Logic\nExample: Natural Language: Nigiri is a type of Sushi which has ingredient Rice and Fish\n\n\nFOL:∀x.[Nigiri(x) → Sushi(x) ∧ ∃y.[hasIngredient(x,y) ∧ Rice(y)] ∧∃z.[hasIngredient(x,z) ∧ Fish(z)]]\nAre the following valid OWL axioms? If not, why? (class: A,B,C,D, role: R)\nA⨅(B⨆ ∃R)\n(∀R.A⨆B)⨅A\n(A⨅B)¬C⨆D\n\n\n\nEverybody knowing some actor has only envious friends\n\n例1：$\\exists$knows.Actor $\\subseteq$​ $\\forall$hasfriend.Envious\n用一阶逻辑来改写：\n\nAll students that attend only master courses.\n例2：Student ⊓∀attendsCourse.MasterCourse\n用一阶逻辑改写：∀x (Student(x) ∧ ∀y(attendsCourse(x,y) ∧ MasterCourse(y)))\n例3：自然语言解释：All students that attend at least one lecture.\nStudent ⊑∃Attend.Lecture用一阶逻辑改写：∀x (Student(x) → ∃y(Attend(x,y) ∧ Lecture(y)))\nExample\n(1) Any student must attend at least one lecture.    Student ⊑ ∃Attend.Lecture(2) John is a brother of Mary.    Brother(John,Mary)(3) Parents are exactly those who are mothers or fathers.    *Parent ≡ Mother ⊔ Father\n\nExercise\n\n\nA Phd Student is a student who has a supervisor. \n\n\n\\text{Phd\\_student} \\subseteq \\text{student}\\ \\cap\\ \\exist\\text{Supervisedby.}\\ \\text{human} 2.Animals are not vegetables.\n\n\\text{Animal} \\subseteq\\ \\neg\\text{vegetable}\nExpress the following sentences in Description Logics.\n\nLi is a student of SEU. Any person who is enrolled in a university is a college student.\n\n\nExample: ¬Feamale ⊓ ∃haschild.Human ⊑ Human\n\n\n\nModify:\n\n\n\n\nExercise:\n\nGive a model for the ontology:\n\n$\\text{PhD}_\\text{student} \\sqsubseteq \\text{Student}$​\n$\\text{Student} \\sqsubseteq \\neg\\text{Employee}$​​, $\\text{PhD}_\\text{student}(\\text{John})$\n$\\text{Emploee}(\\text{Lisa}),\\text{Marriedto}(\\text{John},\\text{Lisa})$​​​ \n\n\nAnswer:\n\n$\\Delta=\\{jo,l\\}$\n$I_I(\\text{John})=jo$​​\n$I_I(\\text{Lisa})=l$​​\n$I_C(\\text{Emploee})={l}$​\n$I_C(\\text{PhD}_\\text{student})={jo}$​​\n$I_C(\\text{Student})={jo}$​​\n$I_R(\\text{Marriedto})={}$\n\n\nInconsistency is often caused by modeling errors. 如果不一致，常常都无法找到一个model\n\nUnicorn(beauty) \nUnicorn ⊑ Fictitious\nUnicorn ⊑ Animal\nAnimal ⊑¬Fictitious\n\n\nA knowledge base is incoherent if a named class is equivalent to ⊥.\n\nIt usually also points to a modeling error. 即通过构造空集可以证明model的错误\n\nUnicorn ⊑ Fictitious\nUnicorn ⊑ Animal \nAnimal ⊓ Fictitious ⊑ ⊥\n\n\nExpress the following sentences in Description Logic.\n\n\n\nEvery teacher must teach someoneTeacher $\\subseteq \\exists$ Teach. Human\nEvery finger is a bodypart and is a part-of hand.Finger $\\subseteq$ BodyPart $\\cap \\exists$ PartOf. Hand\nZhang is a teacher of SEUTeacher (Zhang), TeachIn(Zhang,SEU)\n\n\nGive a model of the following ontology:PhD $_{\\text {student }}$ U Undergraduatge $_{\\text {student }} \\sqsubseteq$ Student,PhD $_{\\text {student }}($ John $)$,Undergraduatge $_{\\text {student }}($ Jack $)$,Sister $($ Lisa,Jack $)$,Employee $($ Lisa $)$\n\n\\begin{array}{c}\\Delta=\\{j o, j a, l\\} \\\\ I_{I}(\\text { John })=j o \\\\ I_{I}(\\text { Jack })=j a \\\\ I_{I}(\\text { Lisa })=l \\\\ I_{C}(\\text { Employee })=\\{l\\}\\end{array}$I_{C}\\left(\\right.$ Undergraduatge $\\left._{\\text {student }}\\right)=\\{j a\\}$$I_{C}\\left(\\right.$ Phd $\\left._{\\text {student }}\\right)=\\{j o\\}$$I_{C}($ Student $)=\\{j o, j a\\}$$I_{R}($ Sister $)=\\{(l, j a)\\}$\n\nGive a model of the following ontology\n\nProfessor⊑FacultyMember,\n\nProfessor(Guilin Qi),\n\nhasAffiliation(Guilin Qi, SEU),\n\nStudentOf(Zhang, Guilin Qi)\n\n\nA model: \n\n∆={a,b,c}\nI(Guilin Qi)=a, I(SEU)=b, I(Zhang)=c\nI( Professor)={a}, I(FacultyMember)={a},\nI(hasAffiliation)={(a,b)},\nI(StudentOf)={(c,a)},\n\nGive a model of the following ontology\n\n\\begin{array}{l}\n\\text{Professor} \\sqsubseteq \\text{FacultyMember},\\\\\n\\text{Professor (Guilin Qi)},\\\\\n\\text{hasAffiliation (Guilin Qi, SEU)},\\\\\n\\text{StudentOf (Zhang, Guilin Qi)}\\\\\n\\end{array}\n\\begin{aligned}\n\\Delta &= \\{q,z,seu\\} \\\\\n\\mathrm{I}_{\\mathbf{I}}(\\text{Guilin Qi})&=q \\\\\n\\mathrm{I}_{\\mathbf{I}}(\\text{Zhang})&=z \\\\\n\\mathrm{I}_{\\mathbf{C}}(\\text{Professor})&=\\{q\\} \\\\\n\\mathrm{I}_{\\mathbf{C}}(\\text{FacultyMember})&=\\{q\\} \\\\\n\\mathrm{I}_{\\mathbf{R}}(\\text{hasAffiliation})&=\\{(q,seu)\\} \\\\\n\\mathrm{I}_{\\mathbf{R}}(\\text{StudentOf})&=\\{(z,q)\\} \\\\\n\\end{aligned}\n\n\n\nWrite the inferred axioms using description logic after conducting classification in forward reasoning on the following axioms.\nEndocarditis $\\sqsubseteq$ Heart DiseaseMiocardial Infarction $\\sqsubseteq$ Heart DiseaseHeart Disease $\\sqsubseteq$ DiseaseEnterococcal Endocarditis $\\sqsubseteq$ Endocarditis\n\nAnswer\nMiocardial_Infarction $\\sqsubseteq$​​​​ DiseaseEndocarditis $\\sqsubseteq$​​​​ DiseaseEnterococcal_Endocarditis $\\sqsubseteq$​​​​ Heart_DiseaseEnterococcal_Endocarditis $\\sqsubseteq$​​​​ Disease\n\n\n\n4. Knowledge Graph ReasoningExample1:\n\nExample2:\n\n\nSo what type of Endocarditis is?\n\n\nExample1:\n\n\nClassification is to compute all implicit subclass relations by applying rules on a TBox.\n利用TBox的规则计算所有子类的依赖关系\n\n\nExample2:\n\nHighvalue_Company ⊑ ∃beInvestedBy. Investment_Company\n高价值公司由投资公司投资。\nInvestment_Company ⊑ Financial_Institute\n投资公司属于金融机构\n∃beInvestedBy. Financial_Institute ⊑ Solvent_Company\n借助金融机构投资的公司都是具备偿还能力的 企业。\n\nAnswer:\n\nHighvalue_Company ⊑ ∃beInvestedBy. Financial_Institute\nHighvalue_Company ⊑ Solvent_Company\n\nExample1:\n\nRule: If you are human, then you are mortal.\nHuman(x)→Mortal(x)\nQuestion: Is Socrates a mortal?\nSolution: Check whether Mortal(Scorates) or Human(Scorates) is true.\n\nExample2:\n\n\nQuery: Find all patients with heart diseases, i.e., Heart_Disease(x)\nRewriting: Endocarditis(x) ⋁ Miocardial_Infarction(x) ⋁ Coronary_disease(x)\n可以理解成重写问题\n\nGiven the following KG, use query rewriting on the query Person(x) (this query is used for checking whether x is a person) .\n\n\n\nExercize：\nFind all sports players with the following KG, i.e., rewrite the query Sports_Player(x).\n\n\nQuery: Find all sports players \nRewriting: SportsPlayer(x) ⋁ BasketballPlayer(x) ⋁ FootballPlayer(x) ⋁ TennisPlayer(x) \n\nExample: \n\nPhDStudent ⊑ Student, \n\nPhDStudent ⊑ Employee, \n\nStudent ⊑ ¬ Employee\n\n以上最终可以退出Student和Employee都是空集，所以找不到一个model\nInconsistent ontology: ontology without a model. \n\n\n\nExample: \n\nPhDStudent ⊑ Student, \n\nPhDStudent ⊑ Employee,\n\nStudent ⊑ ¬ Employee, \n\nPhDStudent(John)\n\n这里则产生了矛盾\n\n\n\nExample: DICE ontology:\n\nBrain $\\sqsubseteq$ CentralNervousSystem $\\sqcap$ $\\exists$systempart.NervousSystem $\\sqcap$ BodyPart $\\sqcap$ $\\exists$ region.HeadAndNeck $\\sqcap$ $\\forall$ region.HeadAndNeck\nCentralNervousSystem $\\sqsubseteq$​​​ NervousSystem\nBodyPart $\\sqsubseteq \\neg$​​ NervousSystem    or   DisjointWith(BodyPart,NervousSystem)\nReasoning: \nBrain $\\sqsubseteq$​​​ Bodypart $\\sqsubseteq$​​​ $\\neg$​​ NervousSystem\nBrain $\\sqsubseteq$​​ CentralNervousSystem $\\sqsubseteq$​​ NervousSystem\nBrain = $\\emptyset$\n\n\nBrain是空集\n\n\n\nExample from Foaf:\n\nPerson(timbl)\nHomepage(timbl, http://w3.org/)\nHomepage(w3c, http://w3c.org/)\nOrganization(w3c)\nInverseFunctionalProperty(Homepage) (Homepage的domain只有一个值，所以矛盾)\nDisjointWith(Organization, Person)\n\nExample from OpenCyc:\n\nArtifactualFeatureType(PopulatedPlace)\nExistingStuffType(PopulatedPlace)\nDisjointWith(ExistingobjectType,ExistingStuffType)\nArtifactualFeatureType $\\sqsubseteq$​ ExistingObjectType\n\nExistingObjectType与ExistingStuddType不相交，但是却拥有相同的实例，所以矛盾\n\n\nExercise：The following ontology is inconsistent, and remove one axiom or assertion in it to make it consistent.\n\n\\begin{aligned}\n&\\operatorname{Ph} D_{\\text {student }} \\sqcup \\text { Undergraduatge }_{\\text {student }} \\sqsubseteq \\text { Student, } \\\\\n&\\text { Student } \\sqsubseteq \\neg \\text { Employee } \\\\\n&\\text { PhD }_{\\text {student }} \\text { (John), } \\\\\n&\\text { Employee(John), } \\\\\n&\\text { Marriedto(John, Lisa), } \\\\\n&\\text { Undergraduate(Jack) }\n\\end{aligned}\n$\\text { Student } \\sqsubseteq \\neg \\text { Employee }$ or $\\text { Employee(John)}$ or $\\text { PhD }_{\\text {student }} \\text { (John) }$\n\n%%Factsfriend(joe,sue) .friend(ann,sue) .friend(sue,max) .friend(max,ann) .%% Rulesfof(X,Y) :- friend(X,Y)fof(X,Z) :- friend(X,Y), fof(Y,Z)%%Quiery 1query(X) :- fof(X,ann)%%Answer:fof(sue,ann) .fof(max,ann) .fof(joe,ann) .fof(ann,ann) .\nExercise\n\nRule：fatherinlawOf(X,Z) :- wifeOf(X,Y), fatherOf(Y,Z)\n\nFact： wifeOf (YaoMing，YeLi)\n\nFact： fatherOf (YeLi，YeFa)\nWho is the father-in-law of Yao Ming？\n\nfatherinlawOf(YaoMing, YeFa)\n\n\n5. Knowledge Graph Construction\n@base &lt;http://foo.example/DB/&gt; . @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;. &lt;People/ID=7&gt; rdf:type &lt;People&gt; . #某条记录的类别&lt;People/ID=7&gt; &lt;People#ID&gt; &quot;7&quot; .  #具体属性&lt;People/ID=7&gt; &lt;People#fname&gt; &quot;Bob&quot; . &lt;People/ID=7&gt; &lt;People#addr&gt; &quot;18&quot; . &lt;People/ID=7&gt; &lt;People#ref-addr&gt; &lt;Addresses/ID=18&gt; . #利用外键关联映射两张表的记录&lt;People/ID=8&gt; rdf:type &lt;People&gt; . &lt;People/ID=8&gt; &lt;People#ID&gt; &quot;8&quot; . &lt;People/ID=8&gt; &lt;People#fname&gt; &quot;Sue&quot; . &lt;Addresses/ID=18&gt; rdf:type &lt;Addresses&gt; . &lt;Addresses/ID=18&gt; &lt;Addresses#ID&gt; &quot;18&quot; . &lt;Addresses/ID=18&gt; &lt;Addresses#city&gt; &quot;Cambridge&quot; . &lt;Addresses/ID=18&gt; &lt;Addresses#state&gt; &quot;MA&quot; .\n\nPlease use direct mapping to map the following two relational tables to RDF triples with the base IRI http://foo.example/DB/ and prefix rdf：http://www.w3.org/1999/02/22-rdf-syntax-ns# .\n\n\n@base &lt;http://foo.example/DB/&gt; .@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;.&lt;Student/ID=001&gt; rdf:type &lt;Student&gt; .&lt;Student/ID=001&gt; &lt;Student#ID&gt; &quot;001&quot; .&lt;Student/ID=001&gt; &lt;Student#sname&gt; &quot;Zhang&quot; .&lt;Student/ID=001&gt; &lt;Student#major&gt; &quot;101&quot; .&lt;Student/ID=001&gt; &lt;Student#ref-major&gt; &lt;Major/ID=101&gt; .&lt;Major/ID=101&gt; rdf:type &lt;Major&gt; .&lt;Major/ID=101&gt; &lt;Major#ID&gt; &quot;101&quot; .&lt;Major/ID=101&gt; &lt;Major#mname&gt; &quot;CS&quot; .&lt;Major/ID=101&gt; &lt;Major#address&gt; &quot;CS_Building&quot; .\n\nExample: database view\n\n\nExample:@prefix rr: &lt;http:l//www.w3.org/ns/r2rml#&gt; .&lt;TriplesMap1&gt;a rr:TriplesMap;       #&lt;TriplesMap1&gt;前没有‘#’时要加这一句rr:logicalTable [rr:tableName &quot;Person&quot;];rr:subjectMap[\trr:template &quot;http://www.ex.com/Person/ID=&#123;ID&#125;&quot;;                 #ID=&#123;ID&#125;  直接&#123;ID&#125;都是可以的  看你自己怎么定义template\trr:class &lt;http://www.ex.com/Person&gt;;#表示从url拿class];rr:predicateObjectMap [\trr:predicate &lt;http:7/www.ex.com/Person#NAME&gt;; #表示从url拿predicate    rr:objectMap [rr:column &quot;NAME&quot;]; #表示从db中取].\n\n\n\nSet of RDF triples\n\n&lt;http://data.example.com/employee/7369&gt; rdf:type ex:Employee.&lt;http://data.example.com/employee/7369&gt; ex:name &quot;SMITH&quot;. &lt;http://data.example.com/employee/7369&gt; ex:department &lt;http://data.example.com/department/10&gt;.&lt;http://data.example.com/department/10&gt; rdf:type ex:Department.&lt;http://data.example.com/department/10&gt; ex:name &quot;APPSERVER&quot;.&lt;http://data.example.com/department/10&gt; ex:location &quot;NEW YORK&quot;.&lt;http://data.example.com/department/10&gt; ex:staff 1.\n\nR2RML\n\n@prefix rr: &lt;http://www.w3.org/ns/r2rml#&gt;.@prefix ex: &lt;http://example.com/ns#&gt;.&lt;#TriplesMap1&gt;rr:logicalTable [ rr:tableName &quot;EMP&quot; ];rr:subjectMap [\trr:template &quot;http://data.example.com/employee/&#123;EMPNO&#125;&quot;;\trr:class ex:Employee;];rr:predicateObjectMap [\trr:predicate ex:name;\trr:objectMap [ rr:column &quot;ENAME&quot; ];].rr:predicateObjectMap [\trr:predicate ex:department; #与rdf对应\trr:objectMap [#定义宾语映射，在第二张表找\t\trr:parentTriplesMap &lt;#TriplesMap2&gt;; #去map2去找subject来当宾语\t\trr:joinCondition [#表示这些属性是相同的\t\t\trr:child &quot;DEPTNO&quot;;#triple2就是child\t\t\trr:parent &quot;DEPTNO&quot;;#triple1就是parent\t\t];\t];].\n&lt;#TriplesMap2&gt; rr:logicalTable &lt;#DeptTableView&gt;;rr:subjectMap [     rr:template &quot;http://data.example.com/department/&#123;DEPTNO&#125;&quot;; \trr:class ex:Department; ]; rr:predicateObjectMap [ \trr:predicate ex:name; #对于rdf的Property\trr:objectMap [ rr:column &quot;DNAME&quot; ]; ]; rr:predicateObjectMap [ \trr:predicate ex:location; \trr:objectMap [ rr:column &quot;LOC&quot; ]; ]; rr:predicateObjectMap [ \trr:predicate ex:staff; \trr:objectMap [ rr:column &quot;STAFF&quot; ]; ].\n\nPlease write the R2RML triples map to map the following relational database to RDF triples with prefix rr: http://www.w3.org/ns/r2rml# and prefix ex: http://example.com/ns#(for classes and properties).\n\n\n\nRDF Triples\n\n&lt;http://data.example.com/student/001&gt; ex:name &quot;Zhang&quot;. &lt;http://data.example.com/student/002&gt; ex:name &quot;Wang&quot;.\n@prefix rr:&lt;http://www.w3.org/ns/r2rml#&gt; .@prefix rr:&lt;http://example.com/ns#&gt; .&lt;#TriplesMap1&gt;\trr:logicalTable [rr:tabelName &quot;RDB&quot;];\trr:subjectMap[\t\trr:template &quot;http://data.example.com/student/&#123;ID&#125;&quot;;\t\trr:class ex:Student;\t];\trr:predicateObjectMap[\t\trr:predicate ex:name; \t\trr:objectMap [rr:column &quot;Name&quot;];\t].\n\nPlease use direct mapping to map the following three relational tables to RDF triples,\nGiven: \n@base http://foo.example/DB/ .\n\n\n@prefix rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns#.\nAnswer:@base &lt;http://foo.example/DB/&gt; .@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;.&lt;Student/ID=001&gt; rdf:type &lt;Student&gt;.&lt;Student/ID=001&gt; &lt;Student#ID&gt; “001”.&lt;Student/ID=001&gt; &lt;Student#sname&gt; “Wang”.&lt;Student/ID=001&gt; &lt;Student# major&gt; ”211”.&lt;Student/ID=001&gt; &lt;Student#ref-major&gt; &lt;Major/ID=211&gt;.&lt;Course/ID=8&gt; rdf:type &lt;Course&gt;.&lt;Course/ID=8&gt; &lt;Course#ID&gt; “8”. &lt;Course/ID=8&gt; &lt;Course#cname&gt; “KG”.&lt;Course/ID=8&gt; &lt;Course#ref-major&gt; &lt;Major/ID=211&gt;.&lt;Course/ID=8&gt; &lt;Course#major&gt; “211”.&lt;Course/ID=8&gt; &lt;Course#ref-major&gt; &lt;Major/ID=211&gt;.&lt;Course/ID=9&gt; rdf:type &lt;Course&gt;.&lt;Course/ID=9&gt; &lt;Course#ID&gt; “9”. &lt;Course/ID=9&gt; &lt;Course#cname&gt; “NLP”.&lt;Course/ID=9&gt; &lt;Course#major&gt; “211”.&lt;Course/ID=9&gt; &lt;Course#ref-major&gt; &lt;Major/ID=211&gt;.&lt;Major/ID=211&gt; rdf:type &lt;Major&gt;.&lt;Major/ID=211&gt; &lt;Major#ID&gt; “211”.&lt;Major/ID=211&gt; &lt;Major#mname&gt; “AI”.&lt;Major/ID=211&gt; &lt;Major#address&gt; “CS_Building”.\n\\2) Given a relational database and RDF triples, please write corresponding R2RML triples maps which can map the given RDB to RDF.\n\nRDF Triples\n&lt;http://data.example.com/student/001&gt; rdf:type ex:Student ;\tex:id &quot;001&quot; ;\tfoaf:name &quot;Wang&quot; ;\tex:major &quot;211&quot; ;    ex:gender &quot;Male&quot;. &lt;http://data.example.com/student/002&gt; rdf:type ex:Student ;    ex:id &quot;002&quot; ;    foaf:name &quot;Wu&quot; ;    ex:major &quot;201&quot; ;    ex:gender &quot;Female&quot;.\nAnswer:@prefix rr: &lt;http:l//www.w3.org/ns/r2rml#&gt; .@prefix ex: &lt;http://example.com/ns#&gt;.@prefix foaf: &lt; http://w3.org/ff#&gt;.&lt;#TriplesMap1&gt;rr:logicalTable [rr:tableName “Student”]rr:subjectMap[\trr:template “http://data.example.com/student/&#123;ID&#125;”;\trr:class ex:Student;];rr:predicateObjectMap[\trr:predicate ex:id;\trr:objectMap [rr:column “ID”];];rr:predicateObjectMap[\trr:predicate foaf:name;\trr:objectMap [rr:column “sname”];];rr:predicateObjectMap[\trr:predicate ex:mayjor;\trr:objectMap [rr:column “major”];];rr:predicateObjectMap[\trr:predicate ex:gender;\trr:objectMap [rr:column “gender”];];\n\nGiven a relational database and RDF triples, please write corresponding R2RML triples maps which can map the given RDB to RDF.\n\n\n&lt;http://data.example.com/student/001&gt; rdf:type ex:Student ;\t\t\t\t\t\t\t\t\t  ex:id &quot;001&quot; ;                                      ex:name &quot;Wang&quot; ;                                      ex:major &lt;http://data.example.com/major/211&gt; .&lt;http://data.example.com/student/002&gt; rdf:type ex:Student ;                                      ex:id &quot;002&quot; ;                                      ex:name &quot;Wu&quot; .&lt;http://data.example.com/major/211&gt; rdf:type ex:Major ;                                                             ex:name &quot;AI&quot; .\n6. Triple Extraction from Relational Web Tables\n\n\nExercise\nCompute the Levenshtein distance between “kitten” and “sitting”.\n1+1+1=3\n\n\nCompute the Jaccard similarity between “University of California, Davis” and “University of California, Berkeley”in word level.\n\n\nA\\cup B=\\{University, of, California, Davis, Berkeley\\}\nA\\cap B=\\{University, of, California\\}\nJ(A, B)=\\frac{|A\\cup B|}{|A\\cap B|}=\\frac{3}{5}\nCompute the Jaccard similarity between “English”and “England”based on character-level trigrams.\n\n\nA=\\{Eng, ngl, gli, lis, ish\\}\nB=\\{Eng, ngl, gla, lan, and\\}\nJ(A, B)=\\frac{|A\\cap B|}{|A\\cup B|}=\\frac{2}{8}=\\frac{1}{4}\nPlease compute the Levenshtein distance between two strings “Saturday” and “Sunday”. \n\nAnswer:\nSaturday→Sturday (delete a)\nSturday→Surday (delete t)\nSurday→Sunday (replace r with n)\nThus, the Levenshtein distance is 3.\n\nPlease compute the Jaccard similarity between “French” and “France” based on character-level bi-grams.\n\nAnswer:\nSegment “French” and “France” into bi-grams as follows:\nFrench: {Fr, re, en, nc, ch}\nFrance: {Fr, ra, an, nc, ce}\nJaccardSimilarity=2/8=0.25\n\nPlease tell the difference between local disambiguation and global disambiguation in entity linking.\n\nAnswer:\nLocal disambiguation does not consider the effects of other referent entities in the same table/ document. With semantic associations between candidate referent entities, global disambiguation can correct mistakes of local disambiguation.\n\nWhy do we need candidate generation in entity linking?\n\nAnswer:\nCandidate generation can significantly reduce the number of comparison between mentions and entities so that we no longer require to iterate through all entities in the knowledge base for each mention. It does improve the efficiency of entity linking.\n7. Knowledge Graph Construction from Semi-Structured Data\n\n\nsubject: the article entity\nQiang Yang\n\n\npredicate: the target relation\nbirthOnDate\n\n\nobject: the string captured by the brackets of the regular expression\n1964 births\n\n\n提取完之后需要做property\nNote: Domain and Range checking is necessary.\n\n\n\n\n\nIn the Wikipedia article page of “Mo Yan”, the categories as follows. Please extract facts from them in turtle using YAGO category maps without domain and range checking, and clarify the subsequent validation steps on the extracted fact.\nYAGO category maps\n\n\n\n\n@prefix expr: &lt;http://www.example.org/resource/&gt;@prefix expp: &lt;http://www.example.org/property/&gt;expr: MoYan expp: bornOnDate &quot;1955&quot;;\t\t\texpp: hasWonPrice expr: Han Chinese Nobel,\t\t\t\t\t\t\t  expr: Nobel,\t\t\t\t\t\t\t  expr: Mao Dun Literature Prize.\n\n需要check property的 range和domain\ntype inference\n\nGiven the Wikipedia article page of “IntelliJ IDEA”, including the first sentence, categories, and the infobox. Please write correct types from them (directly use plain strings).\n\n\n\n\n第一句话：integrated development environment(IDE)\ncategories：Free integrated development environments，Integrated development environments, Java development tools，Products, software\nInfobox：Software\n\n\nPlease extract a taxonomy from the following sentence (denote the answer as A is-a B):\n\nIBM, AMD, and Intel are High-tech companies using nanotechnology for several years.\nIBM is-a High-tech company\nAMD is-a High-tech company\nIntel is-a High-tech company\nHigh-tech company is-a company\nIBM is-a company\nAMD is-a company\nIntel is-a company\n\n8. Unstructured Data\n\n\nAssuming word with higher TF value and higher IDF value is more relevant to domain.\nTF: Term frequency.\n\n\n\n\n\\mathrm{TF}=\\frac{\\text { Number of certain word in a document }}{\\text { Number of all words in a document }}\nIDF: Inverse document frequency\n\n\n\\text{IDF}=\\log \\left(\\frac{\\text { Number of all documents in corpu }}{\\text { Number of documents containing the certain word }+1}\\right)Exercise——TF-IDF\nSuppose there are 100 words in a document, and the word “cow” appears three times. The word “cow” has appeared in 1,000 documents, and the total number of documents is 10,000,000. What is the TF-IDF score of the word “cow”?\n\n\n\\begin{array}{ll}\n\\text{TF}=\\frac{3}{100}\\\\\n\\text{IDF}=\\log \\frac{10000000}{1000+1}=3.9996\\\\\nTF\\times IDF=0.119987\n\\end{array}\n例：假定《亚洲的网络技术》一文长度为1000个词，“亚洲”、“网络”、“技术”各出现20次，则这三个词的“词频”（TF）都为0.02。 然后，搜索Google发现，包含“的”字的网页共有250亿张（假定这就是中文网页总数），包含“亚洲”的网页共有62.3亿张，包含“网络”的网页为0.484亿张，包含“技术”的网页为0.973亿张。计算“亚洲”、“网络”、“技术”的TF-IDF值.\n\n\n\\begin{array}{ll}\n\\text{IDF(亚洲)}=\\lg\\frac{250}{26.3}=0.603\\\\\n\\text{IDF(网络)}=\\lg\\frac{250}{0.484}=2.713\\\\\n\\text{IDF(技术)}=\\lg\\frac{250}{0.973}=2.410\\\\\n\\text{TF-IDF(亚洲)}=0.603\\times 0.02=0.01206\\\\\n\\text{TF-IDF(网络)}=2.713\\times 0.02=0.05426\\\\\n\\text{TF-IDF(技术)}=2.410\\times 0.02=0.04820\\\\\n\\end{array}9. Knowledge Graph Alignment\n\nExample:\n\ntrigrams(nikon) $=\\{$ nik, iko, kon $\\}$\ntrigrams(nike) $=\\{$ nik, ike $\\}$\n$\\operatorname{sim}($​ nikon, nike $)=1 / 4$​\n\n\nCompute the trigram based similarity between two strings University and Universe\n\ntrigrams(University)={Uni,niv,ive,ver,ers,rsi,sit,ity}\n\ntrigrams(Universe)={Uni,niv,ive,ver,ers,rse}\n\n$\\text{sim(University,Universe)}=5/8$\n\nCompute the Jaccard similarity between the first-order neighbor classes of the classes Car and Automobile from different ontologies.\n\n\n\n\nStringSimilarity\nlen(book)=4,len(Textbook)=8\nsubstring(book, Textbook)=book\nlen(substring)=4\nstrIngSimilarity(book, Textbook)=4/4=1\nstrIngSimilarity(Textbook, book)=4/8=1/2\n\n\nNeighbor Class Set Similarity\nNeighbor(book)={literature,volume}\nNeighbor(Textbook)={volume,Engineering,Science}\nNeighbor Class Set Similarity(book, Textbook)=1/2\nNeighbor Class Set Similarity(Textbook, book)=1/3\n\n\nInstance Set Similarity\nInstance(book)={Red Sorghum,Introduction to Algorithm}\nInstance(Textbook)={Red Sorghum}\nInstance Set Similarity(book, Textbook)=1/2\nInstance Set Similarity(Textbook, book)=1\n\n\n\n\nQuestion: now we have String Similarity, Neighbor Class Set Similarity, Textual Context Similarity, and Instance Set Similarity, please tell which one belongs to the element-level matching techniques? Which one is a structure-level matching technique?\n\nString Similarity,Textual Context Similarity element-level matching techniques\n\nNeighbor Class Set Similarity, Instance Set Similarity structure-level matching technique\n\nCompute the Jaccard similarity between the instance set of class “Book” and that of class “Textbook”.\n\n\n\n\n$A\\cup B$:{Red Sorghum, The Republic of Wine, Data Structure, Introduction to Algorithm, Operation System}\n$A\\cap B$:{Data Structure, Introduction to Algorithm}\nJaccard = $\\frac{|A\\cap B|}{|A\\cup B|}=\\frac{2}{5}$​\n\nPlease tell the differences between the rdf:type relation and the general is-a relation.\n\ngeneral is-a relation be inter-class.\n\n\nGiven the Wikipedia article page of “NumPy”, including the first sentence, categories, and the infobox. Please write correct types from them (directly use plain strings).\n\n\n\n\n\n\nAnswer\n\nlibrary, software, Free mathematics software, Free science software, Numerical analysis software for Linux, Numerical analysis software for MacOS, Numerical analysis software for Windows, Numerical programming languages, Python (programming language) scientific libraries, Software using the BSD license.\n\n\nIn the Wikipedia article page of “Tim Berners-Lee”, the categories are as follows. Please extract facts from them in turtle using YAGO category maps and the following prefixes without domain and range checking, and clarify the subsequent validation steps on the extracted fact.\n\n\n\n\n\n\nAnswer\n\nExtract facts\nexpr:Tim Berners-Lee\texpp:bornOnDate\t\t&quot;1955&quot; ;\t\t\t\t\t\texpp:hasWonPrize\texpr:Turing Award ;\t\t\t\t\t\texpp:hasWonPrize\texpr:Roval Medal ;\t\t\t\t\t\texpp:hasWonPrize\texpr:Medal ;\t\t\t\t\t\texpp:hasWonPrize\texpr:Webby Award ;\t\t\t\t\t\texpp:hasWonPrize\texpr:Award ;\n\nValidation\n\nCheck if expr:Tim Berners-Lee is in the domain of expp:bornOnDate and if &quot;1955&quot; is in the range of expp:bornOnDate;\nCheck if expr:Tim Berners-Lee is in the domain of expp:hasWonPrize and if expr:Turing Award expr:Roval Medal expr:Medal expr:Webby Award expr:Award is in the range of expp:hasWonPrize.\n\n\n\n\nExtract is-a relations form the following sentence, and derive all is-a relations using the transitivity of the is-a relation (in RDF Triple)\n\nWe have rules for keeping animals, such as horses and other farm animals, especially cows and chickens.\n\n\nAnswer\nfarm animal $\\to$ animal\nhorse $\\to$ animal\ncow $\\to$ animal\nchicken $\\to$ animal\ncow $\\to$ farm animal\nchicken $\\to$ animal\n\n\n\n\n\nCompute the TF-IDF values of knowledge, AI, Engineering with the statistics in the following tables, and rank them (Computing IDF using Ig).\n\n\\begin{aligned}\n&\\begin{array}{|l|l|}\n\\hline \\text { Doc1 } &  \\\\\n\\hline \\text { Term } & \\text { Count } \\\\\n\\hline \\text { Knowledge } & 3 \\\\\n\\hline \\text { Al } & 6 \\\\\n\\hline \\text { Engineering } & 4 \\\\\n\\hline\n\\end{array}\n&\\begin{array}{|l|l|}\n\\hline \\text { Doc2 } & \\\\\n\\hline \\text { Term } & \\text { Count } \\\\\n\\hline \\text { Knowledge } & 0 \\\\\n\\hline \\text { Al } & 5 \\\\\n\\hline \\text { Engineering } & 1 \\\\\n\\hline\n\\end{array}\n\\quad \\quad\n&\\begin{array}{|l|l|}\n\\hline \\text { Doc3 } &  \\\\\n\\hline \\text { Term } & \\text { Count } \\\\\n\\hline \\text { Knowledge } & 0 \\\\\n\\hline \\text { Ak } & 0 \\\\\n\\hline \\text { Engineering } & 5 \\\\\n\\hline\n\\end{array}\n\\end{aligned}\nAnswer\n\nDoc 1\n\n\n\\begin{array}{l}\nTF-IDF_{Knowledge} = \\frac{3}{13} \\times \\log (\\frac{3}{2}) = 0.0406 \\\\\nTF-IDF_{AI} = \\frac{6}{13} \\times \\log (\\frac{3}{3}) = 0 \\\\\nTF-IDF_{Engineering} = \\frac{4}{13} \\times \\log (\\frac{3}{4}) = -0.0384 \\\\\n\\end{array}\nDoc 2\n\n\\begin{array}{l}\nTF-IDF_{Knowledge} = 0\\\\\nTF-IDF_{AI} = \\frac{5}{6} \\times \\log (\\frac{3}{3}) = 0 \\\\\nTF-IDF_{Engineering} = \\frac{1}{6} \\times \\log (\\frac{3}{4}) = -0.0208 \\\\\n\\end{array}\nDoc 3\n\n\\begin{array}{l}\nTF-IDF_{Knowledge} = 0\\\\\nTF-IDF_{AI} = 0 \\\\\nTF-IDF_{Engineering} = \\frac{5}{5} \\times \\log (\\frac{3}{4}) = -0.1249 \\\\\n\\end{array}\n\n\n\n\n\nPlease compute the Levenshtein distance between two strings Saturday and Sunday.\n\nAnswer\n\nat \\to \\varnothing \\quad r \\to n \\\\\nLD = 3\n\n\n\nPlease compute the Jaccard similarity between “French” and “France” based on character-level bi-grams.\nAnswer\n$\\text{French} = \\{Fr,re,en,nc,ch\\}$\n$\\text{French} = \\{Fr,ra,an,nc, ce\\}$\n$\\text{Jaccard Similarity} = \\frac{2}{5}$\n\n\n\n\n\nPlease tell the difference between local disambiguation and global disambiguation in entity linking.\nLocal Disambiguation (局部消歧)\nOnly use the contextual information of the given mention and target entity for disambiguation without considering the effects of other referent entities in the same table.\n\n\nGlobal (Collective) Disambiguation (联合消歧)\nLeverage the semantic associations between candidate referent entities to jointly disambiguate all entities for the mentions in a table.\n\n\n\n\n\n\n\nWhy do we need candidate generation in entity linking?\nIdentifying candidate referent entities of each string mention in table cells when given a knowledge base.\nSelect Top k entities for further disambiguation to accelerate.\n\n\n\n10. Knowledge Graph Querying\n\n\n\n\nPlease rewrite the following triple pattern by using the same subject only once.\n\n?a int:number 123789 .?a int:pair 34567.?a int:id 666777 .\n?a int:number 123789；   int:pair 34567；   int:id 666777 .\n\nPlease rewrite the following triple pattern by using the same subject and predicate only once.\n\n?a string:name ‘Bob’.?a string:name ’Bobby’.?a string:name ‘Boob’.?a string:name ‘Bob_’.\n?a string:name ‘Bob’,’Bobby’,‘Boob’,‘Bob_’.\nExerciseGiven the dataset and the SPARQL query as follows, please write the query results.\n# Graph: http://example/addresses@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; .&lt;http://example/president25&gt; foaf:givenName &quot;Bill&quot; .&lt;http://example/president25&gt; foaf:familyName &quot;McKinley&quot; .&lt;http://example/president27&gt; foaf:givenName &quot;Bill&quot; .&lt;http://example/president27&gt; foaf:familyName &quot;Taft&quot; .&lt;http://example/president42&gt; foaf:givenName &quot;Bill&quot; .&lt;http://example/president42&gt; foaf:familyName &quot;Clinton&quot; .\nPREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt;SELECT ?name ?familyWHERE &#123; ?x foaf:givenName ?name .?x foaf:familyName ?family .&#125;\n\n\n\n\nname\nfamily\n\n\n\n\n“Bill”\n“McKinley”\n\n\n“Bill”\n“Taft”\n\n\n“Bill”\n“Clinton”\n\n\n\n\n\n\nGiven the dataset and the SPARQL query as follows, please write the query results.\n\n# Graph: http://example/addresses@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; .&lt;http://example/president25&gt; foaf:givenName &quot;Bill&quot; .&lt;http://example/president25&gt; foaf:familyName &quot;McKinley&quot; .&lt;http://example/president27&gt; foaf:givenName “Bob&quot; .&lt;http://example/president27&gt; foaf:id 159486.&lt;http://example/president42&gt; foaf:givenName “Marry&quot; .&lt;http://example/president42&gt; foaf:familyName &quot;Clinton&quot; .\nPREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt;SELECT ?name ?familyWHERE &#123; ?x foaf:givenName ?name .?x foaf:familyName ?family .&#125;\n\n\n\n\nname\nfamily\n\n\n\n\n“Bill”\n“McKinley”\n\n\n“Marry”\n“Clinton”\n\n\n\n\n\nMatching RDF literals\n\n@prefix dt: &lt;http://example.org/datatype#&gt; .@prefix ns: &lt;http://example.org/ns#&gt; .@prefix : &lt;http://example.org/ns#&gt; .@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .:x ns:p &quot;cat&quot;@en .:y ns:p &quot;42&quot;^^xsd:integer .:z ns:p &quot;abc&quot;^^dt:specialDatatype .\n\n第一种是找不到结果的，因为不加@是不一样的意义\n\n\n\n\n\n\n\n\n\nname\nfamily\n\n\n\n\n“Bill”\n“McKinley”\n\n\n\n\n\n\nExercise\nGiven the dataset and the SPARQL query as follows, please write the query results.\n\n\n\n\n\nAnswer\n\n\n\n\n\nname\nfamily\n\n\n\n\n“Bill”\n“McKinley”\n\n\n“Bob”\n\n\n\n“Marry”\n\n\n\n\n\n\n\n\nPlease write the SPARQL query on “list all volcanos located in Italy or Norway” given the following data.\n\ndepedia:Mount_Etna\trdf:type\tumbel-sc:Volcano;\t\t\t\t\trdfs:label \t&quot;Etna&quot;;\t\t\t\t\tp:location\tdbpedia:Italy.depedia:Mount_Baker\trdf:type\tumbel-sc:Volcano;\t\t\t\t\tp:location\tdbpedia:United_States.depedia:Beerenberg\trdf:type\tumbel-sc:Volcano;\t\t\t\t\trdfs:label\t&quot;Beerenberg&quot;@en;\t\t\t\t\tp:location\tdbpedia:Norway.\n\nAnswer\n\nSELECT ?volcano rdf:type umbel-sc:Volcano.WHERE &#123;    &#123;?Mount p:location dbpedia:Italy.&#125;    UNION &#123;?Mount p:location dbpedia:Norway.&#125;&#125;\n\n\n\n\nExercise\nGiven the dataset and the SPARQL query as follows, please write the query results.\nDataset\n\n# Default graph (stored at http://example.org/dft.ttl)@prefix dc: &lt;http://purl.org/dc/elements/1.1/&gt; .&lt;http://example.org/bob&gt; dc:publisher &quot;Bob Hacker&quot; .&lt;http://example.org/alice&gt; dc:publisher &quot;Alice Hacker&quot; .# Named graph: http://example.org/bob@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; ._:a foaf:name &quot;Bob&quot; ._:a foaf:mbox &lt;mailto:bob@oldcorp.example.org&gt; .# Named graph: http://example.org/alice@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; ._:a foaf:name &quot;Alice&quot; ._:a foaf:mbox &lt;mailto:alice@work.example.org&gt; .\n\nSPARQL query\nFROMNAMD可以省略，因为标明了GRAPH ?g\n\n\n\nPREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt;PREFIX dc: &lt;http://purl.org/dc/elements/1.1/&gt;SELECT ?who ?g ?mboxFROM &lt;http://example.org/dft.ttl&gt;FROM NAMED &lt;http://example.org/alice&gt;FROM NAMED &lt;http://example.org/bob&gt;WHERE&#123;?g dc:publisher ?who .GRAPH ?g &#123; ?x foaf:mbox ?mbox &#125;&#125;\n\nAnswer:\n\n\n\n\n\nwho\ng\nmbox\n\n\n\n\n“Bob Hacker”\n\\http://example.org/bob\n\\&#98;&#x6f;&#98;&#x40;&#x6f;&#x6c;&#x64;&#x63;&#x6f;&#x72;&#x70;&#x2e;&#x65;&#x78;&#x61;&#x6d;&#x70;&#108;&#x65;&#46;&#x6f;&#x72;&#x67;&#x5c;\n\n\n“Alice Hacker”\n\\http://example.org/alice\n\n\n\n\n\n\n\n\n\n\n\n\nGiven the dataset and the SPARQL query as follows, please write the query results.\n\nQ1@prefix org: &lt;http://example.com/ns#&gt; ._:a org:employeeName &quot;Alice&quot; ._:a org:employeeId 12345 ._:b org:employeeName &quot;Bob&quot; ._:b org:employeeId 67890 .\nPREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt;PREFIX org: &lt;http://example.com/ns#&gt;DELETE &#123;?person ?p ?o .&#125;where &#123;?person org:employeeId ?id .FILTER (?id &gt; 50000)?person ?p ?o .&#125;\n\n删除后：\n\n_:a org:employeeName &quot;Alice&quot; ._:a org:employeeId 12345 .\n2. Q2@prefix org: &lt;http://example.com/ns#&gt; .# Graph: http://person_:a org:employeeName &quot;Alice&quot; ._:a org:employeeId 12345 ._:b org:employeeName &quot;Bob&quot; ._:b org:employeeId 67890 .# Graph: http://person2_:c org:employeeId 13579\nPREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt;PREFIX org: &lt;http://example.com/ns#&gt;INSERT &#123;GRAPH &lt;http://person2&gt; &#123;?person ?p ?o .&#125;&#125;where &#123;Graph: &lt;http://person&gt;&#123;?person org:employeeId ?id .FILTER (?id &lt; 50000)?person ?p ?o .&#125;&#125;\n@prefix org: &lt;http://example.com/ns#&gt; .# Graph: http://person_:a org:employeeName &quot;Alice&quot; ._:a org:employeeId 12345 ._:b org:employeeName &quot;Bob&quot; ._:b org:employeeId 67890 .# Graph: http://person2_:c org:employeeId 13579 ._:a org:employeeName &quot;Alice&quot; ._:a org:employeeId 12345 .\n\n\n\n\n\n\nOrder the results.\n\n\n\nASC for ascending (default) and DESC (e.g., DESC(?name)) for descending.\n\n\nLIMIT\nlimits the number of result:\n只返回5个\n\n\n\n\nOFFSET\nposition/index of the first reported results:\n\n\n\nOrder of the result should be predictable (combine with ORDER BY)\n\nBINDINGS\n\n\nVALUE\nadd data to the query directly.\n增加限制\n\n\n\n\nAGGREGATES\n\nQuestion: Please use natural language to explain this SPARQL query!\n查选课人数超过5人的课\n\n\n\n\n\nQuestion: Please use natural language to explain the above SPARQL queries!\n\n\nTranslate the following natural language sentences into SPARQL.\n\nWhat are all the countries and their capitals in Africa?\n# Givenhttp://dbpedia.org/property/continent,http://dbpedia.org/resource/Africa,http://dbpedia.org/ontology/Country,http://dbpedia.org/ontology/capital.\nPREFIX dbp: &lt;http://dbpedia.org/&gt;PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;SELECT ?x ?yWHERE&#123;    ?x \trdf:type\tdbp:ontology/Country .    ?x \tdbp:property/continent\tdbp:resource/Africa .    ?x \tdbp:ontology/capital\t?y .&#125;\n\nWhat are the devices whose manufacturers are in Korea or Japan?\n#Givenhttp://dbpedia.org/property/locationCountry,http://dbpedia.org/resource/Korea,http://dbpedia.org/resource/Japan,http://dbpedia.org/ontology/manufactury,http://dbpedia.org/ontology/Device.\nPREFIX dbp: &lt;http://dbpedia.org/&gt;PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;SELECT ?deviceWHERE&#123;    ?device \trdf:type\tdbp:ontology/Device .    ?device \tdbp:ontology/manufacturer\t?location .    &#123;?location \tdbp:property/locationCountry\tdbp:resource/Japan .&#125;    UNION &#123;?location \tdbp:property/locationCountry\tdbp:resource/Korea .&#125;&#125;\n\n\n\nFrom the following RDF triples and the result of query, construct a SPARQL query.\n@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; . @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . _:a \trdf:type \tfoaf:Person . _:a \tfoaf:name \t&quot;Alice&quot; . _:a \tfoaf:mbox \t&lt;mailto:alice@example.com&gt; . _:a \tfoaf:mbox \t&lt;mailto:alice@work.example&gt; . _:b \trdf:type \tfoaf:Person . _:b \tfoaf:name \t&quot;Bob&quot; .\n| name    | mbox                                                   || :——— | :——————————————————————————- || “Alice” | mailto:alice@example.com   || “Alice” | mailto:alice@work.example || “Bob”   |                                                        |\n\nAnswer\nPREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt;PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;SELECT ?name, ?mboxWHERE&#123;    ?x \trdf:type\tfoaf:Person ;    \tfoaf:name   ?name .    OPTIONAL &#123;?x\t\tfoaf:mbox\t?mbox .&#125;&#125;\n\n\n\nGiven the dataset and query results as follows, please write the corresponding SPARQL Query.\n# Datasetdbpedia:Mount_Etna \t\trdf:type \tumbel-sc:Volcano ;\t\t\t\t\t\trdfs:label \t&quot;Etna&quot; ;\t\t\t\t\t\tp:location \tdbpedia:Italy .dbpedia:Mount_Xiqiao \trdf:type \tumbel-sc:Volcano ;\t\t\t\t\t\trdfs:label \t&quot;Mount_Xiqiao&quot; ;\t\t\t\t\t\tp:location \tdbpedia:China .dbpedia:Beerenberg \t\trdf:type \tumbel-sc:Volcano .\t\t\t\t\t\trdfs:label \t&quot;Beerenberg&quot;@en ;\t\t\t\t\t\tp:location \tdbpedia:Norway .# SPARQL Resultsdbpedia:Mount_Etna \t\trdfs:label \t&quot;Etna&quot; ;\t\t\t\t\t\trdf:type \tmyTypes:VolcanosOutsideChina.dbpedia:Beerenberg \t\trdfs:label \t&quot;Beerenberg&quot;@en;\t\t\t\t\t\trdf:type \tmyTypes:VolcanosOutsideChina.\n\nAnswer TODO\nCONSTRUCT &#123;    ?v \trdfs:label\t?name ;        rdf:type\tmyTypes:VolcanosOutsideChina .&#125;WHERE &#123;    ?v\trdf:type\tumbel-sc:Volcano;        rdfs:label\t?name.    OPTIONAL &#123;        ?v\tp:location\t?l . FILTER (?l = dbpedia:China)    &#125; FILTER (!BOUND(?l))&#125;\n\nFrom the following RDF triples and the result of query, construct a SPARQL query.\n@prefix foaf: http://xmlns.com/foaf/0.1/ . \n@prefix rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# . \n_:a rdf:type foaf:Person . \n_:a foaf:name “Alice” . \n_:a foaf:mbox &#x61;&#x6c;&#x69;&#x63;&#101;&#64;&#101;&#x78;&#97;&#x6d;&#112;&#x6c;&#101;&#x2e;&#x63;&#111;&#x6d; . \n_:a foaf:mbox &#x61;&#x6c;&#x69;&#x63;&#101;&#64;&#x77;&#111;&#114;&#107;&#46;&#101;&#120;&#97;&#x6d;&#112;&#x6c;&#x65; . \n_:b rdf:type foaf:Person . \n_:b foaf:name “Bob” . \n\n\n\n10. Cypher\n\nCREATE(\tnode:Movie\t&#123;\t\ttitle:&quot;The American President&quot;\t&#125;)\n\n\nCreate the relationship using cypher.\n\n\nCREATE\t(n1:Person &#123;name:&#x27;Oliver Stone&#x27;&#125;)\t-[r:DIRECTED]\t-&gt;(N2:Movie&#123;title:&quot;Wall Street&quot;&#125;)\n\n查询所有节点\n\nMATCH (n) RETURN n\n\n查询所有电影title\n\nMATCH (movie:Movie)RETURN movie.title\n\n名叫Oliver Stone相关联的事物标题\n\nMATCH (director &#123;name: &#x27;Oliver Stone&#x27;&#125;)--(movie) RETURN movie.title\n\n与人物Oliver Stone相关联的电影标题\n\nMATCH (:Person &#123;name: &#x27;Oliver Stone&#x27;&#125;)--(movie:Movie) RETURN movie.title\n\n与人物Oliver Stone相关联的关系类型\n\nMATCH (:Person &#123;name: &#x27;Oliver Stone&#x27;&#125;)-[r]-(movie) RETURN type(r)\n\n出演名为‘Wall Street’电影的演员姓名\n\nMATCH (wallstreet:Movie &#123;title: &#x27;Wall Street&#x27;&#125;)&lt;-[:ACTED_IN]-(actor) RETURN actor.name\n\n参演或导演名为Wall Street的电影的所有人物\n\nMATCH (wallstreet:Movie &#123;title: &#x27;Wall Street&#x27;&#125;)&lt;-[:ACTED_IN|:DIRECTED]-(person:Person) RETURN person.name\n\n名为Wall Street的电影的中所有的角色\n\nMATCH (wallstreet:Movie &#123;title: &#x27;Wall Street&#x27;&#125;)&lt;-[r:ACTED_IN]-(actor) RETURN r.role\n\nwhen nodes already exist, we can use MATCH first, then CREATE\n\n\nMATCH \t(charlie:Person &#123;name: &#x27;Charlie Sheen&#x27;&#125;),     (rob:Person &#123;name: &#x27;Rob Reiner&#x27;&#125;) CREATE     (rob)-[:TYPE INCLUDING A SPACE]-&gt;(charlie)\n\n查询参演名为‘The American President’的所有演员姓名\n\n\nMATCH\t(person:Person)-[:ACTED_IN]\t-&gt;(movie:Movie&#123;title:&quot;The American President&quot;&#125;)RETURN person.name\n\n查询包含角色’Card Fox’的电影名\n\n\nMATCH\t(actor)-[:ACTED_&#123;role:&quot;Card Fox&quot;&#125;]\t-&gt;(movie:Movie)RETURN movie.title\n\n\n删除名为UNKNOWN的节点\n\nMATCH (n:Person &#123;name: &#x27;UNKNOWN’&#125;) DELETE n\n\n删除数据库中所有节点及与其相连的关系\n\nMATCH (n) DETACH DELETE n\n\n删除名为Andy的节点和与其相连的所有关系\n\nMATCH (n &#123;name: &#x27;Andy’&#125;) DETACH DELETE n\n\n删除Andy的所有KNOWS关系\n\nMATCH (n &#123;name: &#x27;Andy&#x27;&#125;)-[r:KNOWS]-&gt;() DELETE r\n\n删除人物年龄为34岁的所有关系\n\n\nMATCH (person:Person &#123;age:34&#125;)-[r]-&gt;()DELETE r\n\nUpdate a property\n\nMATCH (n &#123;name: &#x27;Andy&#x27;&#125;) SET n.age = toString(n.age) RETURN n.name, n.age\n\nMATCH (n &#123;name: &#x27;Andy&#x27;&#125;) SET n.position = &#x27;Developer&#x27;, n.surname = &#x27;Taylor&#x27;\nMATCH (p &#123;name: &#x27;Peter&#x27;&#125;) SET p = &#123;name: &#x27;Peter Smith’, \tposition: &#x27;Entrepreneur&#x27;&#125; RETURN p.name, p.age, p.position\n\n更新George的年龄为28\n\n\nMATCH (p &#123;name:&#x27;George&#x27;&#125;)SET p = &#123;name: &#x27;George&#x27;\tage: 28&#125;\n\n\nWrite cypher to get the second graph\n\nMATCH \t(actor1:Actor &#123;name: &#x27;Anthony Hopkins&#x27;&#125;),\t(actor2:Actor &#123;name: &#x27;Hitchcock&#x27;&#125;),\t(movie:Movie &#123;title: &#x27;Hitechcock&#x27;&#125;)CREATE\t(actor1)-[r:ACTS_IN]\t-&gt;(movie)DELETE actor2\n\nQuery all relationship types connected by actor Anthony Hopkins\n\nMATCH\t(actor:Actor &#123;name: &#x27;Anthony Hopkins&#x27;&#125;)-[r]-()RETURN type(r)\n\nGiven the dataset and natural language query as follows, please write the corresponding Cypher Query.\nReturn the movie name containing the role “Bud Fox”\n\n\n\n\n\nAnswer\nMATCH (actor) - [:ACTED_IN &#123;role:&#x27;Bud Fox&#x27;&#125;] -&gt; (movie:Movie)RETURN movie.title\n\n\n\n\nDelete all the relationships on the person who is 34 years old using Cypher.\n\n\nAnswer\nMATCH (n:Person &#123;age: 34&#125;) - [r] -&gt; ()DELETE r\n\n\n","categories":["KnowledgeEngineering"]}]